# WARNING. DO NOT EDIT THIS FILE BY HAND. RUN ./scripts/generate-reference-architecture-configs.sh TO GENERATE IT. YOUR CHANGES WILL BE OVERRIDDEN
groups:
- interval: 1m
  name: 'Component-Level SLIs: consul - 5m burn-rate'
  rules: []
- interval: 2m
  name: 'Component-Level SLIs: consul - 30m burn-rate'
  rules: []
- interval: 1m
  name: 'Component-Level SLIs: consul - 1h burn-rate'
  rules: []
- interval: 2m
  name: 'Component-Level SLIs: consul - 6h burn-rate'
  rules: []
- interval: 1m
  name: 'Component mapping: consul'
  rules: []
- interval: 1m
  name: 'Component-Level SLIs: webservice - 5m burn-rate'
  rules:
  - expr: |
      (
        sum by () (
          rate(http_request_duration_seconds_bucket{job="gitlab-rails",le="1"}[5m])
        )
        +
        sum by () (
          rate(http_request_duration_seconds_bucket{job="gitlab-rails",le="10"}[5m])
        )
      )
      /
      2
    labels:
      component: puma
      tier: app
      type: webservice
    record: gitlab_component_apdex:success:rate_5m
  - expr: |
      sum by () (
        rate(http_request_duration_seconds_bucket{job="gitlab-rails",le="+Inf"}[5m])
      )
    labels:
      component: puma
      tier: app
      type: webservice
    record: gitlab_component_apdex:weight:score_5m
  - expr: |
      sum by () (
        rate(http_requests_total{job="gitlab-rails"}[5m])
      )
    labels:
      component: puma
      tier: app
      type: webservice
    record: gitlab_component_ops:rate_5m
  - expr: |
      (
        sum by () (
          rate(http_requests_total{job="gitlab-rails",status=~"5.."}[5m])
        )
      )
      or
      (
        0 * group by() (
          gitlab_component_ops:rate_5m{component="puma",tier="app",type="webservice"}
        )
      )
    labels:
      component: puma
      tier: app
      type: webservice
    record: gitlab_component_errors:rate_5m
  - expr: |
      (
        sum by () (
          rate(gitlab_workhorse_http_request_duration_seconds_bucket{le="1",route!="^/-/health$",route!="^/-/(readiness|liveness)$",route!="^/([^/]+/){1,}[^/]+/uploads\\z",route!="^/([^/]+/){1,}[^/]+\\.git/git-receive-pack\\z",route!="^/([^/]+/){1,}[^/]+\\.git/git-upload-pack\\z",route!="^/([^/]+/){1,}[^/]+\\.git/info/refs\\z",route!="^/([^/]+/){1,}[^/]+\\.git/gitlab-lfs/objects/([0-9a-f]{64})/([0-9]+)\\z"}[5m])
        )
        +
        sum by () (
          rate(gitlab_workhorse_http_request_duration_seconds_bucket{le="10",route!="^/-/health$",route!="^/-/(readiness|liveness)$",route!="^/([^/]+/){1,}[^/]+/uploads\\z",route!="^/([^/]+/){1,}[^/]+\\.git/git-receive-pack\\z",route!="^/([^/]+/){1,}[^/]+\\.git/git-upload-pack\\z",route!="^/([^/]+/){1,}[^/]+\\.git/info/refs\\z",route!="^/([^/]+/){1,}[^/]+\\.git/gitlab-lfs/objects/([0-9a-f]{64})/([0-9]+)\\z"}[5m])
        )
      )
      /
      2
    labels:
      component: workhorse
      tier: app
      type: webservice
    record: gitlab_component_apdex:success:rate_5m
  - expr: |
      sum by () (
        rate(gitlab_workhorse_http_request_duration_seconds_bucket{le="+Inf",route!="^/-/health$",route!="^/-/(readiness|liveness)$",route!="^/([^/]+/){1,}[^/]+/uploads\\z",route!="^/([^/]+/){1,}[^/]+\\.git/git-receive-pack\\z",route!="^/([^/]+/){1,}[^/]+\\.git/git-upload-pack\\z",route!="^/([^/]+/){1,}[^/]+\\.git/info/refs\\z",route!="^/([^/]+/){1,}[^/]+\\.git/gitlab-lfs/objects/([0-9a-f]{64})/([0-9]+)\\z"}[5m])
      )
    labels:
      component: workhorse
      tier: app
      type: webservice
    record: gitlab_component_apdex:weight:score_5m
  - expr: |
      sum by () (
        rate(gitlab_workhorse_http_requests_total{route!="^/-/health$",route!="^/-/(readiness|liveness)$"}[5m])
      )
    labels:
      component: workhorse
      tier: app
      type: webservice
    record: gitlab_component_ops:rate_5m
  - expr: |
      (
        sum by () (
          rate(gitlab_workhorse_http_requests_total{code=~"^5.*",route!="^/-/health$",route!="^/-/(readiness|liveness)$"}[5m])
        )
      )
      or
      (
        0 * group by() (
          gitlab_component_ops:rate_5m{component="workhorse",tier="app",type="webservice"}
        )
      )
    labels:
      component: workhorse
      tier: app
      type: webservice
    record: gitlab_component_errors:rate_5m
- interval: 2m
  name: 'Component-Level SLIs: webservice - 30m burn-rate'
  rules:
  - expr: |
      (
        sum by () (
          rate(http_request_duration_seconds_bucket{job="gitlab-rails",le="1"}[30m])
        )
        +
        sum by () (
          rate(http_request_duration_seconds_bucket{job="gitlab-rails",le="10"}[30m])
        )
      )
      /
      2
    labels:
      component: puma
      tier: app
      type: webservice
    record: gitlab_component_apdex:success:rate_30m
  - expr: |
      sum by () (
        rate(http_request_duration_seconds_bucket{job="gitlab-rails",le="+Inf"}[30m])
      )
    labels:
      component: puma
      tier: app
      type: webservice
    record: gitlab_component_apdex:weight:score_30m
  - expr: |
      sum by () (
        rate(http_requests_total{job="gitlab-rails"}[30m])
      )
    labels:
      component: puma
      tier: app
      type: webservice
    record: gitlab_component_ops:rate_30m
  - expr: |
      (
        sum by () (
          rate(http_requests_total{job="gitlab-rails",status=~"5.."}[30m])
        )
      )
      or
      (
        0 * group by() (
          gitlab_component_ops:rate_30m{component="puma",tier="app",type="webservice"}
        )
      )
    labels:
      component: puma
      tier: app
      type: webservice
    record: gitlab_component_errors:rate_30m
  - expr: |
      (
        sum by () (
          rate(gitlab_workhorse_http_request_duration_seconds_bucket{le="1",route!="^/-/health$",route!="^/-/(readiness|liveness)$",route!="^/([^/]+/){1,}[^/]+/uploads\\z",route!="^/([^/]+/){1,}[^/]+\\.git/git-receive-pack\\z",route!="^/([^/]+/){1,}[^/]+\\.git/git-upload-pack\\z",route!="^/([^/]+/){1,}[^/]+\\.git/info/refs\\z",route!="^/([^/]+/){1,}[^/]+\\.git/gitlab-lfs/objects/([0-9a-f]{64})/([0-9]+)\\z"}[30m])
        )
        +
        sum by () (
          rate(gitlab_workhorse_http_request_duration_seconds_bucket{le="10",route!="^/-/health$",route!="^/-/(readiness|liveness)$",route!="^/([^/]+/){1,}[^/]+/uploads\\z",route!="^/([^/]+/){1,}[^/]+\\.git/git-receive-pack\\z",route!="^/([^/]+/){1,}[^/]+\\.git/git-upload-pack\\z",route!="^/([^/]+/){1,}[^/]+\\.git/info/refs\\z",route!="^/([^/]+/){1,}[^/]+\\.git/gitlab-lfs/objects/([0-9a-f]{64})/([0-9]+)\\z"}[30m])
        )
      )
      /
      2
    labels:
      component: workhorse
      tier: app
      type: webservice
    record: gitlab_component_apdex:success:rate_30m
  - expr: |
      sum by () (
        rate(gitlab_workhorse_http_request_duration_seconds_bucket{le="+Inf",route!="^/-/health$",route!="^/-/(readiness|liveness)$",route!="^/([^/]+/){1,}[^/]+/uploads\\z",route!="^/([^/]+/){1,}[^/]+\\.git/git-receive-pack\\z",route!="^/([^/]+/){1,}[^/]+\\.git/git-upload-pack\\z",route!="^/([^/]+/){1,}[^/]+\\.git/info/refs\\z",route!="^/([^/]+/){1,}[^/]+\\.git/gitlab-lfs/objects/([0-9a-f]{64})/([0-9]+)\\z"}[30m])
      )
    labels:
      component: workhorse
      tier: app
      type: webservice
    record: gitlab_component_apdex:weight:score_30m
  - expr: |
      sum by () (
        rate(gitlab_workhorse_http_requests_total{route!="^/-/health$",route!="^/-/(readiness|liveness)$"}[30m])
      )
    labels:
      component: workhorse
      tier: app
      type: webservice
    record: gitlab_component_ops:rate_30m
  - expr: |
      (
        sum by () (
          rate(gitlab_workhorse_http_requests_total{code=~"^5.*",route!="^/-/health$",route!="^/-/(readiness|liveness)$"}[30m])
        )
      )
      or
      (
        0 * group by() (
          gitlab_component_ops:rate_30m{component="workhorse",tier="app",type="webservice"}
        )
      )
    labels:
      component: workhorse
      tier: app
      type: webservice
    record: gitlab_component_errors:rate_30m
- interval: 1m
  name: 'Component-Level SLIs: webservice - 1h burn-rate'
  rules:
  - expr: |
      (
        sum by () (
          rate(http_request_duration_seconds_bucket{job="gitlab-rails",le="1"}[1h])
        )
        +
        sum by () (
          rate(http_request_duration_seconds_bucket{job="gitlab-rails",le="10"}[1h])
        )
      )
      /
      2
    labels:
      component: puma
      tier: app
      type: webservice
    record: gitlab_component_apdex:success:rate_1h
  - expr: |
      sum by () (
        rate(http_request_duration_seconds_bucket{job="gitlab-rails",le="+Inf"}[1h])
      )
    labels:
      component: puma
      tier: app
      type: webservice
    record: gitlab_component_apdex:weight:score_1h
  - expr: |
      sum by () (
        rate(http_requests_total{job="gitlab-rails"}[1h])
      )
    labels:
      component: puma
      tier: app
      type: webservice
    record: gitlab_component_ops:rate_1h
  - expr: |
      (
        sum by () (
          rate(http_requests_total{job="gitlab-rails",status=~"5.."}[1h])
        )
      )
      or
      (
        0 * group by() (
          gitlab_component_ops:rate_1h{component="puma",tier="app",type="webservice"}
        )
      )
    labels:
      component: puma
      tier: app
      type: webservice
    record: gitlab_component_errors:rate_1h
  - expr: |
      (
        sum by () (
          rate(gitlab_workhorse_http_request_duration_seconds_bucket{le="1",route!="^/-/health$",route!="^/-/(readiness|liveness)$",route!="^/([^/]+/){1,}[^/]+/uploads\\z",route!="^/([^/]+/){1,}[^/]+\\.git/git-receive-pack\\z",route!="^/([^/]+/){1,}[^/]+\\.git/git-upload-pack\\z",route!="^/([^/]+/){1,}[^/]+\\.git/info/refs\\z",route!="^/([^/]+/){1,}[^/]+\\.git/gitlab-lfs/objects/([0-9a-f]{64})/([0-9]+)\\z"}[1h])
        )
        +
        sum by () (
          rate(gitlab_workhorse_http_request_duration_seconds_bucket{le="10",route!="^/-/health$",route!="^/-/(readiness|liveness)$",route!="^/([^/]+/){1,}[^/]+/uploads\\z",route!="^/([^/]+/){1,}[^/]+\\.git/git-receive-pack\\z",route!="^/([^/]+/){1,}[^/]+\\.git/git-upload-pack\\z",route!="^/([^/]+/){1,}[^/]+\\.git/info/refs\\z",route!="^/([^/]+/){1,}[^/]+\\.git/gitlab-lfs/objects/([0-9a-f]{64})/([0-9]+)\\z"}[1h])
        )
      )
      /
      2
    labels:
      component: workhorse
      tier: app
      type: webservice
    record: gitlab_component_apdex:success:rate_1h
  - expr: |
      sum by () (
        rate(gitlab_workhorse_http_request_duration_seconds_bucket{le="+Inf",route!="^/-/health$",route!="^/-/(readiness|liveness)$",route!="^/([^/]+/){1,}[^/]+/uploads\\z",route!="^/([^/]+/){1,}[^/]+\\.git/git-receive-pack\\z",route!="^/([^/]+/){1,}[^/]+\\.git/git-upload-pack\\z",route!="^/([^/]+/){1,}[^/]+\\.git/info/refs\\z",route!="^/([^/]+/){1,}[^/]+\\.git/gitlab-lfs/objects/([0-9a-f]{64})/([0-9]+)\\z"}[1h])
      )
    labels:
      component: workhorse
      tier: app
      type: webservice
    record: gitlab_component_apdex:weight:score_1h
  - expr: |
      sum by () (
        rate(gitlab_workhorse_http_requests_total{route!="^/-/health$",route!="^/-/(readiness|liveness)$"}[1h])
      )
    labels:
      component: workhorse
      tier: app
      type: webservice
    record: gitlab_component_ops:rate_1h
  - expr: |
      (
        sum by () (
          rate(gitlab_workhorse_http_requests_total{code=~"^5.*",route!="^/-/health$",route!="^/-/(readiness|liveness)$"}[1h])
        )
      )
      or
      (
        0 * group by() (
          gitlab_component_ops:rate_1h{component="workhorse",tier="app",type="webservice"}
        )
      )
    labels:
      component: workhorse
      tier: app
      type: webservice
    record: gitlab_component_errors:rate_1h
- interval: 2m
  name: 'Component-Level SLIs: webservice - 6h burn-rate'
  rules:
  - expr: |
      (
        sum by () (
          rate(http_request_duration_seconds_bucket{job="gitlab-rails",le="1"}[6h])
        )
        +
        sum by () (
          rate(http_request_duration_seconds_bucket{job="gitlab-rails",le="10"}[6h])
        )
      )
      /
      2
    labels:
      component: puma
      tier: app
      type: webservice
    record: gitlab_component_apdex:success:rate_6h
  - expr: |
      sum by () (
        rate(http_request_duration_seconds_bucket{job="gitlab-rails",le="+Inf"}[6h])
      )
    labels:
      component: puma
      tier: app
      type: webservice
    record: gitlab_component_apdex:weight:score_6h
  - expr: |
      sum by () (
        rate(http_requests_total{job="gitlab-rails"}[6h])
      )
    labels:
      component: puma
      tier: app
      type: webservice
    record: gitlab_component_ops:rate_6h
  - expr: |
      (
        sum by () (
          rate(http_requests_total{job="gitlab-rails",status=~"5.."}[6h])
        )
      )
      or
      (
        0 * group by() (
          gitlab_component_ops:rate_6h{component="puma",tier="app",type="webservice"}
        )
      )
    labels:
      component: puma
      tier: app
      type: webservice
    record: gitlab_component_errors:rate_6h
  - expr: |
      (
        sum by () (
          rate(gitlab_workhorse_http_request_duration_seconds_bucket{le="1",route!="^/-/health$",route!="^/-/(readiness|liveness)$",route!="^/([^/]+/){1,}[^/]+/uploads\\z",route!="^/([^/]+/){1,}[^/]+\\.git/git-receive-pack\\z",route!="^/([^/]+/){1,}[^/]+\\.git/git-upload-pack\\z",route!="^/([^/]+/){1,}[^/]+\\.git/info/refs\\z",route!="^/([^/]+/){1,}[^/]+\\.git/gitlab-lfs/objects/([0-9a-f]{64})/([0-9]+)\\z"}[6h])
        )
        +
        sum by () (
          rate(gitlab_workhorse_http_request_duration_seconds_bucket{le="10",route!="^/-/health$",route!="^/-/(readiness|liveness)$",route!="^/([^/]+/){1,}[^/]+/uploads\\z",route!="^/([^/]+/){1,}[^/]+\\.git/git-receive-pack\\z",route!="^/([^/]+/){1,}[^/]+\\.git/git-upload-pack\\z",route!="^/([^/]+/){1,}[^/]+\\.git/info/refs\\z",route!="^/([^/]+/){1,}[^/]+\\.git/gitlab-lfs/objects/([0-9a-f]{64})/([0-9]+)\\z"}[6h])
        )
      )
      /
      2
    labels:
      component: workhorse
      tier: app
      type: webservice
    record: gitlab_component_apdex:success:rate_6h
  - expr: |
      sum by () (
        rate(gitlab_workhorse_http_request_duration_seconds_bucket{le="+Inf",route!="^/-/health$",route!="^/-/(readiness|liveness)$",route!="^/([^/]+/){1,}[^/]+/uploads\\z",route!="^/([^/]+/){1,}[^/]+\\.git/git-receive-pack\\z",route!="^/([^/]+/){1,}[^/]+\\.git/git-upload-pack\\z",route!="^/([^/]+/){1,}[^/]+\\.git/info/refs\\z",route!="^/([^/]+/){1,}[^/]+\\.git/gitlab-lfs/objects/([0-9a-f]{64})/([0-9]+)\\z"}[6h])
      )
    labels:
      component: workhorse
      tier: app
      type: webservice
    record: gitlab_component_apdex:weight:score_6h
  - expr: |
      sum by () (
        rate(gitlab_workhorse_http_requests_total{route!="^/-/health$",route!="^/-/(readiness|liveness)$"}[6h])
      )
    labels:
      component: workhorse
      tier: app
      type: webservice
    record: gitlab_component_ops:rate_6h
  - expr: |
      (
        sum by () (
          rate(gitlab_workhorse_http_requests_total{code=~"^5.*",route!="^/-/health$",route!="^/-/(readiness|liveness)$"}[6h])
        )
      )
      or
      (
        0 * group by() (
          gitlab_component_ops:rate_6h{component="workhorse",tier="app",type="webservice"}
        )
      )
    labels:
      component: workhorse
      tier: app
      type: webservice
    record: gitlab_component_errors:rate_6h
- interval: 1m
  name: 'Component mapping: webservice'
  rules:
  - expr: "1"
    labels:
      component: puma
      regional_aggregation: "no"
      service_aggregation: "yes"
      tier: app
      type: webservice
    record: gitlab_component_service:mapping
  - expr: "1"
    labels:
      component: workhorse
      regional_aggregation: "no"
      service_aggregation: "yes"
      tier: app
      type: webservice
    record: gitlab_component_service:mapping
- interval: 1m
  name: 'Component-Level SLIs: gitaly - 5m burn-rate'
  rules:
  - expr: |
      (
        sum by () (
          rate(grpc_server_handling_seconds_bucket{grpc_method!~"CalculateChecksum|CommitLanguages|CommitStats|CreateFork|CreateRepositoryFromURL|FetchInternalRemote|FetchIntoObjectPool|FetchRemote|FetchSourceBranch|FindRemoteRepository|FindRemoteRootRef|Fsck|GarbageCollect|OptimizeRepository|PackObjectsHookWithSidechannel|PostReceiveHook|PostUploadPackWithSidechannel|PreReceiveHook|RepackFull|RepackIncremental|ReplicateRepository|SSHUploadPackWithSidechannel|UpdateHook",grpc_service!="gitaly.OperationService",grpc_type="unary",job="gitaly",le="0.5"}[5m])
        )
        +
        sum by () (
          rate(grpc_server_handling_seconds_bucket{grpc_method!~"CalculateChecksum|CommitLanguages|CommitStats|CreateFork|CreateRepositoryFromURL|FetchInternalRemote|FetchIntoObjectPool|FetchRemote|FetchSourceBranch|FindRemoteRepository|FindRemoteRootRef|Fsck|GarbageCollect|OptimizeRepository|PackObjectsHookWithSidechannel|PostReceiveHook|PostUploadPackWithSidechannel|PreReceiveHook|RepackFull|RepackIncremental|ReplicateRepository|SSHUploadPackWithSidechannel|UpdateHook",grpc_service!="gitaly.OperationService",grpc_type="unary",job="gitaly",le="1"}[5m])
        )
      )
      /
      2
    labels:
      component: goserver
      tier: stor
      type: gitaly
    record: gitlab_component_apdex:success:rate_5m
  - expr: |
      sum by () (
        rate(grpc_server_handling_seconds_bucket{grpc_method!~"CalculateChecksum|CommitLanguages|CommitStats|CreateFork|CreateRepositoryFromURL|FetchInternalRemote|FetchIntoObjectPool|FetchRemote|FetchSourceBranch|FindRemoteRepository|FindRemoteRootRef|Fsck|GarbageCollect|OptimizeRepository|PackObjectsHookWithSidechannel|PostReceiveHook|PostUploadPackWithSidechannel|PreReceiveHook|RepackFull|RepackIncremental|ReplicateRepository|SSHUploadPackWithSidechannel|UpdateHook",grpc_service!="gitaly.OperationService",grpc_type="unary",job="gitaly",le="+Inf"}[5m])
      )
    labels:
      component: goserver
      tier: stor
      type: gitaly
    record: gitlab_component_apdex:weight:score_5m
  - expr: |
      sum by () (
        rate(gitaly_service_client_requests_total{grpc_service!="gitaly.OperationService",job="gitaly"}[5m])
      )
    labels:
      component: goserver
      tier: stor
      type: gitaly
    record: gitlab_component_ops:rate_5m
  - expr: |
      (
        sum by () (
          label_replace(rate(gitaly_service_client_requests_total{grpc_code!~"AlreadyExists|Canceled|DeadlineExceeded|FailedPrecondition|InvalidArgument|NotFound|OK|PermissionDenied|Unauthenticated",grpc_service!="gitaly.OperationService",job="gitaly"}[5m]), "_c", "0", "", "")
          or
          label_replace(rate(gitaly_service_client_requests_total{deadline_type!="limited",grpc_code="DeadlineExceeded",grpc_service!="gitaly.OperationService",job="gitaly"}[5m]), "_c", "1", "", "")
        )
      )
      or
      (
        0 * group by() (
          gitlab_component_ops:rate_5m{component="goserver",tier="stor",type="gitaly"}
        )
      )
    labels:
      component: goserver
      tier: stor
      type: gitaly
    record: gitlab_component_errors:rate_5m
- interval: 2m
  name: 'Component-Level SLIs: gitaly - 30m burn-rate'
  rules:
  - expr: |
      (
        sum by () (
          rate(grpc_server_handling_seconds_bucket{grpc_method!~"CalculateChecksum|CommitLanguages|CommitStats|CreateFork|CreateRepositoryFromURL|FetchInternalRemote|FetchIntoObjectPool|FetchRemote|FetchSourceBranch|FindRemoteRepository|FindRemoteRootRef|Fsck|GarbageCollect|OptimizeRepository|PackObjectsHookWithSidechannel|PostReceiveHook|PostUploadPackWithSidechannel|PreReceiveHook|RepackFull|RepackIncremental|ReplicateRepository|SSHUploadPackWithSidechannel|UpdateHook",grpc_service!="gitaly.OperationService",grpc_type="unary",job="gitaly",le="0.5"}[30m])
        )
        +
        sum by () (
          rate(grpc_server_handling_seconds_bucket{grpc_method!~"CalculateChecksum|CommitLanguages|CommitStats|CreateFork|CreateRepositoryFromURL|FetchInternalRemote|FetchIntoObjectPool|FetchRemote|FetchSourceBranch|FindRemoteRepository|FindRemoteRootRef|Fsck|GarbageCollect|OptimizeRepository|PackObjectsHookWithSidechannel|PostReceiveHook|PostUploadPackWithSidechannel|PreReceiveHook|RepackFull|RepackIncremental|ReplicateRepository|SSHUploadPackWithSidechannel|UpdateHook",grpc_service!="gitaly.OperationService",grpc_type="unary",job="gitaly",le="1"}[30m])
        )
      )
      /
      2
    labels:
      component: goserver
      tier: stor
      type: gitaly
    record: gitlab_component_apdex:success:rate_30m
  - expr: |
      sum by () (
        rate(grpc_server_handling_seconds_bucket{grpc_method!~"CalculateChecksum|CommitLanguages|CommitStats|CreateFork|CreateRepositoryFromURL|FetchInternalRemote|FetchIntoObjectPool|FetchRemote|FetchSourceBranch|FindRemoteRepository|FindRemoteRootRef|Fsck|GarbageCollect|OptimizeRepository|PackObjectsHookWithSidechannel|PostReceiveHook|PostUploadPackWithSidechannel|PreReceiveHook|RepackFull|RepackIncremental|ReplicateRepository|SSHUploadPackWithSidechannel|UpdateHook",grpc_service!="gitaly.OperationService",grpc_type="unary",job="gitaly",le="+Inf"}[30m])
      )
    labels:
      component: goserver
      tier: stor
      type: gitaly
    record: gitlab_component_apdex:weight:score_30m
  - expr: |
      sum by () (
        rate(gitaly_service_client_requests_total{grpc_service!="gitaly.OperationService",job="gitaly"}[30m])
      )
    labels:
      component: goserver
      tier: stor
      type: gitaly
    record: gitlab_component_ops:rate_30m
  - expr: |
      (
        sum by () (
          label_replace(rate(gitaly_service_client_requests_total{grpc_code!~"AlreadyExists|Canceled|DeadlineExceeded|FailedPrecondition|InvalidArgument|NotFound|OK|PermissionDenied|Unauthenticated",grpc_service!="gitaly.OperationService",job="gitaly"}[30m]), "_c", "0", "", "")
          or
          label_replace(rate(gitaly_service_client_requests_total{deadline_type!="limited",grpc_code="DeadlineExceeded",grpc_service!="gitaly.OperationService",job="gitaly"}[30m]), "_c", "1", "", "")
        )
      )
      or
      (
        0 * group by() (
          gitlab_component_ops:rate_30m{component="goserver",tier="stor",type="gitaly"}
        )
      )
    labels:
      component: goserver
      tier: stor
      type: gitaly
    record: gitlab_component_errors:rate_30m
- interval: 1m
  name: 'Component-Level SLIs: gitaly - 1h burn-rate'
  rules:
  - expr: |
      (
        sum by () (
          rate(grpc_server_handling_seconds_bucket{grpc_method!~"CalculateChecksum|CommitLanguages|CommitStats|CreateFork|CreateRepositoryFromURL|FetchInternalRemote|FetchIntoObjectPool|FetchRemote|FetchSourceBranch|FindRemoteRepository|FindRemoteRootRef|Fsck|GarbageCollect|OptimizeRepository|PackObjectsHookWithSidechannel|PostReceiveHook|PostUploadPackWithSidechannel|PreReceiveHook|RepackFull|RepackIncremental|ReplicateRepository|SSHUploadPackWithSidechannel|UpdateHook",grpc_service!="gitaly.OperationService",grpc_type="unary",job="gitaly",le="0.5"}[1h])
        )
        +
        sum by () (
          rate(grpc_server_handling_seconds_bucket{grpc_method!~"CalculateChecksum|CommitLanguages|CommitStats|CreateFork|CreateRepositoryFromURL|FetchInternalRemote|FetchIntoObjectPool|FetchRemote|FetchSourceBranch|FindRemoteRepository|FindRemoteRootRef|Fsck|GarbageCollect|OptimizeRepository|PackObjectsHookWithSidechannel|PostReceiveHook|PostUploadPackWithSidechannel|PreReceiveHook|RepackFull|RepackIncremental|ReplicateRepository|SSHUploadPackWithSidechannel|UpdateHook",grpc_service!="gitaly.OperationService",grpc_type="unary",job="gitaly",le="1"}[1h])
        )
      )
      /
      2
    labels:
      component: goserver
      tier: stor
      type: gitaly
    record: gitlab_component_apdex:success:rate_1h
  - expr: |
      sum by () (
        rate(grpc_server_handling_seconds_bucket{grpc_method!~"CalculateChecksum|CommitLanguages|CommitStats|CreateFork|CreateRepositoryFromURL|FetchInternalRemote|FetchIntoObjectPool|FetchRemote|FetchSourceBranch|FindRemoteRepository|FindRemoteRootRef|Fsck|GarbageCollect|OptimizeRepository|PackObjectsHookWithSidechannel|PostReceiveHook|PostUploadPackWithSidechannel|PreReceiveHook|RepackFull|RepackIncremental|ReplicateRepository|SSHUploadPackWithSidechannel|UpdateHook",grpc_service!="gitaly.OperationService",grpc_type="unary",job="gitaly",le="+Inf"}[1h])
      )
    labels:
      component: goserver
      tier: stor
      type: gitaly
    record: gitlab_component_apdex:weight:score_1h
  - expr: |
      sum by () (
        rate(gitaly_service_client_requests_total{grpc_service!="gitaly.OperationService",job="gitaly"}[1h])
      )
    labels:
      component: goserver
      tier: stor
      type: gitaly
    record: gitlab_component_ops:rate_1h
  - expr: |
      (
        sum by () (
          label_replace(rate(gitaly_service_client_requests_total{grpc_code!~"AlreadyExists|Canceled|DeadlineExceeded|FailedPrecondition|InvalidArgument|NotFound|OK|PermissionDenied|Unauthenticated",grpc_service!="gitaly.OperationService",job="gitaly"}[1h]), "_c", "0", "", "")
          or
          label_replace(rate(gitaly_service_client_requests_total{deadline_type!="limited",grpc_code="DeadlineExceeded",grpc_service!="gitaly.OperationService",job="gitaly"}[1h]), "_c", "1", "", "")
        )
      )
      or
      (
        0 * group by() (
          gitlab_component_ops:rate_1h{component="goserver",tier="stor",type="gitaly"}
        )
      )
    labels:
      component: goserver
      tier: stor
      type: gitaly
    record: gitlab_component_errors:rate_1h
- interval: 2m
  name: 'Component-Level SLIs: gitaly - 6h burn-rate'
  rules:
  - expr: |
      (
        sum by () (
          rate(grpc_server_handling_seconds_bucket{grpc_method!~"CalculateChecksum|CommitLanguages|CommitStats|CreateFork|CreateRepositoryFromURL|FetchInternalRemote|FetchIntoObjectPool|FetchRemote|FetchSourceBranch|FindRemoteRepository|FindRemoteRootRef|Fsck|GarbageCollect|OptimizeRepository|PackObjectsHookWithSidechannel|PostReceiveHook|PostUploadPackWithSidechannel|PreReceiveHook|RepackFull|RepackIncremental|ReplicateRepository|SSHUploadPackWithSidechannel|UpdateHook",grpc_service!="gitaly.OperationService",grpc_type="unary",job="gitaly",le="0.5"}[6h])
        )
        +
        sum by () (
          rate(grpc_server_handling_seconds_bucket{grpc_method!~"CalculateChecksum|CommitLanguages|CommitStats|CreateFork|CreateRepositoryFromURL|FetchInternalRemote|FetchIntoObjectPool|FetchRemote|FetchSourceBranch|FindRemoteRepository|FindRemoteRootRef|Fsck|GarbageCollect|OptimizeRepository|PackObjectsHookWithSidechannel|PostReceiveHook|PostUploadPackWithSidechannel|PreReceiveHook|RepackFull|RepackIncremental|ReplicateRepository|SSHUploadPackWithSidechannel|UpdateHook",grpc_service!="gitaly.OperationService",grpc_type="unary",job="gitaly",le="1"}[6h])
        )
      )
      /
      2
    labels:
      component: goserver
      tier: stor
      type: gitaly
    record: gitlab_component_apdex:success:rate_6h
  - expr: |
      sum by () (
        rate(grpc_server_handling_seconds_bucket{grpc_method!~"CalculateChecksum|CommitLanguages|CommitStats|CreateFork|CreateRepositoryFromURL|FetchInternalRemote|FetchIntoObjectPool|FetchRemote|FetchSourceBranch|FindRemoteRepository|FindRemoteRootRef|Fsck|GarbageCollect|OptimizeRepository|PackObjectsHookWithSidechannel|PostReceiveHook|PostUploadPackWithSidechannel|PreReceiveHook|RepackFull|RepackIncremental|ReplicateRepository|SSHUploadPackWithSidechannel|UpdateHook",grpc_service!="gitaly.OperationService",grpc_type="unary",job="gitaly",le="+Inf"}[6h])
      )
    labels:
      component: goserver
      tier: stor
      type: gitaly
    record: gitlab_component_apdex:weight:score_6h
  - expr: |
      sum by () (
        rate(gitaly_service_client_requests_total{grpc_service!="gitaly.OperationService",job="gitaly"}[6h])
      )
    labels:
      component: goserver
      tier: stor
      type: gitaly
    record: gitlab_component_ops:rate_6h
  - expr: |
      (
        sum by () (
          label_replace(rate(gitaly_service_client_requests_total{grpc_code!~"AlreadyExists|Canceled|DeadlineExceeded|FailedPrecondition|InvalidArgument|NotFound|OK|PermissionDenied|Unauthenticated",grpc_service!="gitaly.OperationService",job="gitaly"}[6h]), "_c", "0", "", "")
          or
          label_replace(rate(gitaly_service_client_requests_total{deadline_type!="limited",grpc_code="DeadlineExceeded",grpc_service!="gitaly.OperationService",job="gitaly"}[6h]), "_c", "1", "", "")
        )
      )
      or
      (
        0 * group by() (
          gitlab_component_ops:rate_6h{component="goserver",tier="stor",type="gitaly"}
        )
      )
    labels:
      component: goserver
      tier: stor
      type: gitaly
    record: gitlab_component_errors:rate_6h
- interval: 1m
  name: 'Component mapping: gitaly'
  rules:
  - expr: "1"
    labels:
      component: goserver
      regional_aggregation: "no"
      service_aggregation: "yes"
      tier: stor
      type: gitaly
    record: gitlab_component_service:mapping
- interval: 1m
  name: 'Component-Level SLIs: praefect - 5m burn-rate'
  rules:
  - expr: |
      (
        sum by () (
          rate(grpc_server_handling_seconds_bucket{grpc_type="unary",job="praefect",le="0.5"}[5m])
        )
        +
        sum by () (
          rate(grpc_server_handling_seconds_bucket{grpc_type="unary",job="praefect",le="1"}[5m])
        )
      )
      /
      2
    labels:
      component: proxy
      tier: stor
      type: praefect
    record: gitlab_component_apdex:success:rate_5m
  - expr: |
      sum by () (
        rate(grpc_server_handling_seconds_bucket{grpc_type="unary",job="praefect",le="+Inf"}[5m])
      )
    labels:
      component: proxy
      tier: stor
      type: praefect
    record: gitlab_component_apdex:weight:score_5m
  - expr: |
      sum by () (
        rate(grpc_server_handled_total{job="praefect"}[5m])
      )
    labels:
      component: proxy
      tier: stor
      type: praefect
    record: gitlab_component_ops:rate_5m
  - expr: |
      (
        sum by () (
          rate(grpc_server_handled_total{grpc_code!~"^(OK|NotFound|Unauthenticated|AlreadyExists|FailedPrecondition|Canceled)$",job="praefect"}[5m])
        )
      )
      or
      (
        0 * group by() (
          gitlab_component_ops:rate_5m{component="proxy",tier="stor",type="praefect"}
        )
      )
    labels:
      component: proxy
      tier: stor
      type: praefect
    record: gitlab_component_errors:rate_5m
  - expr: |
      sum by () (
        rate(gitaly_praefect_replication_delay_bucket{le="300"}[5m])
      )
    labels:
      component: replicator_queue
      tier: stor
      type: praefect
    record: gitlab_component_apdex:success:rate_5m
  - expr: |
      sum by () (
        rate(gitaly_praefect_replication_delay_bucket{le="+Inf"}[5m])
      )
    labels:
      component: replicator_queue
      tier: stor
      type: praefect
    record: gitlab_component_apdex:weight:score_5m
  - expr: |
      sum by () (
        rate(gitaly_praefect_replication_delay_bucket{le="+Inf"}[5m])
      )
    labels:
      component: replicator_queue
      tier: stor
      type: praefect
    record: gitlab_component_ops:rate_5m
- interval: 2m
  name: 'Component-Level SLIs: praefect - 30m burn-rate'
  rules:
  - expr: |
      (
        sum by () (
          rate(grpc_server_handling_seconds_bucket{grpc_type="unary",job="praefect",le="0.5"}[30m])
        )
        +
        sum by () (
          rate(grpc_server_handling_seconds_bucket{grpc_type="unary",job="praefect",le="1"}[30m])
        )
      )
      /
      2
    labels:
      component: proxy
      tier: stor
      type: praefect
    record: gitlab_component_apdex:success:rate_30m
  - expr: |
      sum by () (
        rate(grpc_server_handling_seconds_bucket{grpc_type="unary",job="praefect",le="+Inf"}[30m])
      )
    labels:
      component: proxy
      tier: stor
      type: praefect
    record: gitlab_component_apdex:weight:score_30m
  - expr: |
      sum by () (
        rate(grpc_server_handled_total{job="praefect"}[30m])
      )
    labels:
      component: proxy
      tier: stor
      type: praefect
    record: gitlab_component_ops:rate_30m
  - expr: |
      (
        sum by () (
          rate(grpc_server_handled_total{grpc_code!~"^(OK|NotFound|Unauthenticated|AlreadyExists|FailedPrecondition|Canceled)$",job="praefect"}[30m])
        )
      )
      or
      (
        0 * group by() (
          gitlab_component_ops:rate_30m{component="proxy",tier="stor",type="praefect"}
        )
      )
    labels:
      component: proxy
      tier: stor
      type: praefect
    record: gitlab_component_errors:rate_30m
  - expr: |
      sum by () (
        rate(gitaly_praefect_replication_delay_bucket{le="300"}[30m])
      )
    labels:
      component: replicator_queue
      tier: stor
      type: praefect
    record: gitlab_component_apdex:success:rate_30m
  - expr: |
      sum by () (
        rate(gitaly_praefect_replication_delay_bucket{le="+Inf"}[30m])
      )
    labels:
      component: replicator_queue
      tier: stor
      type: praefect
    record: gitlab_component_apdex:weight:score_30m
  - expr: |
      sum by () (
        rate(gitaly_praefect_replication_delay_bucket{le="+Inf"}[30m])
      )
    labels:
      component: replicator_queue
      tier: stor
      type: praefect
    record: gitlab_component_ops:rate_30m
- interval: 1m
  name: 'Component-Level SLIs: praefect - 1h burn-rate'
  rules:
  - expr: |
      (
        sum by () (
          rate(grpc_server_handling_seconds_bucket{grpc_type="unary",job="praefect",le="0.5"}[1h])
        )
        +
        sum by () (
          rate(grpc_server_handling_seconds_bucket{grpc_type="unary",job="praefect",le="1"}[1h])
        )
      )
      /
      2
    labels:
      component: proxy
      tier: stor
      type: praefect
    record: gitlab_component_apdex:success:rate_1h
  - expr: |
      sum by () (
        rate(grpc_server_handling_seconds_bucket{grpc_type="unary",job="praefect",le="+Inf"}[1h])
      )
    labels:
      component: proxy
      tier: stor
      type: praefect
    record: gitlab_component_apdex:weight:score_1h
  - expr: |
      sum by () (
        rate(grpc_server_handled_total{job="praefect"}[1h])
      )
    labels:
      component: proxy
      tier: stor
      type: praefect
    record: gitlab_component_ops:rate_1h
  - expr: |
      (
        sum by () (
          rate(grpc_server_handled_total{grpc_code!~"^(OK|NotFound|Unauthenticated|AlreadyExists|FailedPrecondition|Canceled)$",job="praefect"}[1h])
        )
      )
      or
      (
        0 * group by() (
          gitlab_component_ops:rate_1h{component="proxy",tier="stor",type="praefect"}
        )
      )
    labels:
      component: proxy
      tier: stor
      type: praefect
    record: gitlab_component_errors:rate_1h
  - expr: |
      sum by () (
        rate(gitaly_praefect_replication_delay_bucket{le="300"}[1h])
      )
    labels:
      component: replicator_queue
      tier: stor
      type: praefect
    record: gitlab_component_apdex:success:rate_1h
  - expr: |
      sum by () (
        rate(gitaly_praefect_replication_delay_bucket{le="+Inf"}[1h])
      )
    labels:
      component: replicator_queue
      tier: stor
      type: praefect
    record: gitlab_component_apdex:weight:score_1h
  - expr: |
      sum by () (
        rate(gitaly_praefect_replication_delay_bucket{le="+Inf"}[1h])
      )
    labels:
      component: replicator_queue
      tier: stor
      type: praefect
    record: gitlab_component_ops:rate_1h
- interval: 2m
  name: 'Component-Level SLIs: praefect - 6h burn-rate'
  rules:
  - expr: |
      (
        sum by () (
          rate(grpc_server_handling_seconds_bucket{grpc_type="unary",job="praefect",le="0.5"}[6h])
        )
        +
        sum by () (
          rate(grpc_server_handling_seconds_bucket{grpc_type="unary",job="praefect",le="1"}[6h])
        )
      )
      /
      2
    labels:
      component: proxy
      tier: stor
      type: praefect
    record: gitlab_component_apdex:success:rate_6h
  - expr: |
      sum by () (
        rate(grpc_server_handling_seconds_bucket{grpc_type="unary",job="praefect",le="+Inf"}[6h])
      )
    labels:
      component: proxy
      tier: stor
      type: praefect
    record: gitlab_component_apdex:weight:score_6h
  - expr: |
      sum by () (
        rate(grpc_server_handled_total{job="praefect"}[6h])
      )
    labels:
      component: proxy
      tier: stor
      type: praefect
    record: gitlab_component_ops:rate_6h
  - expr: |
      (
        sum by () (
          rate(grpc_server_handled_total{grpc_code!~"^(OK|NotFound|Unauthenticated|AlreadyExists|FailedPrecondition|Canceled)$",job="praefect"}[6h])
        )
      )
      or
      (
        0 * group by() (
          gitlab_component_ops:rate_6h{component="proxy",tier="stor",type="praefect"}
        )
      )
    labels:
      component: proxy
      tier: stor
      type: praefect
    record: gitlab_component_errors:rate_6h
  - expr: |
      sum by () (
        rate(gitaly_praefect_replication_delay_bucket{le="300"}[6h])
      )
    labels:
      component: replicator_queue
      tier: stor
      type: praefect
    record: gitlab_component_apdex:success:rate_6h
  - expr: |
      sum by () (
        rate(gitaly_praefect_replication_delay_bucket{le="+Inf"}[6h])
      )
    labels:
      component: replicator_queue
      tier: stor
      type: praefect
    record: gitlab_component_apdex:weight:score_6h
  - expr: |
      sum by () (
        rate(gitaly_praefect_replication_delay_bucket{le="+Inf"}[6h])
      )
    labels:
      component: replicator_queue
      tier: stor
      type: praefect
    record: gitlab_component_ops:rate_6h
- interval: 1m
  name: 'Component mapping: praefect'
  rules:
  - expr: "1"
    labels:
      component: proxy
      regional_aggregation: "no"
      service_aggregation: "yes"
      tier: stor
      type: praefect
    record: gitlab_component_service:mapping
  - expr: "1"
    labels:
      component: replicator_queue
      regional_aggregation: "no"
      service_aggregation: "yes"
      tier: stor
      type: praefect
    record: gitlab_component_service:mapping
- interval: 1m
  name: 'Component-Level SLIs: sidekiq - 5m burn-rate'
  rules:
  - expr: |
      sum by (environment,feature_category,le,queue,shard,stage,tier,type,urgency,worker) (
        rate(sidekiq_jobs_completion_seconds_bucket[5m])
      )
    record: sli_aggregations:sidekiq_jobs_completion_seconds_bucket_rate5m
  - expr: |
      sum by (environment,feature_category,le,queue,shard,stage,tier,type,urgency,worker) (
        rate(sidekiq_jobs_queue_duration_seconds_bucket[5m])
      )
    record: sli_aggregations:sidekiq_jobs_queue_duration_seconds_bucket_rate5m
  - expr: |
      sum by (environment,feature_category,queue,shard,stage,tier,type,urgency,worker) (
        rate(sidekiq_jobs_failed_total[5m])
      )
    record: sli_aggregations:sidekiq_jobs_failed_total_rate5m
  - expr: |
      sum by () (
        rate(sidekiq_jobs_completion_seconds_count{worker=~"EmailReceiverWorker|ServiceDeskEmailReceiverWorker"}[5m])
      )
    labels:
      component: email_receiver
      tier: sv
      type: sidekiq
    record: gitlab_component_ops:rate_5m
  - expr: |
      (
        sum by () (
          rate(gitlab_transaction_event_email_receiver_error_total{error!="Gitlab::Email::AutoGeneratedEmailError"}[5m])
        )
      )
      or
      (
        0 * group by() (
          gitlab_component_ops:rate_5m{component="email_receiver",tier="sv",type="sidekiq"}
        )
      )
    labels:
      component: email_receiver
      tier: sv
      type: sidekiq
    record: gitlab_component_errors:rate_5m
  - expr: |
      sum by () (
        label_replace(sli_aggregations:sidekiq_jobs_completion_seconds_bucket_rate5m{le="10",urgency="high"}, "_c", "0", "", "")
        or
        label_replace(sli_aggregations:sidekiq_jobs_queue_duration_seconds_bucket_rate5m{le="10",urgency="high"}, "_c", "1", "", "")
        or
        label_replace(sli_aggregations:sidekiq_jobs_completion_seconds_bucket_rate5m{le="300",urgency="low"}, "_c", "2", "", "")
        or
        label_replace(sli_aggregations:sidekiq_jobs_queue_duration_seconds_bucket_rate5m{le="60",urgency="low"}, "_c", "3", "", "")
        or
        label_replace(sli_aggregations:sidekiq_jobs_completion_seconds_bucket_rate5m{le="300",urgency="throttled"}, "_c", "4", "", "")
        or
        label_replace(sli_aggregations:sidekiq_jobs_completion_seconds_bucket_rate5m{le="300",urgency=""}, "_c", "5", "", "")
        or
        label_replace(sli_aggregations:sidekiq_jobs_queue_duration_seconds_bucket_rate5m{le="60",urgency=""}, "_c", "6", "", "")
      )
    labels:
      component: shard_catchall
      tier: sv
      type: sidekiq
    record: gitlab_component_apdex:success:rate_5m
  - expr: |
      sum by () (
        label_replace(sli_aggregations:sidekiq_jobs_completion_seconds_bucket_rate5m{le="+Inf",urgency="high"}, "_c", "0", "", "")
        or
        label_replace(sli_aggregations:sidekiq_jobs_queue_duration_seconds_bucket_rate5m{le="+Inf",urgency="high"}, "_c", "1", "", "")
        or
        label_replace(sli_aggregations:sidekiq_jobs_completion_seconds_bucket_rate5m{le="+Inf",urgency="low"}, "_c", "2", "", "")
        or
        label_replace(sli_aggregations:sidekiq_jobs_queue_duration_seconds_bucket_rate5m{le="+Inf",urgency="low"}, "_c", "3", "", "")
        or
        label_replace(sli_aggregations:sidekiq_jobs_completion_seconds_bucket_rate5m{le="+Inf",urgency="throttled"}, "_c", "4", "", "")
        or
        label_replace(sli_aggregations:sidekiq_jobs_completion_seconds_bucket_rate5m{le="+Inf",urgency=""}, "_c", "5", "", "")
        or
        label_replace(sli_aggregations:sidekiq_jobs_queue_duration_seconds_bucket_rate5m{le="+Inf",urgency=""}, "_c", "6", "", "")
      )
    labels:
      component: shard_catchall
      tier: sv
      type: sidekiq
    record: gitlab_component_apdex:weight:score_5m
  - expr: |
      sum by () (
        sli_aggregations:sidekiq_jobs_completion_seconds_bucket_rate5m{le="+Inf",shard="catchall"}
      )
    labels:
      component: shard_catchall
      tier: sv
      type: sidekiq
    record: gitlab_component_ops:rate_5m
  - expr: |
      (
        sum by () (
          sli_aggregations:sidekiq_jobs_failed_total_rate5m{shard="catchall"}
        )
      )
      or
      (
        0 * group by() (
          gitlab_component_ops:rate_5m{component="shard_catchall",tier="sv",type="sidekiq"}
        )
      )
    labels:
      component: shard_catchall
      tier: sv
      type: sidekiq
    record: gitlab_component_errors:rate_5m
  - expr: |
      sum by (environment,tier,type,stage,shard,queue,feature_category,urgency,worker) (
        label_replace(sli_aggregations:sidekiq_jobs_queue_duration_seconds_bucket_rate5m{le="10",urgency="high"}, "_c", "0", "", "")
        or
        label_replace(sli_aggregations:sidekiq_jobs_queue_duration_seconds_bucket_rate5m{le="60",urgency="low"}, "_c", "1", "", "")
      )
    record: gitlab_background_jobs:queue:apdex:success:rate_5m
  - expr: |
      sum by (environment,tier,type,stage,shard,queue,feature_category,urgency,worker) (
        label_replace(sli_aggregations:sidekiq_jobs_queue_duration_seconds_bucket_rate5m{le="+Inf",urgency="high"}, "_c", "0", "", "")
        or
        label_replace(sli_aggregations:sidekiq_jobs_queue_duration_seconds_bucket_rate5m{le="+Inf",urgency="low"}, "_c", "1", "", "")
      )
    record: gitlab_background_jobs:queue:apdex:weight:score_5m
  - expr: |
      sum by (environment,tier,type,stage,shard,queue,feature_category,urgency,worker) (
        rate(sidekiq_enqueued_jobs_total{}[5m])
      )
    record: gitlab_background_jobs:queue:ops:rate_5m
  - expr: |
      sum by (environment,tier,type,stage,shard,queue,feature_category,urgency,worker) (
        label_replace(sli_aggregations:sidekiq_jobs_completion_seconds_bucket_rate5m{le="10",urgency="high"}, "_c", "0", "", "")
        or
        label_replace(sli_aggregations:sidekiq_jobs_completion_seconds_bucket_rate5m{le="300",urgency="low"}, "_c", "1", "", "")
        or
        label_replace(sli_aggregations:sidekiq_jobs_completion_seconds_bucket_rate5m{le="300",urgency="throttled"}, "_c", "2", "", "")
      )
    record: gitlab_background_jobs:execution:apdex:success:rate_5m
  - expr: |
      sum by (environment,tier,type,stage,shard,queue,feature_category,urgency,worker) (
        label_replace(sli_aggregations:sidekiq_jobs_completion_seconds_bucket_rate5m{le="+Inf",urgency="high"}, "_c", "0", "", "")
        or
        label_replace(sli_aggregations:sidekiq_jobs_completion_seconds_bucket_rate5m{le="+Inf",urgency="low"}, "_c", "1", "", "")
        or
        label_replace(sli_aggregations:sidekiq_jobs_completion_seconds_bucket_rate5m{le="+Inf",urgency="throttled"}, "_c", "2", "", "")
      )
    record: gitlab_background_jobs:execution:apdex:weight:score_5m
  - expr: |
      sum by (environment,tier,type,stage,shard,queue,feature_category,urgency,worker) (
        sli_aggregations:sidekiq_jobs_completion_seconds_bucket_rate5m{le="+Inf"}
      )
    record: gitlab_background_jobs:execution:ops:rate_5m
  - expr: |
      sum by (environment,tier,type,stage,shard,queue,feature_category,urgency,worker) (
        sli_aggregations:sidekiq_jobs_failed_total_rate5m{}
      )
      or
      (
        0 * group by (environment,tier,type,stage,shard,queue,feature_category,urgency,worker) (
          gitlab_background_jobs:execution:ops:rate_5m{}
        )
      )
    record: gitlab_background_jobs:execution:error:rate_5m
- interval: 2m
  name: 'Component-Level SLIs: sidekiq - 30m burn-rate'
  rules:
  - expr: |
      sum by (environment,feature_category,le,queue,shard,stage,tier,type,urgency,worker) (
        rate(sidekiq_jobs_completion_seconds_bucket[30m])
      )
    record: sli_aggregations:sidekiq_jobs_completion_seconds_bucket_rate30m
  - expr: |
      sum by (environment,feature_category,le,queue,shard,stage,tier,type,urgency,worker) (
        rate(sidekiq_jobs_queue_duration_seconds_bucket[30m])
      )
    record: sli_aggregations:sidekiq_jobs_queue_duration_seconds_bucket_rate30m
  - expr: |
      sum by (environment,feature_category,queue,shard,stage,tier,type,urgency,worker) (
        rate(sidekiq_jobs_failed_total[30m])
      )
    record: sli_aggregations:sidekiq_jobs_failed_total_rate30m
  - expr: |
      sum by () (
        rate(sidekiq_jobs_completion_seconds_count{worker=~"EmailReceiverWorker|ServiceDeskEmailReceiverWorker"}[30m])
      )
    labels:
      component: email_receiver
      tier: sv
      type: sidekiq
    record: gitlab_component_ops:rate_30m
  - expr: |
      (
        sum by () (
          rate(gitlab_transaction_event_email_receiver_error_total{error!="Gitlab::Email::AutoGeneratedEmailError"}[30m])
        )
      )
      or
      (
        0 * group by() (
          gitlab_component_ops:rate_30m{component="email_receiver",tier="sv",type="sidekiq"}
        )
      )
    labels:
      component: email_receiver
      tier: sv
      type: sidekiq
    record: gitlab_component_errors:rate_30m
  - expr: |
      sum by () (
        label_replace(sli_aggregations:sidekiq_jobs_completion_seconds_bucket_rate30m{le="10",urgency="high"}, "_c", "0", "", "")
        or
        label_replace(sli_aggregations:sidekiq_jobs_queue_duration_seconds_bucket_rate30m{le="10",urgency="high"}, "_c", "1", "", "")
        or
        label_replace(sli_aggregations:sidekiq_jobs_completion_seconds_bucket_rate30m{le="300",urgency="low"}, "_c", "2", "", "")
        or
        label_replace(sli_aggregations:sidekiq_jobs_queue_duration_seconds_bucket_rate30m{le="60",urgency="low"}, "_c", "3", "", "")
        or
        label_replace(sli_aggregations:sidekiq_jobs_completion_seconds_bucket_rate30m{le="300",urgency="throttled"}, "_c", "4", "", "")
        or
        label_replace(sli_aggregations:sidekiq_jobs_completion_seconds_bucket_rate30m{le="300",urgency=""}, "_c", "5", "", "")
        or
        label_replace(sli_aggregations:sidekiq_jobs_queue_duration_seconds_bucket_rate30m{le="60",urgency=""}, "_c", "6", "", "")
      )
    labels:
      component: shard_catchall
      tier: sv
      type: sidekiq
    record: gitlab_component_apdex:success:rate_30m
  - expr: |
      sum by () (
        label_replace(sli_aggregations:sidekiq_jobs_completion_seconds_bucket_rate30m{le="+Inf",urgency="high"}, "_c", "0", "", "")
        or
        label_replace(sli_aggregations:sidekiq_jobs_queue_duration_seconds_bucket_rate30m{le="+Inf",urgency="high"}, "_c", "1", "", "")
        or
        label_replace(sli_aggregations:sidekiq_jobs_completion_seconds_bucket_rate30m{le="+Inf",urgency="low"}, "_c", "2", "", "")
        or
        label_replace(sli_aggregations:sidekiq_jobs_queue_duration_seconds_bucket_rate30m{le="+Inf",urgency="low"}, "_c", "3", "", "")
        or
        label_replace(sli_aggregations:sidekiq_jobs_completion_seconds_bucket_rate30m{le="+Inf",urgency="throttled"}, "_c", "4", "", "")
        or
        label_replace(sli_aggregations:sidekiq_jobs_completion_seconds_bucket_rate30m{le="+Inf",urgency=""}, "_c", "5", "", "")
        or
        label_replace(sli_aggregations:sidekiq_jobs_queue_duration_seconds_bucket_rate30m{le="+Inf",urgency=""}, "_c", "6", "", "")
      )
    labels:
      component: shard_catchall
      tier: sv
      type: sidekiq
    record: gitlab_component_apdex:weight:score_30m
  - expr: |
      sum by () (
        sli_aggregations:sidekiq_jobs_completion_seconds_bucket_rate30m{le="+Inf",shard="catchall"}
      )
    labels:
      component: shard_catchall
      tier: sv
      type: sidekiq
    record: gitlab_component_ops:rate_30m
  - expr: |
      (
        sum by () (
          sli_aggregations:sidekiq_jobs_failed_total_rate30m{shard="catchall"}
        )
      )
      or
      (
        0 * group by() (
          gitlab_component_ops:rate_30m{component="shard_catchall",tier="sv",type="sidekiq"}
        )
      )
    labels:
      component: shard_catchall
      tier: sv
      type: sidekiq
    record: gitlab_component_errors:rate_30m
  - expr: |
      sum by (environment,tier,type,stage,shard,queue,feature_category,urgency,worker) (
        label_replace(sli_aggregations:sidekiq_jobs_queue_duration_seconds_bucket_rate30m{le="10",urgency="high"}, "_c", "0", "", "")
        or
        label_replace(sli_aggregations:sidekiq_jobs_queue_duration_seconds_bucket_rate30m{le="60",urgency="low"}, "_c", "1", "", "")
      )
    record: gitlab_background_jobs:queue:apdex:success:rate_30m
  - expr: |
      sum by (environment,tier,type,stage,shard,queue,feature_category,urgency,worker) (
        label_replace(sli_aggregations:sidekiq_jobs_queue_duration_seconds_bucket_rate30m{le="+Inf",urgency="high"}, "_c", "0", "", "")
        or
        label_replace(sli_aggregations:sidekiq_jobs_queue_duration_seconds_bucket_rate30m{le="+Inf",urgency="low"}, "_c", "1", "", "")
      )
    record: gitlab_background_jobs:queue:apdex:weight:score_30m
  - expr: |
      sum by (environment,tier,type,stage,shard,queue,feature_category,urgency,worker) (
        rate(sidekiq_enqueued_jobs_total{}[30m])
      )
    record: gitlab_background_jobs:queue:ops:rate_30m
  - expr: |
      sum by (environment,tier,type,stage,shard,queue,feature_category,urgency,worker) (
        label_replace(sli_aggregations:sidekiq_jobs_completion_seconds_bucket_rate30m{le="10",urgency="high"}, "_c", "0", "", "")
        or
        label_replace(sli_aggregations:sidekiq_jobs_completion_seconds_bucket_rate30m{le="300",urgency="low"}, "_c", "1", "", "")
        or
        label_replace(sli_aggregations:sidekiq_jobs_completion_seconds_bucket_rate30m{le="300",urgency="throttled"}, "_c", "2", "", "")
      )
    record: gitlab_background_jobs:execution:apdex:success:rate_30m
  - expr: |
      sum by (environment,tier,type,stage,shard,queue,feature_category,urgency,worker) (
        label_replace(sli_aggregations:sidekiq_jobs_completion_seconds_bucket_rate30m{le="+Inf",urgency="high"}, "_c", "0", "", "")
        or
        label_replace(sli_aggregations:sidekiq_jobs_completion_seconds_bucket_rate30m{le="+Inf",urgency="low"}, "_c", "1", "", "")
        or
        label_replace(sli_aggregations:sidekiq_jobs_completion_seconds_bucket_rate30m{le="+Inf",urgency="throttled"}, "_c", "2", "", "")
      )
    record: gitlab_background_jobs:execution:apdex:weight:score_30m
  - expr: |
      sum by (environment,tier,type,stage,shard,queue,feature_category,urgency,worker) (
        sli_aggregations:sidekiq_jobs_completion_seconds_bucket_rate30m{le="+Inf"}
      )
    record: gitlab_background_jobs:execution:ops:rate_30m
  - expr: |
      sum by (environment,tier,type,stage,shard,queue,feature_category,urgency,worker) (
        sli_aggregations:sidekiq_jobs_failed_total_rate30m{}
      )
      or
      (
        0 * group by (environment,tier,type,stage,shard,queue,feature_category,urgency,worker) (
          gitlab_background_jobs:execution:ops:rate_30m{}
        )
      )
    record: gitlab_background_jobs:execution:error:rate_30m
- interval: 1m
  name: 'Component-Level SLIs: sidekiq - 1h burn-rate'
  rules:
  - expr: |
      sum by (environment,feature_category,le,queue,shard,stage,tier,type,urgency,worker) (
        rate(sidekiq_jobs_completion_seconds_bucket[1h])
      )
    record: sli_aggregations:sidekiq_jobs_completion_seconds_bucket_rate1h
  - expr: |
      sum by (environment,feature_category,le,queue,shard,stage,tier,type,urgency,worker) (
        rate(sidekiq_jobs_queue_duration_seconds_bucket[1h])
      )
    record: sli_aggregations:sidekiq_jobs_queue_duration_seconds_bucket_rate1h
  - expr: |
      sum by (environment,feature_category,queue,shard,stage,tier,type,urgency,worker) (
        rate(sidekiq_jobs_failed_total[1h])
      )
    record: sli_aggregations:sidekiq_jobs_failed_total_rate1h
  - expr: |
      sum by () (
        rate(sidekiq_jobs_completion_seconds_count{worker=~"EmailReceiverWorker|ServiceDeskEmailReceiverWorker"}[1h])
      )
    labels:
      component: email_receiver
      tier: sv
      type: sidekiq
    record: gitlab_component_ops:rate_1h
  - expr: |
      (
        sum by () (
          rate(gitlab_transaction_event_email_receiver_error_total{error!="Gitlab::Email::AutoGeneratedEmailError"}[1h])
        )
      )
      or
      (
        0 * group by() (
          gitlab_component_ops:rate_1h{component="email_receiver",tier="sv",type="sidekiq"}
        )
      )
    labels:
      component: email_receiver
      tier: sv
      type: sidekiq
    record: gitlab_component_errors:rate_1h
  - expr: |
      sum by () (
        label_replace(sli_aggregations:sidekiq_jobs_completion_seconds_bucket_rate1h{le="10",urgency="high"}, "_c", "0", "", "")
        or
        label_replace(sli_aggregations:sidekiq_jobs_queue_duration_seconds_bucket_rate1h{le="10",urgency="high"}, "_c", "1", "", "")
        or
        label_replace(sli_aggregations:sidekiq_jobs_completion_seconds_bucket_rate1h{le="300",urgency="low"}, "_c", "2", "", "")
        or
        label_replace(sli_aggregations:sidekiq_jobs_queue_duration_seconds_bucket_rate1h{le="60",urgency="low"}, "_c", "3", "", "")
        or
        label_replace(sli_aggregations:sidekiq_jobs_completion_seconds_bucket_rate1h{le="300",urgency="throttled"}, "_c", "4", "", "")
        or
        label_replace(sli_aggregations:sidekiq_jobs_completion_seconds_bucket_rate1h{le="300",urgency=""}, "_c", "5", "", "")
        or
        label_replace(sli_aggregations:sidekiq_jobs_queue_duration_seconds_bucket_rate1h{le="60",urgency=""}, "_c", "6", "", "")
      )
    labels:
      component: shard_catchall
      tier: sv
      type: sidekiq
      upscale_source: "yes"
    record: gitlab_component_apdex:success:rate_1h
  - expr: |
      sum by () (
        label_replace(sli_aggregations:sidekiq_jobs_completion_seconds_bucket_rate1h{le="+Inf",urgency="high"}, "_c", "0", "", "")
        or
        label_replace(sli_aggregations:sidekiq_jobs_queue_duration_seconds_bucket_rate1h{le="+Inf",urgency="high"}, "_c", "1", "", "")
        or
        label_replace(sli_aggregations:sidekiq_jobs_completion_seconds_bucket_rate1h{le="+Inf",urgency="low"}, "_c", "2", "", "")
        or
        label_replace(sli_aggregations:sidekiq_jobs_queue_duration_seconds_bucket_rate1h{le="+Inf",urgency="low"}, "_c", "3", "", "")
        or
        label_replace(sli_aggregations:sidekiq_jobs_completion_seconds_bucket_rate1h{le="+Inf",urgency="throttled"}, "_c", "4", "", "")
        or
        label_replace(sli_aggregations:sidekiq_jobs_completion_seconds_bucket_rate1h{le="+Inf",urgency=""}, "_c", "5", "", "")
        or
        label_replace(sli_aggregations:sidekiq_jobs_queue_duration_seconds_bucket_rate1h{le="+Inf",urgency=""}, "_c", "6", "", "")
      )
    labels:
      component: shard_catchall
      tier: sv
      type: sidekiq
      upscale_source: "yes"
    record: gitlab_component_apdex:weight:score_1h
  - expr: |
      sum by () (
        sli_aggregations:sidekiq_jobs_completion_seconds_bucket_rate1h{le="+Inf",shard="catchall"}
      )
    labels:
      component: shard_catchall
      tier: sv
      type: sidekiq
      upscale_source: "yes"
    record: gitlab_component_ops:rate_1h
  - expr: |
      (
        sum by () (
          sli_aggregations:sidekiq_jobs_failed_total_rate1h{shard="catchall"}
        )
      )
      or
      (
        0 * group by() (
          gitlab_component_ops:rate_1h{component="shard_catchall",tier="sv",type="sidekiq",upscale_source="yes"}
        )
      )
    labels:
      component: shard_catchall
      tier: sv
      type: sidekiq
      upscale_source: "yes"
    record: gitlab_component_errors:rate_1h
  - expr: |
      sum by (environment,tier,type,stage,shard,queue,feature_category,urgency,worker) (
        label_replace(sli_aggregations:sidekiq_jobs_queue_duration_seconds_bucket_rate1h{le="10",urgency="high"}, "_c", "0", "", "")
        or
        label_replace(sli_aggregations:sidekiq_jobs_queue_duration_seconds_bucket_rate1h{le="60",urgency="low"}, "_c", "1", "", "")
      )
    record: gitlab_background_jobs:queue:apdex:success:rate_1h
  - expr: |
      sum by (environment,tier,type,stage,shard,queue,feature_category,urgency,worker) (
        label_replace(sli_aggregations:sidekiq_jobs_queue_duration_seconds_bucket_rate1h{le="+Inf",urgency="high"}, "_c", "0", "", "")
        or
        label_replace(sli_aggregations:sidekiq_jobs_queue_duration_seconds_bucket_rate1h{le="+Inf",urgency="low"}, "_c", "1", "", "")
      )
    record: gitlab_background_jobs:queue:apdex:weight:score_1h
  - expr: |
      sum by (environment,tier,type,stage,shard,queue,feature_category,urgency,worker) (
        rate(sidekiq_enqueued_jobs_total{}[1h])
      )
    record: gitlab_background_jobs:queue:ops:rate_1h
  - expr: |
      sum by (environment,tier,type,stage,shard,queue,feature_category,urgency,worker) (
        label_replace(sli_aggregations:sidekiq_jobs_completion_seconds_bucket_rate1h{le="10",urgency="high"}, "_c", "0", "", "")
        or
        label_replace(sli_aggregations:sidekiq_jobs_completion_seconds_bucket_rate1h{le="300",urgency="low"}, "_c", "1", "", "")
        or
        label_replace(sli_aggregations:sidekiq_jobs_completion_seconds_bucket_rate1h{le="300",urgency="throttled"}, "_c", "2", "", "")
      )
    record: gitlab_background_jobs:execution:apdex:success:rate_1h
  - expr: |
      sum by (environment,tier,type,stage,shard,queue,feature_category,urgency,worker) (
        label_replace(sli_aggregations:sidekiq_jobs_completion_seconds_bucket_rate1h{le="+Inf",urgency="high"}, "_c", "0", "", "")
        or
        label_replace(sli_aggregations:sidekiq_jobs_completion_seconds_bucket_rate1h{le="+Inf",urgency="low"}, "_c", "1", "", "")
        or
        label_replace(sli_aggregations:sidekiq_jobs_completion_seconds_bucket_rate1h{le="+Inf",urgency="throttled"}, "_c", "2", "", "")
      )
    record: gitlab_background_jobs:execution:apdex:weight:score_1h
  - expr: |
      sum by (environment,tier,type,stage,shard,queue,feature_category,urgency,worker) (
        sli_aggregations:sidekiq_jobs_completion_seconds_bucket_rate1h{le="+Inf"}
      )
    record: gitlab_background_jobs:execution:ops:rate_1h
  - expr: |
      sum by (environment,tier,type,stage,shard,queue,feature_category,urgency,worker) (
        sli_aggregations:sidekiq_jobs_failed_total_rate1h{}
      )
      or
      (
        0 * group by (environment,tier,type,stage,shard,queue,feature_category,urgency,worker) (
          gitlab_background_jobs:execution:ops:rate_1h{}
        )
      )
    record: gitlab_background_jobs:execution:error:rate_1h
- interval: 2m
  name: 'Component-Level SLIs: sidekiq - 6h burn-rate'
  rules:
  - expr: |
      sum by () (
        rate(sidekiq_jobs_completion_seconds_count{worker=~"EmailReceiverWorker|ServiceDeskEmailReceiverWorker"}[6h])
      )
    labels:
      component: email_receiver
      tier: sv
      type: sidekiq
    record: gitlab_component_ops:rate_6h
  - expr: |
      (
        sum by () (
          rate(gitlab_transaction_event_email_receiver_error_total{error!="Gitlab::Email::AutoGeneratedEmailError"}[6h])
        )
      )
      or
      (
        0 * group by() (
          gitlab_component_ops:rate_6h{component="email_receiver",tier="sv",type="sidekiq"}
        )
      )
    labels:
      component: email_receiver
      tier: sv
      type: sidekiq
    record: gitlab_component_errors:rate_6h
- interval: 1m
  name: 'Component mapping: sidekiq'
  rules:
  - expr: "1"
    labels:
      component: email_receiver
      regional_aggregation: "no"
      service_aggregation: "yes"
      tier: sv
      type: sidekiq
    record: gitlab_component_service:mapping
  - expr: "1"
    labels:
      component: shard_catchall
      regional_aggregation: "no"
      service_aggregation: "yes"
      tier: sv
      type: sidekiq
    record: gitlab_component_service:mapping
- interval: 1m
  name: Global Service-Aggregated Metrics (fast burn)
  rules:
  - expr: |
      sum by (type) (
        (gitlab_component_errors:rate_5m{} >= 0) and on(component, type) (gitlab_component_service:mapping{service_aggregation="yes"})
      )
    record: gitlab_service_errors:rate_5m
  - expr: |
      sum by (type) (
        (gitlab_component_ops:rate_5m{} >= 0) and on(component, type) (gitlab_component_service:mapping{service_aggregation="yes"})
      )
    record: gitlab_service_ops:rate_5m
  - expr: |
      sum by (type)(
        (gitlab_component_errors:rate_5m{} >= 0) and on(component, type) (gitlab_component_service:mapping{service_aggregation="yes"})
      )
      /
      sum by (type)(
        (gitlab_component_ops:rate_5m{} >= 0) and on(component, type) (gitlab_component_service:mapping{service_aggregation="yes"})
        and
        (gitlab_component_errors:rate_5m{} >= 0) and on(component, type) (gitlab_component_service:mapping{service_aggregation="yes"})
      )
    record: gitlab_service_errors:ratio_5m
  - expr: |
      sum by (type) (
        (gitlab_component_apdex:weight:score_5m{} >= 0) and on(component, type) (gitlab_component_service:mapping{service_aggregation="yes"})
      )
    record: gitlab_service_apdex:weight:score_5m
  - expr: |
      sum by (type) (
        (gitlab_component_apdex:success:rate_5m{} >= 0) and on(component, type) (gitlab_component_service:mapping{service_aggregation="yes"})
      )
    record: gitlab_service_apdex:success:rate_5m
  - expr: |
      sum by (type) (
        (gitlab_component_apdex:success:rate_5m{} >= 0) and on(component, type) (gitlab_component_service:mapping{service_aggregation="yes"})
      )
      /
      sum by (type) (
        (gitlab_component_apdex:weight:score_5m{} >= 0) and on(component, type) (gitlab_component_service:mapping{service_aggregation="yes"})
      )
    record: gitlab_service_apdex:ratio_5m
  - expr: |
      sum by (type) (
        (gitlab_component_errors:rate_1h{} >= 0) and on(component, type) (gitlab_component_service:mapping{service_aggregation="yes"})
      )
    record: gitlab_service_errors:rate_1h
  - expr: |
      sum by (type) (
        (gitlab_component_ops:rate_1h{} >= 0) and on(component, type) (gitlab_component_service:mapping{service_aggregation="yes"})
      )
    record: gitlab_service_ops:rate_1h
  - expr: |
      sum by (type)(
        (gitlab_component_errors:rate_1h{} >= 0) and on(component, type) (gitlab_component_service:mapping{service_aggregation="yes"})
      )
      /
      sum by (type)(
        (gitlab_component_ops:rate_1h{} >= 0) and on(component, type) (gitlab_component_service:mapping{service_aggregation="yes"})
        and
        (gitlab_component_errors:rate_1h{} >= 0) and on(component, type) (gitlab_component_service:mapping{service_aggregation="yes"})
      )
    record: gitlab_service_errors:ratio_1h
  - expr: |
      sum by (type) (
        (gitlab_component_apdex:weight:score_1h{} >= 0) and on(component, type) (gitlab_component_service:mapping{service_aggregation="yes"})
      )
    record: gitlab_service_apdex:weight:score_1h
  - expr: |
      sum by (type) (
        (gitlab_component_apdex:success:rate_1h{} >= 0) and on(component, type) (gitlab_component_service:mapping{service_aggregation="yes"})
      )
    record: gitlab_service_apdex:success:rate_1h
  - expr: |
      sum by (type) (
        (gitlab_component_apdex:success:rate_1h{} >= 0) and on(component, type) (gitlab_component_service:mapping{service_aggregation="yes"})
      )
      /
      sum by (type) (
        (gitlab_component_apdex:weight:score_1h{} >= 0) and on(component, type) (gitlab_component_service:mapping{service_aggregation="yes"})
      )
    record: gitlab_service_apdex:ratio_1h
- interval: 2m
  name: Global Service-Aggregated Metrics (slow burn)
  rules:
  - expr: |
      sum by (type) (
        (gitlab_component_errors:rate_30m{} >= 0) and on(component, type) (gitlab_component_service:mapping{service_aggregation="yes"})
      )
    record: gitlab_service_errors:rate_30m
  - expr: |
      sum by (type) (
        (gitlab_component_ops:rate_30m{} >= 0) and on(component, type) (gitlab_component_service:mapping{service_aggregation="yes"})
      )
    record: gitlab_service_ops:rate_30m
  - expr: |
      sum by (type)(
        (gitlab_component_errors:rate_30m{} >= 0) and on(component, type) (gitlab_component_service:mapping{service_aggregation="yes"})
      )
      /
      sum by (type)(
        (gitlab_component_ops:rate_30m{} >= 0) and on(component, type) (gitlab_component_service:mapping{service_aggregation="yes"})
        and
        (gitlab_component_errors:rate_30m{} >= 0) and on(component, type) (gitlab_component_service:mapping{service_aggregation="yes"})
      )
    record: gitlab_service_errors:ratio_30m
  - expr: |
      sum by (type) (
        (gitlab_component_apdex:weight:score_30m{} >= 0) and on(component, type) (gitlab_component_service:mapping{service_aggregation="yes"})
      )
    record: gitlab_service_apdex:weight:score_30m
  - expr: |
      sum by (type) (
        (gitlab_component_apdex:success:rate_30m{} >= 0) and on(component, type) (gitlab_component_service:mapping{service_aggregation="yes"})
      )
    record: gitlab_service_apdex:success:rate_30m
  - expr: |
      sum by (type) (
        (gitlab_component_apdex:success:rate_30m{} >= 0) and on(component, type) (gitlab_component_service:mapping{service_aggregation="yes"})
      )
      /
      sum by (type) (
        (gitlab_component_apdex:weight:score_30m{} >= 0) and on(component, type) (gitlab_component_service:mapping{service_aggregation="yes"})
      )
    record: gitlab_service_apdex:ratio_30m
  - expr: |
      (
        sum by (type) (
          (gitlab_component_errors:rate_6h{} >= 0) and on(component, type) (gitlab_component_service:mapping{service_aggregation="yes"})
        )
      )
      or
      (
        sum by (type) (
          avg_over_time(gitlab_component_errors:rate_1h{upscale_source="yes"}[6h]) and on(component, type) (gitlab_component_service:mapping{service_aggregation="yes"})
        )
      )
    record: gitlab_service_errors:rate_6h
  - expr: |
      (
        sum by (type) (
          (gitlab_component_ops:rate_6h{} >= 0) and on(component, type) (gitlab_component_service:mapping{service_aggregation="yes"})
        )
      )
      or
      (
        sum by (type) (
          avg_over_time(gitlab_component_ops:rate_1h{upscale_source="yes"}[6h]) and on(component, type) (gitlab_component_service:mapping{service_aggregation="yes"})
        )
      )
    record: gitlab_service_ops:rate_6h
  - expr: |
      (
        sum by (type)(
          (gitlab_component_errors:rate_6h{} >= 0) and on(component, type) (gitlab_component_service:mapping{service_aggregation="yes"})
        )
        /
        sum by (type)(
          (gitlab_component_ops:rate_6h{} >= 0) and on(component, type) (gitlab_component_service:mapping{service_aggregation="yes"})
          and
          (gitlab_component_errors:rate_6h{} >= 0) and on(component, type) (gitlab_component_service:mapping{service_aggregation="yes"})
        )
      )
      or
      (
        sum by (type) (
          sum_over_time(gitlab_component_errors:rate_1h{upscale_source="yes"}[6h]) and on(component, type) (gitlab_component_service:mapping{service_aggregation="yes"})
        )
        /
        sum by (type) (
          sum_over_time(gitlab_component_ops:rate_1h{upscale_source="yes"}[6h]) and on(component, type) (gitlab_component_service:mapping{service_aggregation="yes"})
          and
          gitlab_component_errors:rate_1h{upscale_source="yes"} >= 0
        )
      )
    record: gitlab_service_errors:ratio_6h
  - expr: |
      (
        sum by (type) (
          (gitlab_component_apdex:weight:score_6h{} >= 0) and on(component, type) (gitlab_component_service:mapping{service_aggregation="yes"})
        )
      )
      or
      (
        sum by (type) (
          avg_over_time(gitlab_component_apdex:weight:score_1h{upscale_source="yes"}[6h]) and on(component, type) (gitlab_component_service:mapping{service_aggregation="yes"})
        )
      )
    record: gitlab_service_apdex:weight:score_6h
  - expr: |
      (
        sum by (type) (
          (gitlab_component_apdex:success:rate_6h{} >= 0) and on(component, type) (gitlab_component_service:mapping{service_aggregation="yes"})
        )
      )
      or
      (
        sum by (type) (
          avg_over_time(gitlab_component_apdex:success:rate_1h{upscale_source="yes"}[6h]) and on(component, type) (gitlab_component_service:mapping{service_aggregation="yes"})
        )
      )
    record: gitlab_service_apdex:success:rate_6h
  - expr: |
      (
        sum by (type) (
          (gitlab_component_apdex:success:rate_6h{} >= 0) and on(component, type) (gitlab_component_service:mapping{service_aggregation="yes"})
        )
        /
        sum by (type) (
          (gitlab_component_apdex:weight:score_6h{} >= 0) and on(component, type) (gitlab_component_service:mapping{service_aggregation="yes"})
        )
      )
      or
      (
        sum by (type) (
          sum_over_time(gitlab_component_apdex:success:rate_1h{upscale_source="yes"}[6h]) and on(component, type) (gitlab_component_service:mapping{service_aggregation="yes"})
        )
        /
        sum by (type) (
          sum_over_time(gitlab_component_apdex:weight:score_1h{upscale_source="yes"}[6h]) and on(component, type) (gitlab_component_service:mapping{service_aggregation="yes"})
        )
      )
    record: gitlab_service_apdex:ratio_6h
- interval: 1m
  name: Global Component SLI Metrics (fast burn)
  rules:
  - expr: |
      sum by (type,component) (
        gitlab_component_errors:rate_5m{}
      )
      /
      sum by (type,component) (
        gitlab_component_ops:rate_5m{}
      )
    record: gitlab_component_errors:ratio_5m
  - expr: |
      sum by (type,component) (
        gitlab_component_apdex:success:rate_5m{}
      )
      /
      sum by (type,component) (
        gitlab_component_apdex:weight:score_5m{}
      )
    record: gitlab_component_apdex:ratio_5m
  - expr: |
      sum by (type,component) (
        gitlab_component_errors:rate_1h{}
      )
      /
      sum by (type,component) (
        gitlab_component_ops:rate_1h{}
      )
    record: gitlab_component_errors:ratio_1h
  - expr: |
      sum by (type,component) (
        gitlab_component_apdex:success:rate_1h{}
      )
      /
      sum by (type,component) (
        gitlab_component_apdex:weight:score_1h{}
      )
    record: gitlab_component_apdex:ratio_1h
- interval: 2m
  name: Global Component SLI Metrics (slow burn)
  rules:
  - expr: |
      sum by (type,component) (
        gitlab_component_errors:rate_30m{}
      )
      /
      sum by (type,component) (
        gitlab_component_ops:rate_30m{}
      )
    record: gitlab_component_errors:ratio_30m
  - expr: |
      sum by (type,component) (
        gitlab_component_apdex:success:rate_30m{}
      )
      /
      sum by (type,component) (
        gitlab_component_apdex:weight:score_30m{}
      )
    record: gitlab_component_apdex:ratio_30m
  - expr: |
      sum by (type,component) (
        gitlab_component_errors:rate_6h{}
      )
      /
      sum by (type,component) (
        gitlab_component_ops:rate_6h{}
      )
    record: gitlab_component_errors:ratio_6h
  - expr: |
      sum by (type,component) (
        gitlab_component_apdex:success:rate_6h{}
      )
      /
      sum by (type,component) (
        gitlab_component_apdex:weight:score_6h{}
      )
    record: gitlab_component_apdex:ratio_6h
- interval: 5m
  name: Autogenerated Service SLOs
  rules:
  - expr: "0.998000"
    labels:
      tier: app
      type: webservice
    record: slo:min:events:gitlab_service_apdex:ratio
  - expr: "0.000100"
    labels:
      tier: app
      type: webservice
    record: slo:max:events:gitlab_service_errors:ratio
  - expr: "0.998000"
    labels:
      component: puma
      tier: app
      type: webservice
    record: slo:min:events:gitlab_service_apdex:ratio
  - expr: "0.000100"
    labels:
      component: puma
      tier: app
      type: webservice
    record: slo:max:events:gitlab_service_errors:ratio
  - expr: "0.998000"
    labels:
      component: workhorse
      tier: app
      type: webservice
    record: slo:min:events:gitlab_service_apdex:ratio
  - expr: "0.000100"
    labels:
      component: workhorse
      tier: app
      type: webservice
    record: slo:max:events:gitlab_service_errors:ratio
  - expr: "0.999000"
    labels:
      tier: stor
      type: gitaly
    record: slo:min:events:gitlab_service_apdex:ratio
  - expr: "0.000500"
    labels:
      tier: stor
      type: gitaly
    record: slo:max:events:gitlab_service_errors:ratio
  - expr: "0.999000"
    labels:
      component: goserver
      tier: stor
      type: gitaly
    record: slo:min:events:gitlab_service_apdex:ratio
  - expr: "0.000500"
    labels:
      component: goserver
      tier: stor
      type: gitaly
    record: slo:max:events:gitlab_service_errors:ratio
  - expr: "0.995000"
    labels:
      tier: stor
      type: praefect
    record: slo:min:events:gitlab_service_apdex:ratio
  - expr: "0.000500"
    labels:
      tier: stor
      type: praefect
    record: slo:max:events:gitlab_service_errors:ratio
  - expr: "0.995000"
    labels:
      component: proxy
      tier: stor
      type: praefect
    record: slo:min:events:gitlab_service_apdex:ratio
  - expr: "0.000500"
    labels:
      component: proxy
      tier: stor
      type: praefect
    record: slo:max:events:gitlab_service_errors:ratio
  - expr: "0.995000"
    labels:
      component: replicator_queue
      tier: stor
      type: praefect
    record: slo:min:events:gitlab_service_apdex:ratio
  - expr: "0.000500"
    labels:
      component: replicator_queue
      tier: stor
      type: praefect
    record: slo:max:events:gitlab_service_errors:ratio
  - expr: "0.995000"
    labels:
      tier: sv
      type: sidekiq
    record: slo:min:events:gitlab_service_apdex:ratio
  - expr: "0.005000"
    labels:
      tier: sv
      type: sidekiq
    record: slo:max:events:gitlab_service_errors:ratio
  - expr: "0.995000"
    labels:
      component: email_receiver
      tier: sv
      type: sidekiq
    record: slo:min:events:gitlab_service_apdex:ratio
  - expr: "0.300000"
    labels:
      component: email_receiver
      tier: sv
      type: sidekiq
    record: slo:max:events:gitlab_service_errors:ratio
  - expr: "0.995000"
    labels:
      component: shard_catchall
      tier: sv
      type: sidekiq
    record: slo:min:events:gitlab_service_apdex:ratio
  - expr: "0.005000"
    labels:
      component: shard_catchall
      tier: sv
      type: sidekiq
    record: slo:max:events:gitlab_service_errors:ratio
  - expr: "0.995000"
    labels:
      tier: sv
      type: sidekiq
    record: slo:min:deployment:gitlab_service_apdex:ratio
  - expr: "0.005000"
    labels:
      tier: sv
      type: sidekiq
    record: slo:max:deployment:gitlab_service_errors:ratio
- interval: 1m
  name: Saturation Rules (autogenerated)
  rules:
  - expr: |
      max by(type) (
        clamp_min(
          clamp_max(
            1 - avg by (type) (
              rate(node_cpu_seconds_total{mode="idle", type=~"gitaly|consul|praefect"}[5m])
            )
            ,
            1)
        ,
        0)
      )
    labels:
      component: cpu
    record: gitlab_component_saturation:ratio
  - expr: |
      max by(type) (
        clamp_min(
          clamp_max(
            1 - (
              node_filesystem_files_free{fstype=~"(ext.|xfs)", type=~"gitaly|consul|praefect"}
              /
              node_filesystem_files{fstype=~"(ext.|xfs)", type=~"gitaly|consul|praefect"}
            )
            ,
            1)
        ,
        0)
      )
    labels:
      component: disk_inodes
    record: gitlab_component_saturation:ratio
  - expr: |
      max by(type) (
        clamp_min(
          clamp_max(
            (
              1 - node_filesystem_avail_bytes{fstype=~"ext.|xfs", type=~"gitaly|consul|praefect"} / node_filesystem_size_bytes{fstype=~"ext.|xfs", type=~"gitaly|consul|praefect"}
            )
            ,
            1)
        ,
        0)
      )
    labels:
      component: disk_space
    record: gitlab_component_saturation:ratio
  - expr: |
      max by(type) (
        clamp_min(
          clamp_max(
            sum by (type, fqdn) (
              go_memstats_alloc_bytes{type=~"gitaly|praefect"}
            )
            /
            sum by (type, fqdn) (
              node_memory_MemTotal_bytes{type=~"gitaly|praefect"}
            )
            ,
            1)
        ,
        0)
      )
    labels:
      component: go_memory
    record: gitlab_component_saturation:ratio
  - expr: |
      max by(type) (
        clamp_min(
          clamp_max(
            sum by (type, pod, container) (
              rate(container_cpu_usage_seconds_total:labeled{container!="", container!="POD", type=~"consul|webservice|sidekiq"}[5m])
            )
            /
            sum by(type, pod, container) (
              container_spec_cpu_quota:labeled{container!="", container!="POD", type=~"consul|webservice|sidekiq"}
              /
              container_spec_cpu_period:labeled{container!="", container!="POD", type=~"consul|webservice|sidekiq"}
            )
            ,
            1)
        ,
        0)
      )
    labels:
      component: kube_container_cpu
    record: gitlab_component_saturation:ratio
  - expr: |
      max by(type) (
        clamp_min(
          clamp_max(
            container_memory_working_set_bytes:labeled{container!="", container!="POD", type=~"consul|webservice|sidekiq"}
            /
            (container_spec_memory_limit_bytes:labeled{container!="", container!="POD", type=~"consul|webservice|sidekiq"} > 0)
            ,
            1)
        ,
        0)
      )
    labels:
      component: kube_container_memory
    record: gitlab_component_saturation:ratio
  - expr: |
      max by(type) (
        clamp_min(
          clamp_max(
            1 - avg by (type) (
              rate(node_cpu_seconds_total:labeled{mode="idle", type=~"webservice|sidekiq"}[5m])
            )
            ,
            1)
        ,
        0)
      )
    labels:
      component: kube_pool_cpu
    record: gitlab_component_saturation:ratio
  - expr: |
      max by(type) (
        clamp_min(
          clamp_max(
            instance:node_memory_utilization:ratio{type=~"gitaly|consul|praefect"} or instance:node_memory_utilisation:ratio{type=~"gitaly|consul|praefect"}
            ,
            1)
        ,
        0)
      )
    labels:
      component: memory
    record: gitlab_component_saturation:ratio
  - expr: |
      max by(type) (
        clamp_min(
          clamp_max(
            avg without (cpu) (rate(node_schedstat_waiting_seconds_total{type=~"consul|gitaly|praefect"}[1h]))
            ,
            1)
        ,
        0)
      )
    labels:
      component: node_schedstat_waiting
    record: gitlab_component_saturation:ratio
  - expr: |
      max by(type) (
        clamp_min(
          clamp_max(
            avg without(cpu, mode) (1 - rate(node_cpu_seconds_total{mode="idle", type=~"gitaly|consul|praefect"}[5m]))
            ,
            1)
        ,
        0)
      )
    labels:
      component: single_node_cpu
    record: gitlab_component_saturation:ratio
- interval: 5m
  name: GitLab Component Saturation Max SLOs
  rules:
  - expr: "0.8"
    labels:
      component: cpu
    record: slo:max:soft:gitlab_component_saturation:ratio
  - expr: "0.9"
    labels:
      component: cpu
    record: slo:max:hard:gitlab_component_saturation:ratio
  - expr: "0.75"
    labels:
      component: disk_inodes
    record: slo:max:soft:gitlab_component_saturation:ratio
  - expr: "0.8"
    labels:
      component: disk_inodes
    record: slo:max:hard:gitlab_component_saturation:ratio
  - expr: "0.85"
    labels:
      component: disk_space
    record: slo:max:soft:gitlab_component_saturation:ratio
  - expr: "0.9"
    labels:
      component: disk_space
    record: slo:max:hard:gitlab_component_saturation:ratio
  - expr: "0.9"
    labels:
      component: go_memory
    record: slo:max:soft:gitlab_component_saturation:ratio
  - expr: "0.98"
    labels:
      component: go_memory
    record: slo:max:hard:gitlab_component_saturation:ratio
  - expr: "0.9"
    labels:
      component: kube_container_cpu
    record: slo:max:soft:gitlab_component_saturation:ratio
  - expr: "0.99"
    labels:
      component: kube_container_cpu
    record: slo:max:hard:gitlab_component_saturation:ratio
  - expr: "0.9"
    labels:
      component: kube_container_memory
    record: slo:max:soft:gitlab_component_saturation:ratio
  - expr: "0.99"
    labels:
      component: kube_container_memory
    record: slo:max:hard:gitlab_component_saturation:ratio
  - expr: "0.8"
    labels:
      component: kube_pool_cpu
    record: slo:max:soft:gitlab_component_saturation:ratio
  - expr: "0.9"
    labels:
      component: kube_pool_cpu
    record: slo:max:hard:gitlab_component_saturation:ratio
  - expr: "0.9"
    labels:
      component: memory
    record: slo:max:soft:gitlab_component_saturation:ratio
  - expr: "0.98"
    labels:
      component: memory
    record: slo:max:hard:gitlab_component_saturation:ratio
  - expr: "0.1"
    labels:
      component: node_schedstat_waiting
    record: slo:max:soft:gitlab_component_saturation:ratio
  - expr: "0.15"
    labels:
      component: node_schedstat_waiting
    record: slo:max:hard:gitlab_component_saturation:ratio
  - expr: "0.9"
    labels:
      component: single_node_cpu
    record: slo:max:soft:gitlab_component_saturation:ratio
  - expr: "0.95"
    labels:
      component: single_node_cpu
    record: slo:max:hard:gitlab_component_saturation:ratio
- interval: 5m
  name: GitLab Component Saturation Metadata
  rules:
  - expr: "1"
    labels:
      capacity_planning_strategy: quantile95_1h
      component: cpu
      horiz_scaling: "yes"
      quantile: max
      severity: s3
    record: gitlab_component_saturation_info
  - expr: "1"
    labels:
      capacity_planning_strategy: quantile95_1h
      component: disk_inodes
      horiz_scaling: "yes"
      quantile: max
      severity: s2
    record: gitlab_component_saturation_info
  - expr: "1"
    labels:
      capacity_planning_strategy: quantile95_1h
      component: disk_space
      horiz_scaling: "yes"
      quantile: max
      severity: s2
    record: gitlab_component_saturation_info
  - expr: "1"
    labels:
      capacity_planning_strategy: quantile95_1h
      component: go_memory
      horiz_scaling: "yes"
      quantile: max
      severity: s4
    record: gitlab_component_saturation_info
  - expr: "1"
    labels:
      capacity_planning_strategy: quantile95_1h
      component: kube_container_cpu
      horiz_scaling: "yes"
      quantile: max
      severity: s4
    record: gitlab_component_saturation_info
  - expr: "1"
    labels:
      capacity_planning_strategy: quantile95_1h
      component: kube_container_memory
      horiz_scaling: "yes"
      quantile: max
      severity: s4
    record: gitlab_component_saturation_info
  - expr: "1"
    labels:
      capacity_planning_strategy: quantile95_1h
      component: kube_pool_cpu
      horiz_scaling: "yes"
      quantile: max
      severity: s3
    record: gitlab_component_saturation_info
  - expr: "1"
    labels:
      capacity_planning_strategy: quantile95_1h
      component: memory
      horiz_scaling: "yes"
      quantile: max
      severity: s4
    record: gitlab_component_saturation_info
  - expr: "1"
    labels:
      capacity_planning_strategy: quantile95_1h
      component: node_schedstat_waiting
      horiz_scaling: "yes"
      quantile: max
      severity: s4
    record: gitlab_component_saturation_info
  - expr: "1"
    labels:
      capacity_planning_strategy: quantile95_1h
      component: single_node_cpu
      horiz_scaling: "yes"
      quantile: max
      severity: s4
    record: gitlab_component_saturation_info
- interval: 5m
  name: GitLab Component Saturation Statistics
  rules:
  - expr: quantile_over_time(0.95, gitlab_component_saturation:ratio{monitor="global"}[1w])
    record: gitlab_component_saturation:ratio_quantile95_1w
  - expr: quantile_over_time(0.99, gitlab_component_saturation:ratio{monitor="global"}[1w])
    record: gitlab_component_saturation:ratio_quantile99_1w
  - expr: quantile_over_time(0.95, gitlab_component_saturation:ratio{monitor="global"}[1h])
    record: gitlab_component_saturation:ratio_quantile95_1h
  - expr: quantile_over_time(0.99, gitlab_component_saturation:ratio{monitor="global"}[1h])
    record: gitlab_component_saturation:ratio_quantile99_1h
  - expr: avg_over_time(gitlab_component_saturation:ratio{monitor="global"}[1h])
    record: gitlab_component_saturation:ratio_avg_1h
- interval: 1m
  name: GitLab Saturation Alerts
  rules:
  - alert: component_saturation_slo_out_of_bounds
    annotations:
      description: |+
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Average Service CPU Utilization resource:

        This resource measures average CPU utilization across an all cores in a service fleet. If it is becoming saturated, it may indicate that the fleet needs horizontal or vertical scaling.

      grafana_dashboard_id: alerts-sat_cpu
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_cpu?from=now-6h/m&to=now-1m/m&var-type={{
        $labels.type }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1465724101"
      grafana_variables: type
      promql_query: |
        max by(type) (
          clamp_min(
            clamp_max(
              1 - avg by (type) (
                rate(node_cpu_seconds_total{mode="idle", type="{{ $labels.type }}"}[5m])
              )
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
      title: The Average Service CPU Utilization resource of the {{ $labels.type }}
        service has a saturation exceeding SLO and is close to its capacity limit.
    expr: |
      gitlab_component_saturation:ratio{component="cpu",monitor="global"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="cpu",monitor="global"}
    for: 5m
    labels:
      alert_type: cause
      rules_domain: general
      severity: s3
  - alert: component_saturation_slo_out_of_bounds
    annotations:
      description: |+
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Disk inode Utilization per Device per Node resource:

        Disk inode utilization per device per node.

        If this is too high, its possible that a directory is filling up with files. Consider logging in an checking temp directories for large numbers of files

      grafana_dashboard_id: alerts-sat_disk_inodes
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_disk_inodes?from=now-6h/m&to=now-1m/m&var-type={{
        $labels.type }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "39965907"
      grafana_variables: type
      promql_query: |
        max by(type, fqdn, device) (
          clamp_min(
            clamp_max(
              1 - (
                node_filesystem_files_free{fstype=~"(ext.|xfs)", type="{{ $labels.type }}"}
                /
                node_filesystem_files{fstype=~"(ext.|xfs)", type="{{ $labels.type }}"}
              )
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
      title: The Disk inode Utilization per Device per Node resource of the {{ $labels.type
        }} service has a saturation exceeding SLO and is close to its capacity limit.
    expr: |
      gitlab_component_saturation:ratio{component="disk_inodes",monitor="global"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="disk_inodes",monitor="global"}
    for: 15m
    labels:
      alert_type: cause
      pager: pagerduty
      rules_domain: general
      severity: s2
  - alert: component_saturation_slo_out_of_bounds
    annotations:
      description: |+
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Disk Space Utilization per Device per Node resource:

        Disk space utilization per device per node.

      grafana_dashboard_id: alerts-sat_disk_space
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_disk_space?from=now-6h/m&to=now-1m/m&var-type={{
        $labels.type }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2661375984"
      grafana_variables: type
      promql_query: |
        max by(type, node, device) (
          clamp_min(
            clamp_max(
              (
                1 - node_filesystem_avail_bytes{fstype=~"ext.|xfs", type="{{ $labels.type }}"} / node_filesystem_size_bytes{fstype=~"ext.|xfs", type="{{ $labels.type }}"}
              )
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
      title: The Disk Space Utilization per Device per Node resource of the {{ $labels.type
        }} service has a saturation exceeding SLO and is close to its capacity limit.
    expr: |
      gitlab_component_saturation:ratio{component="disk_space",monitor="global"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="disk_space",monitor="global"}
    for: 15m
    labels:
      alert_type: cause
      pager: pagerduty
      rules_domain: general
      severity: s2
  - alert: component_saturation_slo_out_of_bounds
    annotations:
      description: |+
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Go Memory Utilization per Node resource:

        Go's memory allocation strategy can make it look like a Go process is saturating memory when measured using RSS, when in fact the process is not at risk of memory saturation. For this reason, we measure Go processes using the `go_memstat_alloc_bytes` metric instead of RSS.

      grafana_dashboard_id: alerts-sat_go_memory
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_go_memory?from=now-6h/m&to=now-1m/m&var-type={{
        $labels.type }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3631721613"
      grafana_variables: type
      promql_query: |
        max by(type, fqdn) (
          clamp_min(
            clamp_max(
              sum by (type, fqdn) (
                go_memstats_alloc_bytes{type="{{ $labels.type }}"}
              )
              /
              sum by (type, fqdn) (
                node_memory_MemTotal_bytes{type="{{ $labels.type }}"}
              )
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
      title: The Go Memory Utilization per Node resource of the {{ $labels.type }}
        service has a saturation exceeding SLO and is close to its capacity limit.
    expr: |
      gitlab_component_saturation:ratio{component="go_memory",monitor="global"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="go_memory",monitor="global"}
    for: 5m
    labels:
      alert_type: cause
      rules_domain: general
      severity: s4
  - alert: component_saturation_slo_out_of_bounds
    annotations:
      description: |+
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Kube Container CPU Utilization resource:

        Kubernetes containers are allocated a share of CPU. When this is exhausted, the container may be thottled.

      grafana_dashboard_id: alerts-sat_kube_container_cpu
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_kube_container_cpu?from=now-6h/m&to=now-1m/m&var-type={{
        $labels.type }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2713861591"
      grafana_variables: type
      promql_query: |
        max by(type, pod, container) (
          clamp_min(
            clamp_max(
              sum by (type, pod, container) (
                rate(container_cpu_usage_seconds_total:labeled{container!="", container!="POD", type="{{ $labels.type }}"}[5m])
              )
              /
              sum by(type, pod, container) (
                container_spec_cpu_quota:labeled{container!="", container!="POD", type="{{ $labels.type }}"}
                /
                container_spec_cpu_period:labeled{container!="", container!="POD", type="{{ $labels.type }}"}
              )
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
      title: The Kube Container CPU Utilization resource of the {{ $labels.type }}
        service has a saturation exceeding SLO and is close to its capacity limit.
    expr: |
      gitlab_component_saturation:ratio{component="kube_container_cpu",monitor="global"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="kube_container_cpu",monitor="global"}
    for: 15m
    labels:
      alert_type: cause
      rules_domain: general
      severity: s4
  - alert: component_saturation_slo_out_of_bounds
    annotations:
      description: |+
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Kube Container Memory Utilization resource:

        Records the total memory utilization for containers for this service, as a percentage of the memory limit as configured through Kubernetes.

      grafana_dashboard_id: alerts-sat_kube_container_memory
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_kube_container_memory?from=now-6h/m&to=now-1m/m&var-type={{
        $labels.type }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "172578411"
      grafana_variables: type
      promql_query: |
        max by(type, pod, container) (
          clamp_min(
            clamp_max(
              container_memory_working_set_bytes:labeled{container!="", container!="POD", type="{{ $labels.type }}"}
              /
              (container_spec_memory_limit_bytes:labeled{container!="", container!="POD", type="{{ $labels.type }}"} > 0)
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
      title: The Kube Container Memory Utilization resource of the {{ $labels.type
        }} service has a saturation exceeding SLO and is close to its capacity limit.
    expr: |
      gitlab_component_saturation:ratio{component="kube_container_memory",monitor="global"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="kube_container_memory",monitor="global"}
    for: 15m
    labels:
      alert_type: cause
      rules_domain: general
      severity: s4
  - alert: component_saturation_slo_out_of_bounds
    annotations:
      description: |+
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Average Node Pool CPU Utilization resource:

        This resource measures average CPU utilization across an all cores in the node pool for a service fleet.

        If it is becoming saturated, it may indicate that the fleet needs horizontal scaling.

      grafana_dashboard_id: alerts-sat_kube_pool_cpu
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_kube_pool_cpu?from=now-6h/m&to=now-1m/m&var-type={{
        $labels.type }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1839360107"
      grafana_variables: type
      promql_query: |
        max by(type) (
          clamp_min(
            clamp_max(
              1 - avg by (type) (
                rate(node_cpu_seconds_total:labeled{mode="idle", type="{{ $labels.type }}"}[5m])
              )
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
      title: The Average Node Pool CPU Utilization resource of the {{ $labels.type
        }} service has a saturation exceeding SLO and is close to its capacity limit.
    expr: |
      gitlab_component_saturation:ratio{component="kube_pool_cpu",monitor="global"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="kube_pool_cpu",monitor="global"}
    for: 5m
    labels:
      alert_type: cause
      rules_domain: general
      severity: s3
  - alert: component_saturation_slo_out_of_bounds
    annotations:
      description: |+
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Memory Utilization per Node resource:

        Memory utilization per device per node.

      grafana_dashboard_id: alerts-sat_memory
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_memory?from=now-6h/m&to=now-1m/m&var-type={{
        $labels.type }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1955556769"
      grafana_variables: type
      promql_query: |
        max by(type, node) (
          clamp_min(
            clamp_max(
              instance:node_memory_utilization:ratio{type="{{ $labels.type }}"} or instance:node_memory_utilisation:ratio{type="{{ $labels.type }}"}
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
      title: The Memory Utilization per Node resource of the {{ $labels.type }} service
        has a saturation exceeding SLO and is close to its capacity limit.
    expr: |
      gitlab_component_saturation:ratio{component="memory",monitor="global"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="memory",monitor="global"}
    for: 5m
    labels:
      alert_type: cause
      rules_domain: general
      severity: s4
  - alert: component_saturation_slo_out_of_bounds
    annotations:
      description: |+
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Node Scheduler Waiting Time resource:

        Measures the amount of scheduler waiting time that processes are waiting to be scheduled, according to [`CPU Scheduling Metrics`](https://www.robustperception.io/cpu-scheduling-metrics-from-the-node-exporter).

        A high value indicates that a node has more processes to be run than CPU time available to handle them, and may lead to degraded responsiveness and performance from the application.

        Additionally, it may indicate that the fleet is under-provisioned.

      grafana_dashboard_id: alerts-sat_node_schedstat_waiting
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_node_schedstat_waiting?from=now-6h/m&to=now-1m/m&var-type={{
        $labels.type }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1415313189"
      grafana_variables: type
      promql_query: |
        max by(type, fqdn, shard) (
          clamp_min(
            clamp_max(
              avg without (cpu) (rate(node_schedstat_waiting_seconds_total{type="{{ $labels.type }}"}[1h]))
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
      title: The Node Scheduler Waiting Time resource of the {{ $labels.type }} service
        has a saturation exceeding SLO and is close to its capacity limit.
    expr: |
      gitlab_component_saturation:ratio{component="node_schedstat_waiting",monitor="global"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="node_schedstat_waiting",monitor="global"}
    for: 90m
    labels:
      alert_type: cause
      rules_domain: general
      severity: s4
  - alert: component_saturation_slo_out_of_bounds
    annotations:
      description: |+
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Average CPU Utilization per Node resource:

        Average CPU utilization per Node.

        If average CPU is saturated, it may indicate that a fleet is in need to horizontal or vertical scaling. It may also indicate imbalances in load in a fleet.

      grafana_dashboard_id: alerts-sat_single_node_cpu
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_single_node_cpu?from=now-6h/m&to=now-1m/m&var-type={{
        $labels.type }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3372411356"
      grafana_variables: type
      promql_query: |
        max by(type, node) (
          clamp_min(
            clamp_max(
              avg without(cpu, mode) (1 - rate(node_cpu_seconds_total{mode="idle", type="{{ $labels.type }}"}[5m]))
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
      title: The Average CPU Utilization per Node resource of the {{ $labels.type
        }} service has a saturation exceeding SLO and is close to its capacity limit.
    expr: |
      gitlab_component_saturation:ratio{component="single_node_cpu",monitor="global"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="single_node_cpu",monitor="global"}
    for: 10m
    labels:
      alert_type: cause
      rules_domain: general
      severity: s4
- interval: 1m
  name: 'kube-state-metrics-recording-rules: consul'
  rules:
  - expr: |
      group without(label_deployment) (
        label_replace(
          topk by(cluster,pod) (1, kube_pod_labels{namespace="consul"}),
          "deployment", "$0", "label_deployment", ".*"
        )
      )
    labels:
      tier: sv
      type: consul
    record: kube_pod_labels:labeled
- interval: 1m
  name: 'kube-state-metrics-recording-rules: webservice'
  rules:
  - expr: |
      group without(label_deployment) (
        label_replace(
          topk by(cluster,pod) (1, kube_pod_labels{label_app="webservice"}),
          "deployment", "$0", "label_deployment", ".*"
        )
      )
    labels:
      tier: app
      type: webservice
    record: kube_pod_labels:labeled
  - expr: topk by(cluster,horizontalpodautoscaler) (1, kube_horizontalpodautoscaler_labels{label_app="webservice"})
    labels:
      tier: app
      type: webservice
    record: kube_horizontalpodautoscaler_labels:labeled
  - expr: topk by(cluster,node) (1, kube_node_labels{label_eks_amazonaws_com_nodegroup="gitlab_webservice_pool"})
    labels:
      tier: app
      type: webservice
    record: kube_node_labels:labeled
  - expr: topk by(cluster,ingress) (1, kube_ingress_labels{label_app="webservice"})
    labels:
      tier: app
      type: webservice
    record: kube_ingress_labels:labeled
  - expr: topk by(cluster,deployment) (1, kube_deployment_labels{label_app="webservice"})
    labels:
      tier: app
      type: webservice
    record: kube_deployment_labels:labeled
- interval: 1m
  name: 'kube-state-metrics-recording-rules: sidekiq'
  rules:
  - expr: |
      group without(label_deployment) (
        label_replace(
          topk by(cluster,pod) (1, kube_pod_labels{label_type="sidekiq"}),
          "deployment", "$0", "label_deployment", ".*"
        )
      )
    labels:
      stage: catchall
      tier: sv
      type: sidekiq
    record: kube_pod_labels:labeled
  - expr: topk by(cluster,horizontalpodautoscaler) (1, kube_horizontalpodautoscaler_labels{label_type="sidekiq"})
    labels:
      tier: sv
      type: sidekiq
    record: kube_horizontalpodautoscaler_labels:labeled
  - expr: topk by(cluster,node) (1, kube_node_labels{label_type=~"catchall"})
    labels:
      stage: main
      tier: sv
      type: sidekiq
    record: kube_node_labels:labeled
  - expr: topk by(cluster,deployment) (1, kube_deployment_labels{label_type="sidekiq"})
    labels:
      tier: sv
      type: sidekiq
    record: kube_deployment_labels:labeled
- interval: 1m
  name: 'kube-state-metrics-recording-rules: enriched label recording rules'
  rules:
  - expr: |
      container_start_time_seconds{metrics_path="/metrics/cadvisor"}
      *
      on(cluster,pod) group_left(type,deployment)
      topk by (cluster,pod) (1, kube_pod_labels:labeled{type!=""})
    record: container_start_time_seconds:labeled
  - expr: |
      container_cpu_cfs_periods_total{metrics_path="/metrics/cadvisor"}
      *
      on(cluster,pod) group_left(type,deployment)
      topk by (cluster,pod) (1, kube_pod_labels:labeled{type!=""})
    record: container_cpu_cfs_periods_total:labeled
  - expr: |
      container_cpu_cfs_throttled_periods_total{metrics_path="/metrics/cadvisor"}
      *
      on(cluster,pod) group_left(type,deployment)
      topk by (cluster,pod) (1, kube_pod_labels:labeled{type!=""})
    record: container_cpu_cfs_throttled_periods_total:labeled
  - expr: |
      container_cpu_usage_seconds_total{metrics_path="/metrics/cadvisor"}
      *
      on(cluster,pod) group_left(type,deployment)
      topk by (cluster,pod) (1, kube_pod_labels:labeled{type!=""})
    record: container_cpu_usage_seconds_total:labeled
  - expr: |
      container_memory_cache{metrics_path="/metrics/cadvisor"}
      *
      on(cluster,pod) group_left(type,deployment)
      topk by (cluster,pod) (1, kube_pod_labels:labeled{type!=""})
    record: container_memory_cache:labeled
  - expr: |
      container_memory_swap{metrics_path="/metrics/cadvisor"}
      *
      on(cluster,pod) group_left(type,deployment)
      topk by (cluster,pod) (1, kube_pod_labels:labeled{type!=""})
    record: container_memory_swap:labeled
  - expr: |
      container_memory_usage_bytes{metrics_path="/metrics/cadvisor"}
      *
      on(cluster,pod) group_left(type,deployment)
      topk by (cluster,pod) (1, kube_pod_labels:labeled{type!=""})
    record: container_memory_usage_bytes:labeled
  - expr: |
      container_memory_working_set_bytes{metrics_path="/metrics/cadvisor"}
      *
      on(cluster,pod) group_left(type,deployment)
      topk by (cluster,pod) (1, kube_pod_labels:labeled{type!=""})
    record: container_memory_working_set_bytes:labeled
  - expr: |
      container_network_receive_bytes_total{metrics_path="/metrics/cadvisor"}
      *
      on(cluster,pod) group_left(type,deployment)
      topk by (cluster,pod) (1, kube_pod_labels:labeled{type!=""})
    record: container_network_receive_bytes_total:labeled
  - expr: |
      container_network_transmit_bytes_total{metrics_path="/metrics/cadvisor"}
      *
      on(cluster,pod) group_left(type,deployment)
      topk by (cluster,pod) (1, kube_pod_labels:labeled{type!=""})
    record: container_network_transmit_bytes_total:labeled
  - expr: |
      container_spec_cpu_period{metrics_path="/metrics/cadvisor"}
      *
      on(cluster,pod) group_left(type,deployment)
      topk by (cluster,pod) (1, kube_pod_labels:labeled{type!=""})
    record: container_spec_cpu_period:labeled
  - expr: |
      container_spec_cpu_quota{metrics_path="/metrics/cadvisor"}
      *
      on(cluster,pod) group_left(type,deployment)
      topk by (cluster,pod) (1, kube_pod_labels:labeled{type!=""})
    record: container_spec_cpu_quota:labeled
  - expr: |
      container_spec_cpu_shares{metrics_path="/metrics/cadvisor"}
      *
      on(cluster,pod) group_left(type,deployment)
      topk by (cluster,pod) (1, kube_pod_labels:labeled{type!=""})
    record: container_spec_cpu_shares:labeled
  - expr: |
      container_spec_memory_limit_bytes{metrics_path="/metrics/cadvisor"}
      *
      on(cluster,pod) group_left(type,deployment)
      topk by (cluster,pod) (1, kube_pod_labels:labeled{type!=""})
    record: container_spec_memory_limit_bytes:labeled
  - expr: |
      kube_pod_container_status_running
      *
      on(cluster,pod) group_left(type,deployment)
      topk by (cluster,pod) (1, kube_pod_labels:labeled{type!=""})
    record: kube_pod_container_status_running:labeled
  - expr: |
      kube_pod_container_resource_limits
      *
      on(cluster,pod) group_left(type,deployment)
      topk by (cluster,pod) (1, kube_pod_labels:labeled{type!=""})
    record: kube_pod_container_resource_limits:labeled
  - expr: |
      kube_pod_container_resource_requests
      *
      on(cluster,pod) group_left(type,deployment)
      topk by (cluster,pod) (1, kube_pod_labels:labeled{type!=""})
    record: kube_pod_container_resource_requests:labeled
  - expr: |
      kube_pod_container_status_last_terminated_reason
      *
      on(cluster,pod) group_left(type,deployment)
      topk by (cluster,pod) (1, kube_pod_labels:labeled{type!=""})
    record: kube_pod_container_status_last_terminated_reason:labeled
  - expr: |
      kube_pod_container_status_ready
      *
      on(cluster,pod) group_left(type,deployment)
      topk by (cluster,pod) (1, kube_pod_labels:labeled{type!=""})
    record: kube_pod_container_status_ready:labeled
  - expr: |
      kube_pod_container_status_restarts_total
      *
      on(cluster,pod) group_left(type,deployment)
      topk by (cluster,pod) (1, kube_pod_labels:labeled{type!=""})
    record: kube_pod_container_status_restarts_total:labeled
  - expr: |
      kube_pod_container_status_running
      *
      on(cluster,pod) group_left(type,deployment)
      topk by (cluster,pod) (1, kube_pod_labels:labeled{type!=""})
    record: kube_pod_container_status_running:labeled
  - expr: |
      kube_pod_container_status_terminated
      *
      on(cluster,pod) group_left(type,deployment)
      topk by (cluster,pod) (1, kube_pod_labels:labeled{type!=""})
    record: kube_pod_container_status_terminated:labeled
  - expr: |
      kube_pod_container_status_terminated_reason
      *
      on(cluster,pod) group_left(type,deployment)
      topk by (cluster,pod) (1, kube_pod_labels:labeled{type!=""})
    record: kube_pod_container_status_terminated_reason:labeled
  - expr: |
      kube_pod_container_status_waiting
      *
      on(cluster,pod) group_left(type,deployment)
      topk by (cluster,pod) (1, kube_pod_labels:labeled{type!=""})
    record: kube_pod_container_status_waiting:labeled
  - expr: |
      kube_pod_container_status_waiting_reason
      *
      on(cluster,pod) group_left(type,deployment)
      topk by (cluster,pod) (1, kube_pod_labels:labeled{type!=""})
    record: kube_pod_container_status_waiting_reason:labeled
  - expr: |
      kube_horizontalpodautoscaler_spec_target_metric
      *
      on(cluster,horizontalpodautoscaler) group_left(type)
      topk by (cluster,horizontalpodautoscaler) (1, kube_horizontalpodautoscaler_labels:labeled{type!=""})
    record: kube_horizontalpodautoscaler_spec_target_metric:labeled
  - expr: |
      kube_horizontalpodautoscaler_status_condition
      *
      on(cluster,horizontalpodautoscaler) group_left(type)
      topk by (cluster,horizontalpodautoscaler) (1, kube_horizontalpodautoscaler_labels:labeled{type!=""})
    record: kube_horizontalpodautoscaler_status_condition:labeled
  - expr: |
      kube_horizontalpodautoscaler_status_current_replicas
      *
      on(cluster,horizontalpodautoscaler) group_left(type)
      topk by (cluster,horizontalpodautoscaler) (1, kube_horizontalpodautoscaler_labels:labeled{type!=""})
    record: kube_horizontalpodautoscaler_status_current_replicas:labeled
  - expr: |
      kube_horizontalpodautoscaler_status_desired_replicas
      *
      on(cluster,horizontalpodautoscaler) group_left(type)
      topk by (cluster,horizontalpodautoscaler) (1, kube_horizontalpodautoscaler_labels:labeled{type!=""})
    record: kube_horizontalpodautoscaler_status_desired_replicas:labeled
  - expr: |
      kube_horizontalpodautoscaler_metadata_generation
      *
      on(cluster,horizontalpodautoscaler) group_left(type)
      topk by (cluster,horizontalpodautoscaler) (1, kube_horizontalpodautoscaler_labels:labeled{type!=""})
    record: kube_horizontalpodautoscaler_metadata_generation:labeled
  - expr: |
      kube_horizontalpodautoscaler_spec_max_replicas
      *
      on(cluster,horizontalpodautoscaler) group_left(type)
      topk by (cluster,horizontalpodautoscaler) (1, kube_horizontalpodautoscaler_labels:labeled{type!=""})
    record: kube_horizontalpodautoscaler_spec_max_replicas:labeled
  - expr: |
      kube_horizontalpodautoscaler_spec_min_replicas
      *
      on(cluster,horizontalpodautoscaler) group_left(type)
      topk by (cluster,horizontalpodautoscaler) (1, kube_horizontalpodautoscaler_labels:labeled{type!=""})
    record: kube_horizontalpodautoscaler_spec_min_replicas:labeled
  - expr: |
      kube_node_status_capacity
      *
      on(cluster,node) group_left(type)
      topk by (cluster,node) (1, kube_node_labels:labeled{type!=""})
    record: kube_node_status_capacity:labeled
  - expr: |
      kube_node_status_allocatable
      *
      on(cluster,node) group_left(type)
      topk by (cluster,node) (1, kube_node_labels:labeled{type!=""})
    record: kube_node_status_allocatable:labeled
  - expr: |
      kube_node_status_condition
      *
      on(cluster,node) group_left(type)
      topk by (cluster,node) (1, kube_node_labels:labeled{type!=""})
    record: kube_node_status_condition:labeled
  - expr: |
      node_schedstat_waiting_seconds_total
      *
      on(cluster,node) group_left(type)
      topk by (cluster,node) (1, kube_node_labels:labeled{type!=""})
    record: node_schedstat_waiting_seconds_total:labeled
  - expr: |
      node_cpu_seconds_total
      *
      on(cluster,node) group_left(type)
      topk by (cluster,node) (1, kube_node_labels:labeled{type!=""})
    record: node_cpu_seconds_total:labeled
  - expr: |
      node_network_transmit_bytes_total
      *
      on(cluster,node) group_left(type)
      topk by (cluster,node) (1, kube_node_labels:labeled{type!=""})
    record: node_network_transmit_bytes_total:labeled
  - expr: |
      node_network_receive_bytes_total
      *
      on(cluster,node) group_left(type)
      topk by (cluster,node) (1, kube_node_labels:labeled{type!=""})
    record: node_network_receive_bytes_total:labeled
  - expr: |
      node_disk_reads_completed_total
      *
      on(cluster,node) group_left(type)
      topk by (cluster,node) (1, kube_node_labels:labeled{type!=""})
    record: node_disk_reads_completed_total:labeled
  - expr: |
      node_disk_writes_completed_total
      *
      on(cluster,node) group_left(type)
      topk by (cluster,node) (1, kube_node_labels:labeled{type!=""})
    record: node_disk_writes_completed_total:labeled
  - expr: |
      node_disk_read_bytes_total
      *
      on(cluster,node) group_left(type)
      topk by (cluster,node) (1, kube_node_labels:labeled{type!=""})
    record: node_disk_read_bytes_total:labeled
  - expr: |
      node_disk_written_bytes_total
      *
      on(cluster,node) group_left(type)
      topk by (cluster,node) (1, kube_node_labels:labeled{type!=""})
    record: node_disk_written_bytes_total:labeled
  - expr: |
      node_disk_read_time_seconds_total
      *
      on(cluster,node) group_left(type)
      topk by (cluster,node) (1, kube_node_labels:labeled{type!=""})
    record: node_disk_read_time_seconds_total:labeled
  - expr: |
      node_disk_write_time_seconds_total
      *
      on(cluster,node) group_left(type)
      topk by (cluster,node) (1, kube_node_labels:labeled{type!=""})
    record: node_disk_write_time_seconds_total:labeled
  - expr: |
      node_load1
      *
      on(cluster,node) group_left(type)
      topk by (cluster,node) (1, kube_node_labels:labeled{type!=""})
    record: node_load1:labeled
  - expr: |
      node_load5
      *
      on(cluster,node) group_left(type)
      topk by (cluster,node) (1, kube_node_labels:labeled{type!=""})
    record: node_load5:labeled
  - expr: |
      node_load15
      *
      on(cluster,node) group_left(type)
      topk by (cluster,node) (1, kube_node_labels:labeled{type!=""})
    record: node_load15:labeled
  - expr: |
      node_vmstat_oom_kill
      *
      on(cluster,node) group_left(type)
      topk by (cluster,node) (1, kube_node_labels:labeled{type!=""})
    record: node_vmstat_oom_kill:labeled
  - expr: |
      nginx_ingress_controller_bytes_sent_count
      *
      on(cluster,ingress) group_left(type)
      topk by (cluster,ingress) (1, kube_ingress_labels:labeled{type!=""})
    record: nginx_ingress_controller_bytes_sent_count:labeled
  - expr: |
      nginx_ingress_controller_request_duration_seconds_bucket
      *
      on(cluster,ingress) group_left(type)
      topk by (cluster,ingress) (1, kube_ingress_labels:labeled{type!=""})
    record: nginx_ingress_controller_request_duration_seconds_bucket:labeled
  - expr: |
      nginx_ingress_controller_request_duration_seconds_sum
      *
      on(cluster,ingress) group_left(type)
      topk by (cluster,ingress) (1, kube_ingress_labels:labeled{type!=""})
    record: nginx_ingress_controller_request_duration_seconds_sum:labeled
  - expr: |
      nginx_ingress_controller_response_size_count
      *
      on(cluster,ingress) group_left(type)
      topk by (cluster,ingress) (1, kube_ingress_labels:labeled{type!=""})
    record: nginx_ingress_controller_response_size_count:labeled
  - expr: |
      nginx_ingress_controller_ingress_upstream_latency_seconds
      *
      on(cluster,ingress) group_left(type)
      topk by (cluster,ingress) (1, kube_ingress_labels:labeled{type!=""})
    record: nginx_ingress_controller_ingress_upstream_latency_seconds:labeled
  - expr: |
      nginx_ingress_controller_ingress_upstream_latency_seconds_count
      *
      on(cluster,ingress) group_left(type)
      topk by (cluster,ingress) (1, kube_ingress_labels:labeled{type!=""})
    record: nginx_ingress_controller_ingress_upstream_latency_seconds_count:labeled
  - expr: |
      nginx_ingress_controller_ingress_upstream_latency_seconds_sum
      *
      on(cluster,ingress) group_left(type)
      topk by (cluster,ingress) (1, kube_ingress_labels:labeled{type!=""})
    record: nginx_ingress_controller_ingress_upstream_latency_seconds_sum:labeled
  - expr: |
      nginx_ingress_controller_request_duration_seconds_count
      *
      on(cluster,ingress) group_left(type)
      topk by (cluster,ingress) (1, kube_ingress_labels:labeled{type!=""})
    record: nginx_ingress_controller_request_duration_seconds_count:labeled
  - expr: |
      nginx_ingress_controller_request_size_bucket
      *
      on(cluster,ingress) group_left(type)
      topk by (cluster,ingress) (1, kube_ingress_labels:labeled{type!=""})
    record: nginx_ingress_controller_request_size_bucket:labeled
  - expr: |
      nginx_ingress_controller_response_duration_seconds_bucket
      *
      on(cluster,ingress) group_left(type)
      topk by (cluster,ingress) (1, kube_ingress_labels:labeled{type!=""})
    record: nginx_ingress_controller_response_duration_seconds_bucket:labeled
  - expr: |
      nginx_ingress_controller_bytes_sent_bucket
      *
      on(cluster,ingress) group_left(type)
      topk by (cluster,ingress) (1, kube_ingress_labels:labeled{type!=""})
    record: nginx_ingress_controller_bytes_sent_bucket:labeled
  - expr: |
      nginx_ingress_controller_bytes_sent_sum
      *
      on(cluster,ingress) group_left(type)
      topk by (cluster,ingress) (1, kube_ingress_labels:labeled{type!=""})
    record: nginx_ingress_controller_bytes_sent_sum:labeled
  - expr: |
      nginx_ingress_controller_request_size_count
      *
      on(cluster,ingress) group_left(type)
      topk by (cluster,ingress) (1, kube_ingress_labels:labeled{type!=""})
    record: nginx_ingress_controller_request_size_count:labeled
  - expr: |
      nginx_ingress_controller_response_duration_seconds_sum
      *
      on(cluster,ingress) group_left(type)
      topk by (cluster,ingress) (1, kube_ingress_labels:labeled{type!=""})
    record: nginx_ingress_controller_response_duration_seconds_sum:labeled
  - expr: |
      nginx_ingress_controller_request_size_sum
      *
      on(cluster,ingress) group_left(type)
      topk by (cluster,ingress) (1, kube_ingress_labels:labeled{type!=""})
    record: nginx_ingress_controller_request_size_sum:labeled
  - expr: |
      nginx_ingress_controller_requests
      *
      on(cluster,ingress) group_left(type)
      topk by (cluster,ingress) (1, kube_ingress_labels:labeled{type!=""})
    record: nginx_ingress_controller_requests:labeled
  - expr: |
      nginx_ingress_controller_response_duration_seconds_count
      *
      on(cluster,ingress) group_left(type)
      topk by (cluster,ingress) (1, kube_ingress_labels:labeled{type!=""})
    record: nginx_ingress_controller_response_duration_seconds_count:labeled
  - expr: |
      nginx_ingress_controller_response_size_bucket
      *
      on(cluster,ingress) group_left(type)
      topk by (cluster,ingress) (1, kube_ingress_labels:labeled{type!=""})
    record: nginx_ingress_controller_response_size_bucket:labeled
  - expr: |
      nginx_ingress_controller_response_size_sum
      *
      on(cluster,ingress) group_left(type)
      topk by (cluster,ingress) (1, kube_ingress_labels:labeled{type!=""})
    record: nginx_ingress_controller_response_size_sum:labeled
  - expr: |
      kube_deployment_status_replicas_unavailable
      *
      on(cluster,deployment) group_left(type)
      topk by (cluster,deployment) (1, kube_deployment_labels:labeled{type!=""})
    record: kube_deployment_status_replicas_unavailable:labeled
  - expr: |
      kube_deployment_status_replicas_updated
      *
      on(cluster,deployment) group_left(type)
      topk by (cluster,deployment) (1, kube_deployment_labels:labeled{type!=""})
    record: kube_deployment_status_replicas_updated:labeled
  - expr: |
      kube_deployment_spec_paused
      *
      on(cluster,deployment) group_left(type)
      topk by (cluster,deployment) (1, kube_deployment_labels:labeled{type!=""})
    record: kube_deployment_spec_paused:labeled
  - expr: |
      kube_deployment_spec_replicas
      *
      on(cluster,deployment) group_left(type)
      topk by (cluster,deployment) (1, kube_deployment_labels:labeled{type!=""})
    record: kube_deployment_spec_replicas:labeled
  - expr: |
      kube_deployment_spec_strategy_rollingupdate_max_surge
      *
      on(cluster,deployment) group_left(type)
      topk by (cluster,deployment) (1, kube_deployment_labels:labeled{type!=""})
    record: kube_deployment_spec_strategy_rollingupdate_max_surge:labeled
  - expr: |
      kube_deployment_spec_strategy_rollingupdate_max_unavailable
      *
      on(cluster,deployment) group_left(type)
      topk by (cluster,deployment) (1, kube_deployment_labels:labeled{type!=""})
    record: kube_deployment_spec_strategy_rollingupdate_max_unavailable:labeled
  - expr: |
      kube_deployment_status_condition
      *
      on(cluster,deployment) group_left(type)
      topk by (cluster,deployment) (1, kube_deployment_labels:labeled{type!=""})
    record: kube_deployment_status_condition:labeled
  - expr: |
      kube_deployment_status_replicas_available
      *
      on(cluster,deployment) group_left(type)
      topk by (cluster,deployment) (1, kube_deployment_labels:labeled{type!=""})
    record: kube_deployment_status_replicas_available:labeled
  - expr: |
      kube_deployment_created
      *
      on(cluster,deployment) group_left(type)
      topk by (cluster,deployment) (1, kube_deployment_labels:labeled{type!=""})
    record: kube_deployment_created:labeled
  - expr: |
      kube_deployment_metadata_generation
      *
      on(cluster,deployment) group_left(type)
      topk by (cluster,deployment) (1, kube_deployment_labels:labeled{type!=""})
    record: kube_deployment_metadata_generation:labeled
  - expr: |
      kube_deployment_status_observed_generation
      *
      on(cluster,deployment) group_left(type)
      topk by (cluster,deployment) (1, kube_deployment_labels:labeled{type!=""})
    record: kube_deployment_status_observed_generation:labeled
  - expr: |
      kube_deployment_status_replicas
      *
      on(cluster,deployment) group_left(type)
      topk by (cluster,deployment) (1, kube_deployment_labels:labeled{type!=""})
    record: kube_deployment_status_replicas:labeled
