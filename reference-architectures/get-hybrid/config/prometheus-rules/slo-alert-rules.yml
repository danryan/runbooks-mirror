# WARNING. DO NOT EDIT THIS FILE BY HAND. RUN ./scripts/generate-all-reference-architecture-configs.sh TO GENERATE IT. YOUR CHANGES WILL BE OVERRIDDEN
groups:
- interval: 1m
  name: 'Service Component Alerts: gitaly'
  rules:
  - alert: GitalyServiceGoserverApdexSLOViolation
    annotations:
      description: |
        This SLI monitors all Gitaly GRPC requests in aggregate, excluding the OperationService. GRPC failures which are considered to be the "server's fault" are counted as errors. The apdex score is based on a subset of GRPC methods which are expected to be fast.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: gitaly-main/gitaly-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/gitaly-main/gitaly-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "877804374"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (le) (
            rate(grpc_server_handling_seconds_bucket{grpc_method!~"CalculateChecksum|CommitLanguages|CommitStats|CreateFork|CreateRepositoryFromURL|FetchInternalRemote|FetchIntoObjectPool|FetchRemote|FetchSourceBranch|FindRemoteRepository|FindRemoteRootRef|Fsck|GarbageCollect|OptimizeRepository|PackObjectsHookWithSidechannel|PostReceiveHook|PostUploadPackWithSidechannel|PreReceiveHook|RepackFull|RepackIncremental|ReplicateRepository|RepositorySize|SSHUploadPackWithSidechannel|UpdateHook",grpc_service!="gitaly.OperationService",grpc_type="unary",job="gitaly"}[5m])
          )
        )
      runbook: docs/gitaly/README.md
      title: The goserver SLI of the gitaly service has an apdex violating SLO
    expr: |
      (
        (
          gitlab_component_apdex:ratio_1h{component="goserver",type="gitaly"}
          < (1 - 14.4 * 0.001000)
        )
        and
        (
          gitlab_component_apdex:ratio_5m{component="goserver",type="gitaly"}
          < (1 - 14.4 * 0.001000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_1h{component="goserver",type="gitaly"}) >= 1
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "yes"
      window: 1h
  - alert: GitalyServiceGoserverApdexSLOViolation
    annotations:
      description: |
        This SLI monitors all Gitaly GRPC requests in aggregate, excluding the OperationService. GRPC failures which are considered to be the "server's fault" are counted as errors. The apdex score is based on a subset of GRPC methods which are expected to be fast.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: gitaly-main/gitaly-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/gitaly-main/gitaly-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "877804374"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (le) (
            rate(grpc_server_handling_seconds_bucket{grpc_method!~"CalculateChecksum|CommitLanguages|CommitStats|CreateFork|CreateRepositoryFromURL|FetchInternalRemote|FetchIntoObjectPool|FetchRemote|FetchSourceBranch|FindRemoteRepository|FindRemoteRootRef|Fsck|GarbageCollect|OptimizeRepository|PackObjectsHookWithSidechannel|PostReceiveHook|PostUploadPackWithSidechannel|PreReceiveHook|RepackFull|RepackIncremental|ReplicateRepository|RepositorySize|SSHUploadPackWithSidechannel|UpdateHook",grpc_service!="gitaly.OperationService",grpc_type="unary",job="gitaly"}[5m])
          )
        )
      runbook: docs/gitaly/README.md
      title: The goserver SLI of the gitaly service has an apdex violating SLO
    expr: |
      (
        (
          gitlab_component_apdex:ratio_6h{component="goserver",type="gitaly"}
          < (1 - 6 * 0.001000)
        )
        and
        (
          gitlab_component_apdex:ratio_30m{component="goserver",type="gitaly"}
          < (1 - 6 * 0.001000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_6h{component="goserver",type="gitaly"}) >= 0.16667
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "yes"
      window: 6h
  - alert: GitalyServiceGoserverErrorSLOViolation
    annotations:
      description: |
        This SLI monitors all Gitaly GRPC requests in aggregate, excluding the OperationService. GRPC failures which are considered to be the "server's fault" are counted as errors. The apdex score is based on a subset of GRPC methods which are expected to be fast.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: gitaly-main/gitaly-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/gitaly-main/gitaly-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "594670537"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          label_replace(rate(gitaly_service_client_requests_total{grpc_code!~"AlreadyExists|Canceled|DeadlineExceeded|FailedPrecondition|InvalidArgument|NotFound|OK|PermissionDenied|ResourceExhausted|Unauthenticated",grpc_service!="gitaly.OperationService",job="gitaly"}[5m]), "_c", "0", "", "")
          or
          label_replace(rate(gitaly_service_client_requests_total{deadline_type!="limited",grpc_code="DeadlineExceeded",grpc_service!="gitaly.OperationService",job="gitaly"}[5m]), "_c", "1", "", "")
        )
      runbook: docs/gitaly/README.md
      title: The goserver SLI of the gitaly service has an error rate violating SLO
    expr: |
      (
        (
          gitlab_component_errors:ratio_1h{component="goserver",type="gitaly"}
          > (14.4 * 0.000500)
        )
        and
        (
          gitlab_component_errors:ratio_5m{component="goserver",type="gitaly"}
          > (14.4 * 0.000500)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_1h{component="goserver",type="gitaly"}) >= 1
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "yes"
      window: 1h
  - alert: GitalyServiceGoserverErrorSLOViolation
    annotations:
      description: |
        This SLI monitors all Gitaly GRPC requests in aggregate, excluding the OperationService. GRPC failures which are considered to be the "server's fault" are counted as errors. The apdex score is based on a subset of GRPC methods which are expected to be fast.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: gitaly-main/gitaly-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/gitaly-main/gitaly-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "594670537"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          label_replace(rate(gitaly_service_client_requests_total{grpc_code!~"AlreadyExists|Canceled|DeadlineExceeded|FailedPrecondition|InvalidArgument|NotFound|OK|PermissionDenied|ResourceExhausted|Unauthenticated",grpc_service!="gitaly.OperationService",job="gitaly"}[5m]), "_c", "0", "", "")
          or
          label_replace(rate(gitaly_service_client_requests_total{deadline_type!="limited",grpc_code="DeadlineExceeded",grpc_service!="gitaly.OperationService",job="gitaly"}[5m]), "_c", "1", "", "")
        )
      runbook: docs/gitaly/README.md
      title: The goserver SLI of the gitaly service has an error rate violating SLO
    expr: |
      (
        (
          gitlab_component_errors:ratio_6h{component="goserver",type="gitaly"}
          > (6 * 0.000500)
        )
        and
        (
          gitlab_component_errors:ratio_30m{component="goserver",type="gitaly"}
          > (6 * 0.000500)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_6h{component="goserver",type="gitaly"}) >= 0.16667
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "yes"
      window: 6h
  - alert: GitalyServiceGoserverTrafficCessation
    annotations:
      description: |
        This SLI monitors all Gitaly GRPC requests in aggregate, excluding the OperationService. GRPC failures which are considered to be the "server's fault" are counted as errors. The apdex score is based on a subset of GRPC methods which are expected to be fast.

        This alert signifies that the SLI is reporting a cessation of traffic; the signal is present, but is zero.
      grafana_dashboard_id: gitaly-main/gitaly-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/gitaly-main/gitaly-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1687078843"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(gitaly_service_client_requests_total{grpc_service!="gitaly.OperationService",job="gitaly"}[5m])
        )
      runbook: docs/gitaly/README.md
      title: The goserver SLI of the gitaly service has not received any traffic in
        the past 30m
    expr: |
      gitlab_component_ops:rate_30m{component="goserver",type="gitaly"} == 0
      and
      gitlab_component_ops:rate_30m{component="goserver",type="gitaly"} offset 1h >= 0.16666666666666666
    for: 5m
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
  - alert: GitalyServiceGoserverTrafficAbsent
    annotations:
      description: |
        This SLI monitors all Gitaly GRPC requests in aggregate, excluding the OperationService. GRPC failures which are considered to be the "server's fault" are counted as errors. The apdex score is based on a subset of GRPC methods which are expected to be fast.

        This alert signifies that the SLI was previously reporting traffic, but is no longer - the signal is absent.

        This could be caused by a change to the metrics used in the SLI, or by the service not receiving traffic.
      grafana_dashboard_id: gitaly-main/gitaly-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/gitaly-main/gitaly-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1687078843"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(gitaly_service_client_requests_total{grpc_service!="gitaly.OperationService",job="gitaly"}[5m])
        )
      runbook: docs/gitaly/README.md
      title: The goserver SLI of the gitaly service has not reported any traffic in
        the past 30m
    expr: |
      gitlab_component_ops:rate_5m{component="goserver",type="gitaly"} offset 1h
      unless
      gitlab_component_ops:rate_5m{component="goserver",type="gitaly"}
    for: 30m
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
- interval: 1m
  name: 'Service Component Alerts: gitlab-shell'
  rules:
  - alert: GitlabShellServiceGrpcRequestsApdexSLOViolation
    annotations:
      description: |
        A proxy measurement of the number of GRPC SSH service requests made to Gitaly and Praefect.

        Since we are unable to measure gitlab-shell directly at present, this is the best substitute we can provide.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: gitlab-shell-main/gitlab-shell-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/gitlab-shell-main/gitlab-shell-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2041855155"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (le) (
            rate(grpc_server_handling_seconds_bucket{grpc_service="gitaly.SSHService",grpc_type="unary",job="gitaly"}[5m])
          )
        )
      runbook: docs/gitlab-shell/README.md
      title: The grpc_requests SLI of the gitlab-shell service has an apdex violating
        SLO
    expr: |
      (
        (
          gitlab_component_apdex:ratio_1h{component="grpc_requests",type="gitlab-shell"}
          < (1 - 14.4 * 0.001000)
        )
        and
        (
          gitlab_component_apdex:ratio_5m{component="grpc_requests",type="gitlab-shell"}
          < (1 - 14.4 * 0.001000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_1h{component="grpc_requests",type="gitlab-shell"}) >= 1
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "yes"
      window: 1h
  - alert: GitlabShellServiceGrpcRequestsApdexSLOViolation
    annotations:
      description: |
        A proxy measurement of the number of GRPC SSH service requests made to Gitaly and Praefect.

        Since we are unable to measure gitlab-shell directly at present, this is the best substitute we can provide.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: gitlab-shell-main/gitlab-shell-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/gitlab-shell-main/gitlab-shell-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2041855155"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (le) (
            rate(grpc_server_handling_seconds_bucket{grpc_service="gitaly.SSHService",grpc_type="unary",job="gitaly"}[5m])
          )
        )
      runbook: docs/gitlab-shell/README.md
      title: The grpc_requests SLI of the gitlab-shell service has an apdex violating
        SLO
    expr: |
      (
        (
          gitlab_component_apdex:ratio_6h{component="grpc_requests",type="gitlab-shell"}
          < (1 - 6 * 0.001000)
        )
        and
        (
          gitlab_component_apdex:ratio_30m{component="grpc_requests",type="gitlab-shell"}
          < (1 - 6 * 0.001000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_6h{component="grpc_requests",type="gitlab-shell"}) >= 0.16667
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "yes"
      window: 6h
  - alert: GitlabShellServiceGrpcRequestsErrorSLOViolation
    annotations:
      description: |
        A proxy measurement of the number of GRPC SSH service requests made to Gitaly and Praefect.

        Since we are unable to measure gitlab-shell directly at present, this is the best substitute we can provide.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: gitlab-shell-main/gitlab-shell-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/gitlab-shell-main/gitlab-shell-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "928307072"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(grpc_server_handled_total{grpc_code!~"AlreadyExists|Canceled|DeadlineExceeded|FailedPrecondition|InvalidArgument|NotFound|OK|PermissionDenied|ResourceExhausted|Unauthenticated",grpc_service="gitaly.SSHService",job="gitaly"}[5m])
        )
      runbook: docs/gitlab-shell/README.md
      title: The grpc_requests SLI of the gitlab-shell service has an error rate violating
        SLO
    expr: |
      (
        (
          gitlab_component_errors:ratio_1h{component="grpc_requests",type="gitlab-shell"}
          > (14.4 * 0.001000)
        )
        and
        (
          gitlab_component_errors:ratio_5m{component="grpc_requests",type="gitlab-shell"}
          > (14.4 * 0.001000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_1h{component="grpc_requests",type="gitlab-shell"}) >= 1
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "yes"
      window: 1h
  - alert: GitlabShellServiceGrpcRequestsErrorSLOViolation
    annotations:
      description: |
        A proxy measurement of the number of GRPC SSH service requests made to Gitaly and Praefect.

        Since we are unable to measure gitlab-shell directly at present, this is the best substitute we can provide.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: gitlab-shell-main/gitlab-shell-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/gitlab-shell-main/gitlab-shell-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "928307072"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(grpc_server_handled_total{grpc_code!~"AlreadyExists|Canceled|DeadlineExceeded|FailedPrecondition|InvalidArgument|NotFound|OK|PermissionDenied|ResourceExhausted|Unauthenticated",grpc_service="gitaly.SSHService",job="gitaly"}[5m])
        )
      runbook: docs/gitlab-shell/README.md
      title: The grpc_requests SLI of the gitlab-shell service has an error rate violating
        SLO
    expr: |
      (
        (
          gitlab_component_errors:ratio_6h{component="grpc_requests",type="gitlab-shell"}
          > (6 * 0.001000)
        )
        and
        (
          gitlab_component_errors:ratio_30m{component="grpc_requests",type="gitlab-shell"}
          > (6 * 0.001000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_6h{component="grpc_requests",type="gitlab-shell"}) >= 0.16667
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "yes"
      window: 6h
  - alert: GitlabShellServiceGrpcRequestsTrafficCessation
    annotations:
      description: |
        A proxy measurement of the number of GRPC SSH service requests made to Gitaly and Praefect.

        Since we are unable to measure gitlab-shell directly at present, this is the best substitute we can provide.

        This alert signifies that the SLI is reporting a cessation of traffic; the signal is present, but is zero.
      grafana_dashboard_id: gitlab-shell-main/gitlab-shell-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/gitlab-shell-main/gitlab-shell-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3837654202"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(gitaly_service_client_requests_total{grpc_service="gitaly.SSHService",job="gitaly"}[5m])
        )
      runbook: docs/gitlab-shell/README.md
      title: The grpc_requests SLI of the gitlab-shell service has not received any
        traffic in the past 30m
    expr: |
      gitlab_component_ops:rate_30m{component="grpc_requests",type="gitlab-shell"} == 0
      and
      gitlab_component_ops:rate_30m{component="grpc_requests",type="gitlab-shell"} offset 1h >= 0.16666666666666666
    for: 5m
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
  - alert: GitlabShellServiceGrpcRequestsTrafficAbsent
    annotations:
      description: |
        A proxy measurement of the number of GRPC SSH service requests made to Gitaly and Praefect.

        Since we are unable to measure gitlab-shell directly at present, this is the best substitute we can provide.

        This alert signifies that the SLI was previously reporting traffic, but is no longer - the signal is absent.

        This could be caused by a change to the metrics used in the SLI, or by the service not receiving traffic.
      grafana_dashboard_id: gitlab-shell-main/gitlab-shell-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/gitlab-shell-main/gitlab-shell-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3837654202"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(gitaly_service_client_requests_total{grpc_service="gitaly.SSHService",job="gitaly"}[5m])
        )
      runbook: docs/gitlab-shell/README.md
      title: The grpc_requests SLI of the gitlab-shell service has not reported any
        traffic in the past 30m
    expr: |
      gitlab_component_ops:rate_5m{component="grpc_requests",type="gitlab-shell"} offset 1h
      unless
      gitlab_component_ops:rate_5m{component="grpc_requests",type="gitlab-shell"}
    for: 30m
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
- interval: 1m
  name: 'Service Component Alerts: praefect'
  rules:
  - alert: PraefectServiceProxyApdexSLOViolation
    annotations:
      description: |
        All Gitaly operations pass through the Praefect proxy on the way to a Gitaly instance. This SLI monitors those operations in aggregate.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: praefect-main/praefect-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/praefect-main/praefect-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2100325234"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (le) (
            rate(grpc_server_handling_seconds_bucket{grpc_type="unary",job="praefect"}[5m])
          )
        )
      runbook: docs/praefect/README.md
      title: The proxy SLI of the praefect service has an apdex violating SLO
    expr: |
      (
        (
          gitlab_component_apdex:ratio_1h{component="proxy",type="praefect"}
          < (1 - 14.4 * 0.005000)
        )
        and
        (
          gitlab_component_apdex:ratio_5m{component="proxy",type="praefect"}
          < (1 - 14.4 * 0.005000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_1h{component="proxy",type="praefect"}) >= 1
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "yes"
      window: 1h
  - alert: PraefectServiceProxyApdexSLOViolation
    annotations:
      description: |
        All Gitaly operations pass through the Praefect proxy on the way to a Gitaly instance. This SLI monitors those operations in aggregate.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: praefect-main/praefect-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/praefect-main/praefect-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2100325234"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (le) (
            rate(grpc_server_handling_seconds_bucket{grpc_type="unary",job="praefect"}[5m])
          )
        )
      runbook: docs/praefect/README.md
      title: The proxy SLI of the praefect service has an apdex violating SLO
    expr: |
      (
        (
          gitlab_component_apdex:ratio_6h{component="proxy",type="praefect"}
          < (1 - 6 * 0.005000)
        )
        and
        (
          gitlab_component_apdex:ratio_30m{component="proxy",type="praefect"}
          < (1 - 6 * 0.005000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_6h{component="proxy",type="praefect"}) >= 0.16667
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "yes"
      window: 6h
  - alert: PraefectServiceProxyErrorSLOViolation
    annotations:
      description: |
        All Gitaly operations pass through the Praefect proxy on the way to a Gitaly instance. This SLI monitors those operations in aggregate.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: praefect-main/praefect-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/praefect-main/praefect-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1178914866"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(grpc_server_handled_total{grpc_code!~"^(OK|NotFound|Unauthenticated|AlreadyExists|FailedPrecondition|Canceled)$",job="praefect"}[5m])
        )
      runbook: docs/praefect/README.md
      title: The proxy SLI of the praefect service has an error rate violating SLO
    expr: |
      (
        (
          gitlab_component_errors:ratio_1h{component="proxy",type="praefect"}
          > (14.4 * 0.000500)
        )
        and
        (
          gitlab_component_errors:ratio_5m{component="proxy",type="praefect"}
          > (14.4 * 0.000500)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_1h{component="proxy",type="praefect"}) >= 1
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "yes"
      window: 1h
  - alert: PraefectServiceProxyErrorSLOViolation
    annotations:
      description: |
        All Gitaly operations pass through the Praefect proxy on the way to a Gitaly instance. This SLI monitors those operations in aggregate.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: praefect-main/praefect-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/praefect-main/praefect-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1178914866"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(grpc_server_handled_total{grpc_code!~"^(OK|NotFound|Unauthenticated|AlreadyExists|FailedPrecondition|Canceled)$",job="praefect"}[5m])
        )
      runbook: docs/praefect/README.md
      title: The proxy SLI of the praefect service has an error rate violating SLO
    expr: |
      (
        (
          gitlab_component_errors:ratio_6h{component="proxy",type="praefect"}
          > (6 * 0.000500)
        )
        and
        (
          gitlab_component_errors:ratio_30m{component="proxy",type="praefect"}
          > (6 * 0.000500)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_6h{component="proxy",type="praefect"}) >= 0.16667
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "yes"
      window: 6h
  - alert: PraefectServiceProxyTrafficCessation
    annotations:
      description: |
        All Gitaly operations pass through the Praefect proxy on the way to a Gitaly instance. This SLI monitors those operations in aggregate.

        This alert signifies that the SLI is reporting a cessation of traffic; the signal is present, but is zero.
      grafana_dashboard_id: praefect-main/praefect-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/praefect-main/praefect-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2797965992"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(grpc_server_handled_total{job="praefect"}[5m])
        )
      runbook: docs/praefect/README.md
      title: The proxy SLI of the praefect service has not received any traffic in
        the past 30m
    expr: |
      gitlab_component_ops:rate_30m{component="proxy",type="praefect"} == 0
      and
      gitlab_component_ops:rate_30m{component="proxy",type="praefect"} offset 1h >= 0.16666666666666666
    for: 5m
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
  - alert: PraefectServiceProxyTrafficAbsent
    annotations:
      description: |
        All Gitaly operations pass through the Praefect proxy on the way to a Gitaly instance. This SLI monitors those operations in aggregate.

        This alert signifies that the SLI was previously reporting traffic, but is no longer - the signal is absent.

        This could be caused by a change to the metrics used in the SLI, or by the service not receiving traffic.
      grafana_dashboard_id: praefect-main/praefect-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/praefect-main/praefect-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2797965992"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(grpc_server_handled_total{job="praefect"}[5m])
        )
      runbook: docs/praefect/README.md
      title: The proxy SLI of the praefect service has not reported any traffic in
        the past 30m
    expr: |
      gitlab_component_ops:rate_5m{component="proxy",type="praefect"} offset 1h
      unless
      gitlab_component_ops:rate_5m{component="proxy",type="praefect"}
    for: 30m
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
  - alert: PraefectServiceReplicatorQueueApdexSLOViolation
    annotations:
      description: |
        Praefect replication operations. Latency represents the queuing delay before replication is carried out.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: praefect-main/praefect-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/praefect-main/praefect-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "374957850"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (le) (
            rate(gitaly_praefect_replication_delay_bucket{}[5m])
          )
        )
      runbook: docs/praefect/README.md
      title: The replicator_queue SLI of the praefect service has an apdex violating
        SLO
    expr: |
      (
        (
          gitlab_component_apdex:ratio_1h{component="replicator_queue",type="praefect"}
          < (1 - 14.4 * 0.005000)
        )
        and
        (
          gitlab_component_apdex:ratio_5m{component="replicator_queue",type="praefect"}
          < (1 - 14.4 * 0.005000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_1h{component="replicator_queue",type="praefect"}) >= 1
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "no"
      window: 1h
  - alert: PraefectServiceReplicatorQueueApdexSLOViolation
    annotations:
      description: |
        Praefect replication operations. Latency represents the queuing delay before replication is carried out.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: praefect-main/praefect-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/praefect-main/praefect-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "374957850"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (le) (
            rate(gitaly_praefect_replication_delay_bucket{}[5m])
          )
        )
      runbook: docs/praefect/README.md
      title: The replicator_queue SLI of the praefect service has an apdex violating
        SLO
    expr: |
      (
        (
          gitlab_component_apdex:ratio_6h{component="replicator_queue",type="praefect"}
          < (1 - 6 * 0.005000)
        )
        and
        (
          gitlab_component_apdex:ratio_30m{component="replicator_queue",type="praefect"}
          < (1 - 6 * 0.005000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_6h{component="replicator_queue",type="praefect"}) >= 0.16667
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "no"
      window: 6h
  - alert: PraefectServiceReplicatorQueueTrafficCessation
    annotations:
      description: |
        Praefect replication operations. Latency represents the queuing delay before replication is carried out.

        This alert signifies that the SLI is reporting a cessation of traffic; the signal is present, but is zero.
      grafana_dashboard_id: praefect-main/praefect-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/praefect-main/praefect-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1815534001"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(gitaly_praefect_replication_delay_bucket{le="+Inf"}[5m])
        )
      runbook: docs/praefect/README.md
      title: The replicator_queue SLI of the praefect service has not received any
        traffic in the past 30m
    expr: |
      gitlab_component_ops:rate_30m{component="replicator_queue",type="praefect"} == 0
      and
      gitlab_component_ops:rate_30m{component="replicator_queue",type="praefect"} offset 1h >= 0.16666666666666666
    for: 5m
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "no"
  - alert: PraefectServiceReplicatorQueueTrafficAbsent
    annotations:
      description: |
        Praefect replication operations. Latency represents the queuing delay before replication is carried out.

        This alert signifies that the SLI was previously reporting traffic, but is no longer - the signal is absent.

        This could be caused by a change to the metrics used in the SLI, or by the service not receiving traffic.
      grafana_dashboard_id: praefect-main/praefect-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/praefect-main/praefect-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1815534001"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(gitaly_praefect_replication_delay_bucket{le="+Inf"}[5m])
        )
      runbook: docs/praefect/README.md
      title: The replicator_queue SLI of the praefect service has not reported any
        traffic in the past 30m
    expr: |
      gitlab_component_ops:rate_5m{component="replicator_queue",type="praefect"} offset 1h
      unless
      gitlab_component_ops:rate_5m{component="replicator_queue",type="praefect"}
    for: 30m
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "no"
- interval: 1m
  name: 'Service Component Alerts: registry'
  rules:
  - alert: RegistryServiceServerApdexSLOViolation
    annotations:
      description: |
        Aggregation of all registry HTTP requests.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: registry-main/registry-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/registry-main/registry-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2863319079"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (le) (
            rate(registry_http_request_duration_seconds_bucket{route!~"/v2/{name}/blobs/uploads/{uuid}|/v2/{name}/blobs/{digest}"}[5m])
          )
        )
      runbook: docs/registry/README.md
      title: The server SLI of the registry service has an apdex violating SLO
    expr: |
      (
        (
          gitlab_component_apdex:ratio_1h{component="server",type="registry"}
          < (1 - 14.4 * 0.003000)
        )
        and
        (
          gitlab_component_apdex:ratio_5m{component="server",type="registry"}
          < (1 - 14.4 * 0.003000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_1h{component="server",type="registry"}) >= 1
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "yes"
      window: 1h
  - alert: RegistryServiceServerApdexSLOViolation
    annotations:
      description: |
        Aggregation of all registry HTTP requests.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: registry-main/registry-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/registry-main/registry-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2863319079"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (le) (
            rate(registry_http_request_duration_seconds_bucket{route!~"/v2/{name}/blobs/uploads/{uuid}|/v2/{name}/blobs/{digest}"}[5m])
          )
        )
      runbook: docs/registry/README.md
      title: The server SLI of the registry service has an apdex violating SLO
    expr: |
      (
        (
          gitlab_component_apdex:ratio_6h{component="server",type="registry"}
          < (1 - 6 * 0.003000)
        )
        and
        (
          gitlab_component_apdex:ratio_30m{component="server",type="registry"}
          < (1 - 6 * 0.003000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_6h{component="server",type="registry"}) >= 0.16667
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "yes"
      window: 6h
  - alert: RegistryServiceServerErrorSLOViolation
    annotations:
      description: |
        Aggregation of all registry HTTP requests.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: registry-main/registry-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/registry-main/registry-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1631973137"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(registry_http_requests_total{code=~"5.."}[5m])
        )
      runbook: docs/registry/README.md
      title: The server SLI of the registry service has an error rate violating SLO
    expr: |
      (
        (
          gitlab_component_errors:ratio_1h{component="server",type="registry"}
          > (14.4 * 0.000100)
        )
        and
        (
          gitlab_component_errors:ratio_5m{component="server",type="registry"}
          > (14.4 * 0.000100)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_1h{component="server",type="registry"}) >= 1
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "yes"
      window: 1h
  - alert: RegistryServiceServerErrorSLOViolation
    annotations:
      description: |
        Aggregation of all registry HTTP requests.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: registry-main/registry-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/registry-main/registry-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1631973137"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(registry_http_requests_total{code=~"5.."}[5m])
        )
      runbook: docs/registry/README.md
      title: The server SLI of the registry service has an error rate violating SLO
    expr: |
      (
        (
          gitlab_component_errors:ratio_6h{component="server",type="registry"}
          > (6 * 0.000100)
        )
        and
        (
          gitlab_component_errors:ratio_30m{component="server",type="registry"}
          > (6 * 0.000100)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_6h{component="server",type="registry"}) >= 0.16667
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "yes"
      window: 6h
  - alert: RegistryServiceServerTrafficCessation
    annotations:
      description: |
        Aggregation of all registry HTTP requests.

        This alert signifies that the SLI is reporting a cessation of traffic; the signal is present, but is zero.
      grafana_dashboard_id: registry-main/registry-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/registry-main/registry-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2414183688"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(registry_http_requests_total{}[5m])
        )
      runbook: docs/registry/README.md
      title: The server SLI of the registry service has not received any traffic in
        the past 30m
    expr: |
      gitlab_component_ops:rate_30m{component="server",type="registry"} == 0
      and
      gitlab_component_ops:rate_30m{component="server",type="registry"} offset 1h >= 0.16666666666666666
    for: 5m
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
  - alert: RegistryServiceServerTrafficAbsent
    annotations:
      description: |
        Aggregation of all registry HTTP requests.

        This alert signifies that the SLI was previously reporting traffic, but is no longer - the signal is absent.

        This could be caused by a change to the metrics used in the SLI, or by the service not receiving traffic.
      grafana_dashboard_id: registry-main/registry-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/registry-main/registry-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2414183688"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(registry_http_requests_total{}[5m])
        )
      runbook: docs/registry/README.md
      title: The server SLI of the registry service has not reported any traffic in
        the past 30m
    expr: |
      gitlab_component_ops:rate_5m{component="server",type="registry"} offset 1h
      unless
      gitlab_component_ops:rate_5m{component="server",type="registry"}
    for: 30m
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
  - alert: RegistryServiceServerRouteBlobDigestDeletesApdexSLOViolation
    annotations:
      description: |
        Delete requests for the blob digest endpoints on the registry.

        Used to delete blobs identified by name and digest.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: registry-main/registry-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/registry-main/registry-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "693957856"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (le) (
            rate(registry_http_request_duration_seconds_bucket{method=~"delete",route="/v2/{name}/blobs/{digest}"}[5m])
          )
        )
      runbook: docs/registry/README.md
      title: The server_route_blob_digest_deletes SLI of the registry service has
        an apdex violating SLO
    expr: |
      (
        (
          gitlab_component_apdex:ratio_1h{component="server_route_blob_digest_deletes",type="registry"}
          < (1 - 14.4 * 0.001000)
        )
        and
        (
          gitlab_component_apdex:ratio_5m{component="server_route_blob_digest_deletes",type="registry"}
          < (1 - 14.4 * 0.001000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_1h{component="server_route_blob_digest_deletes",type="registry"}) >= 1
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "yes"
      window: 1h
  - alert: RegistryServiceServerRouteBlobDigestDeletesApdexSLOViolation
    annotations:
      description: |
        Delete requests for the blob digest endpoints on the registry.

        Used to delete blobs identified by name and digest.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: registry-main/registry-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/registry-main/registry-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "693957856"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (le) (
            rate(registry_http_request_duration_seconds_bucket{method=~"delete",route="/v2/{name}/blobs/{digest}"}[5m])
          )
        )
      runbook: docs/registry/README.md
      title: The server_route_blob_digest_deletes SLI of the registry service has
        an apdex violating SLO
    expr: |
      (
        (
          gitlab_component_apdex:ratio_6h{component="server_route_blob_digest_deletes",type="registry"}
          < (1 - 6 * 0.001000)
        )
        and
        (
          gitlab_component_apdex:ratio_30m{component="server_route_blob_digest_deletes",type="registry"}
          < (1 - 6 * 0.001000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_6h{component="server_route_blob_digest_deletes",type="registry"}) >= 0.16667
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "yes"
      window: 6h
  - alert: RegistryServiceServerRouteBlobDigestDeletesTrafficCessation
    annotations:
      description: |
        Delete requests for the blob digest endpoints on the registry.

        Used to delete blobs identified by name and digest.

        This alert signifies that the SLI is reporting a cessation of traffic; the signal is present, but is zero.
      grafana_dashboard_id: registry-main/registry-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/registry-main/registry-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "4060903410"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(registry_http_request_duration_seconds_count{method=~"delete",route="/v2/{name}/blobs/{digest}"}[5m])
        )
      runbook: docs/registry/README.md
      title: The server_route_blob_digest_deletes SLI of the registry service has
        not received any traffic in the past 30m
    expr: |
      gitlab_component_ops:rate_30m{component="server_route_blob_digest_deletes",type="registry"} == 0
      and
      gitlab_component_ops:rate_30m{component="server_route_blob_digest_deletes",type="registry"} offset 1h >= 0.16666666666666666
    for: 5m
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
  - alert: RegistryServiceServerRouteBlobDigestDeletesTrafficAbsent
    annotations:
      description: |
        Delete requests for the blob digest endpoints on the registry.

        Used to delete blobs identified by name and digest.

        This alert signifies that the SLI was previously reporting traffic, but is no longer - the signal is absent.

        This could be caused by a change to the metrics used in the SLI, or by the service not receiving traffic.
      grafana_dashboard_id: registry-main/registry-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/registry-main/registry-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "4060903410"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(registry_http_request_duration_seconds_count{method=~"delete",route="/v2/{name}/blobs/{digest}"}[5m])
        )
      runbook: docs/registry/README.md
      title: The server_route_blob_digest_deletes SLI of the registry service has
        not reported any traffic in the past 30m
    expr: |
      gitlab_component_ops:rate_5m{component="server_route_blob_digest_deletes",type="registry"} offset 1h
      unless
      gitlab_component_ops:rate_5m{component="server_route_blob_digest_deletes",type="registry"}
    for: 30m
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
  - alert: RegistryServiceServerRouteBlobDigestReadsApdexSLOViolation
    annotations:
      description: |
        All read-requests (GET or HEAD) for the blob endpoints on the registry.

        GET is used to pull a layer gated by the name of repository and uniquely identified by the digest in the registry.

        HEAD is used to check the existence of a layer.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: registry-main/registry-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/registry-main/registry-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "623369547"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (le) (
            rate(registry_http_request_duration_seconds_bucket{method=~"get|head",route="/v2/{name}/blobs/{digest}"}[5m])
          )
        )
      runbook: docs/registry/README.md
      title: The server_route_blob_digest_reads SLI of the registry service has an
        apdex violating SLO
    expr: |
      (
        (
          gitlab_component_apdex:ratio_1h{component="server_route_blob_digest_reads",type="registry"}
          < (1 - 14.4 * 0.020000)
        )
        and
        (
          gitlab_component_apdex:ratio_5m{component="server_route_blob_digest_reads",type="registry"}
          < (1 - 14.4 * 0.020000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_1h{component="server_route_blob_digest_reads",type="registry"}) >= 1
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "yes"
      window: 1h
  - alert: RegistryServiceServerRouteBlobDigestReadsApdexSLOViolation
    annotations:
      description: |
        All read-requests (GET or HEAD) for the blob endpoints on the registry.

        GET is used to pull a layer gated by the name of repository and uniquely identified by the digest in the registry.

        HEAD is used to check the existence of a layer.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: registry-main/registry-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/registry-main/registry-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "623369547"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (le) (
            rate(registry_http_request_duration_seconds_bucket{method=~"get|head",route="/v2/{name}/blobs/{digest}"}[5m])
          )
        )
      runbook: docs/registry/README.md
      title: The server_route_blob_digest_reads SLI of the registry service has an
        apdex violating SLO
    expr: |
      (
        (
          gitlab_component_apdex:ratio_6h{component="server_route_blob_digest_reads",type="registry"}
          < (1 - 6 * 0.020000)
        )
        and
        (
          gitlab_component_apdex:ratio_30m{component="server_route_blob_digest_reads",type="registry"}
          < (1 - 6 * 0.020000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_6h{component="server_route_blob_digest_reads",type="registry"}) >= 0.16667
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "yes"
      window: 6h
  - alert: RegistryServiceServerRouteBlobDigestReadsTrafficCessation
    annotations:
      description: |
        All read-requests (GET or HEAD) for the blob endpoints on the registry.

        GET is used to pull a layer gated by the name of repository and uniquely identified by the digest in the registry.

        HEAD is used to check the existence of a layer.

        This alert signifies that the SLI is reporting a cessation of traffic; the signal is present, but is zero.
      grafana_dashboard_id: registry-main/registry-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/registry-main/registry-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3739996643"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(registry_http_request_duration_seconds_count{method=~"get|head",route="/v2/{name}/blobs/{digest}"}[5m])
        )
      runbook: docs/registry/README.md
      title: The server_route_blob_digest_reads SLI of the registry service has not
        received any traffic in the past 30m
    expr: |
      gitlab_component_ops:rate_30m{component="server_route_blob_digest_reads",type="registry"} == 0
      and
      gitlab_component_ops:rate_30m{component="server_route_blob_digest_reads",type="registry"} offset 1h >= 0.16666666666666666
    for: 5m
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
  - alert: RegistryServiceServerRouteBlobDigestReadsTrafficAbsent
    annotations:
      description: |
        All read-requests (GET or HEAD) for the blob endpoints on the registry.

        GET is used to pull a layer gated by the name of repository and uniquely identified by the digest in the registry.

        HEAD is used to check the existence of a layer.

        This alert signifies that the SLI was previously reporting traffic, but is no longer - the signal is absent.

        This could be caused by a change to the metrics used in the SLI, or by the service not receiving traffic.
      grafana_dashboard_id: registry-main/registry-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/registry-main/registry-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3739996643"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(registry_http_request_duration_seconds_count{method=~"get|head",route="/v2/{name}/blobs/{digest}"}[5m])
        )
      runbook: docs/registry/README.md
      title: The server_route_blob_digest_reads SLI of the registry service has not
        reported any traffic in the past 30m
    expr: |
      gitlab_component_ops:rate_5m{component="server_route_blob_digest_reads",type="registry"} offset 1h
      unless
      gitlab_component_ops:rate_5m{component="server_route_blob_digest_reads",type="registry"}
    for: 30m
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
  - alert: RegistryServiceServerRouteBlobDigestWritesApdexSLOViolation
    annotations:
      description: |
        Write requests (PUT or PATCH or POST) for the registry blob digest endpoints.

        Currently not part of the spec.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: registry-main/registry-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/registry-main/registry-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2614494702"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (le) (
            rate(registry_http_request_duration_seconds_bucket{method=~"patch|post|put",route="/v2/{name}/blobs/{digest}"}[5m])
          )
        )
      runbook: docs/registry/README.md
      title: The server_route_blob_digest_writes SLI of the registry service has an
        apdex violating SLO
    expr: |
      (
        (
          gitlab_component_apdex:ratio_1h{component="server_route_blob_digest_writes",type="registry"}
          < (1 - 14.4 * 0.003000)
        )
        and
        (
          gitlab_component_apdex:ratio_5m{component="server_route_blob_digest_writes",type="registry"}
          < (1 - 14.4 * 0.003000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_1h{component="server_route_blob_digest_writes",type="registry"}) >= 1
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "yes"
      window: 1h
  - alert: RegistryServiceServerRouteBlobDigestWritesApdexSLOViolation
    annotations:
      description: |
        Write requests (PUT or PATCH or POST) for the registry blob digest endpoints.

        Currently not part of the spec.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: registry-main/registry-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/registry-main/registry-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2614494702"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (le) (
            rate(registry_http_request_duration_seconds_bucket{method=~"patch|post|put",route="/v2/{name}/blobs/{digest}"}[5m])
          )
        )
      runbook: docs/registry/README.md
      title: The server_route_blob_digest_writes SLI of the registry service has an
        apdex violating SLO
    expr: |
      (
        (
          gitlab_component_apdex:ratio_6h{component="server_route_blob_digest_writes",type="registry"}
          < (1 - 6 * 0.003000)
        )
        and
        (
          gitlab_component_apdex:ratio_30m{component="server_route_blob_digest_writes",type="registry"}
          < (1 - 6 * 0.003000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_6h{component="server_route_blob_digest_writes",type="registry"}) >= 0.16667
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "yes"
      window: 6h
  - alert: RegistryServiceServerRouteBlobDigestWritesTrafficCessation
    annotations:
      description: |
        Write requests (PUT or PATCH or POST) for the registry blob digest endpoints.

        Currently not part of the spec.

        This alert signifies that the SLI is reporting a cessation of traffic; the signal is present, but is zero.
      grafana_dashboard_id: registry-main/registry-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/registry-main/registry-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2206725588"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(registry_http_request_duration_seconds_count{method=~"patch|post|put",route="/v2/{name}/blobs/{digest}"}[5m])
        )
      runbook: docs/registry/README.md
      title: The server_route_blob_digest_writes SLI of the registry service has not
        received any traffic in the past 30m
    expr: |
      gitlab_component_ops:rate_30m{component="server_route_blob_digest_writes",type="registry"} == 0
      and
      gitlab_component_ops:rate_30m{component="server_route_blob_digest_writes",type="registry"} offset 1h >= 0.16666666666666666
    for: 5m
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
  - alert: RegistryServiceServerRouteBlobDigestWritesTrafficAbsent
    annotations:
      description: |
        Write requests (PUT or PATCH or POST) for the registry blob digest endpoints.

        Currently not part of the spec.

        This alert signifies that the SLI was previously reporting traffic, but is no longer - the signal is absent.

        This could be caused by a change to the metrics used in the SLI, or by the service not receiving traffic.
      grafana_dashboard_id: registry-main/registry-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/registry-main/registry-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2206725588"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(registry_http_request_duration_seconds_count{method=~"patch|post|put",route="/v2/{name}/blobs/{digest}"}[5m])
        )
      runbook: docs/registry/README.md
      title: The server_route_blob_digest_writes SLI of the registry service has not
        reported any traffic in the past 30m
    expr: |
      gitlab_component_ops:rate_5m{component="server_route_blob_digest_writes",type="registry"} offset 1h
      unless
      gitlab_component_ops:rate_5m{component="server_route_blob_digest_writes",type="registry"}
    for: 30m
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
  - alert: RegistryServiceServerRouteBlobUploadUuidDeletesApdexSLOViolation
    annotations:
      description: |
        Delete requests for the registry blob upload endpoints.

        Used to cancel outstanding upload processes, releasing associated resources.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: registry-main/registry-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/registry-main/registry-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2147876809"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (le) (
            rate(registry_http_request_duration_seconds_bucket{method=~"delete",route="/v2/{name}/blobs/uploads/{uuid}"}[5m])
          )
        )
      runbook: docs/registry/README.md
      title: The server_route_blob_upload_uuid_deletes SLI of the registry service
        has an apdex violating SLO
    expr: |
      (
        (
          gitlab_component_apdex:ratio_1h{component="server_route_blob_upload_uuid_deletes",type="registry"}
          < (1 - 14.4 * 0.003000)
        )
        and
        (
          gitlab_component_apdex:ratio_5m{component="server_route_blob_upload_uuid_deletes",type="registry"}
          < (1 - 14.4 * 0.003000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_1h{component="server_route_blob_upload_uuid_deletes",type="registry"}) >= 1
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "yes"
      window: 1h
  - alert: RegistryServiceServerRouteBlobUploadUuidDeletesApdexSLOViolation
    annotations:
      description: |
        Delete requests for the registry blob upload endpoints.

        Used to cancel outstanding upload processes, releasing associated resources.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: registry-main/registry-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/registry-main/registry-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2147876809"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (le) (
            rate(registry_http_request_duration_seconds_bucket{method=~"delete",route="/v2/{name}/blobs/uploads/{uuid}"}[5m])
          )
        )
      runbook: docs/registry/README.md
      title: The server_route_blob_upload_uuid_deletes SLI of the registry service
        has an apdex violating SLO
    expr: |
      (
        (
          gitlab_component_apdex:ratio_6h{component="server_route_blob_upload_uuid_deletes",type="registry"}
          < (1 - 6 * 0.003000)
        )
        and
        (
          gitlab_component_apdex:ratio_30m{component="server_route_blob_upload_uuid_deletes",type="registry"}
          < (1 - 6 * 0.003000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_6h{component="server_route_blob_upload_uuid_deletes",type="registry"}) >= 0.16667
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "yes"
      window: 6h
  - alert: RegistryServiceServerRouteBlobUploadUuidDeletesTrafficCessation
    annotations:
      description: |
        Delete requests for the registry blob upload endpoints.

        Used to cancel outstanding upload processes, releasing associated resources.

        This alert signifies that the SLI is reporting a cessation of traffic; the signal is present, but is zero.
      grafana_dashboard_id: registry-main/registry-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/registry-main/registry-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "714276442"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(registry_http_request_duration_seconds_count{method=~"delete",route="/v2/{name}/blobs/uploads/{uuid}"}[5m])
        )
      runbook: docs/registry/README.md
      title: The server_route_blob_upload_uuid_deletes SLI of the registry service
        has not received any traffic in the past 30m
    expr: |
      gitlab_component_ops:rate_30m{component="server_route_blob_upload_uuid_deletes",type="registry"} == 0
      and
      gitlab_component_ops:rate_30m{component="server_route_blob_upload_uuid_deletes",type="registry"} offset 1h >= 0.16666666666666666
    for: 5m
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
  - alert: RegistryServiceServerRouteBlobUploadUuidDeletesTrafficAbsent
    annotations:
      description: |
        Delete requests for the registry blob upload endpoints.

        Used to cancel outstanding upload processes, releasing associated resources.

        This alert signifies that the SLI was previously reporting traffic, but is no longer - the signal is absent.

        This could be caused by a change to the metrics used in the SLI, or by the service not receiving traffic.
      grafana_dashboard_id: registry-main/registry-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/registry-main/registry-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "714276442"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(registry_http_request_duration_seconds_count{method=~"delete",route="/v2/{name}/blobs/uploads/{uuid}"}[5m])
        )
      runbook: docs/registry/README.md
      title: The server_route_blob_upload_uuid_deletes SLI of the registry service
        has not reported any traffic in the past 30m
    expr: |
      gitlab_component_ops:rate_5m{component="server_route_blob_upload_uuid_deletes",type="registry"} offset 1h
      unless
      gitlab_component_ops:rate_5m{component="server_route_blob_upload_uuid_deletes",type="registry"}
    for: 30m
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
  - alert: RegistryServiceServerRouteBlobUploadUuidReadsApdexSLOViolation
    annotations:
      description: |
        Read requests (GET) for the registry blob upload endpoints.

        GET is used to retrieve the current status of a resumable upload.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: registry-main/registry-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/registry-main/registry-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "41208249"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (le) (
            rate(registry_http_request_duration_seconds_bucket{method=~"get|head",route="/v2/{name}/blobs/uploads/{uuid}"}[5m])
          )
        )
      runbook: docs/registry/README.md
      title: The server_route_blob_upload_uuid_reads SLI of the registry service has
        an apdex violating SLO
    expr: |
      (
        (
          gitlab_component_apdex:ratio_1h{component="server_route_blob_upload_uuid_reads",type="registry"}
          < (1 - 14.4 * 0.003000)
        )
        and
        (
          gitlab_component_apdex:ratio_5m{component="server_route_blob_upload_uuid_reads",type="registry"}
          < (1 - 14.4 * 0.003000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_1h{component="server_route_blob_upload_uuid_reads",type="registry"}) >= 1
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "yes"
      window: 1h
  - alert: RegistryServiceServerRouteBlobUploadUuidReadsApdexSLOViolation
    annotations:
      description: |
        Read requests (GET) for the registry blob upload endpoints.

        GET is used to retrieve the current status of a resumable upload.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: registry-main/registry-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/registry-main/registry-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "41208249"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (le) (
            rate(registry_http_request_duration_seconds_bucket{method=~"get|head",route="/v2/{name}/blobs/uploads/{uuid}"}[5m])
          )
        )
      runbook: docs/registry/README.md
      title: The server_route_blob_upload_uuid_reads SLI of the registry service has
        an apdex violating SLO
    expr: |
      (
        (
          gitlab_component_apdex:ratio_6h{component="server_route_blob_upload_uuid_reads",type="registry"}
          < (1 - 6 * 0.003000)
        )
        and
        (
          gitlab_component_apdex:ratio_30m{component="server_route_blob_upload_uuid_reads",type="registry"}
          < (1 - 6 * 0.003000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_6h{component="server_route_blob_upload_uuid_reads",type="registry"}) >= 0.16667
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "yes"
      window: 6h
  - alert: RegistryServiceServerRouteBlobUploadUuidWritesApdexSLOViolation
    annotations:
      description: |
        Write requests (PUT or PATCH) for the registry blob upload endpoints.

        PUT is used to complete the upload specified by uuid, optionally appending the body as the final chunk.

        PATCH is used to upload a chunk of data for the specified upload.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: registry-main/registry-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/registry-main/registry-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2199109348"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (le) (
            rate(registry_http_request_duration_seconds_bucket{method=~"put|patch|post",route="/v2/{name}/blobs/uploads/{uuid}"}[5m])
          )
        )
      runbook: docs/registry/README.md
      title: The server_route_blob_upload_uuid_writes SLI of the registry service
        has an apdex violating SLO
    expr: |
      (
        (
          gitlab_component_apdex:ratio_1h{component="server_route_blob_upload_uuid_writes",type="registry"}
          < (1 - 14.4 * 0.030000)
        )
        and
        (
          gitlab_component_apdex:ratio_5m{component="server_route_blob_upload_uuid_writes",type="registry"}
          < (1 - 14.4 * 0.030000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_1h{component="server_route_blob_upload_uuid_writes",type="registry"}) >= 1
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "yes"
      window: 1h
  - alert: RegistryServiceServerRouteBlobUploadUuidWritesApdexSLOViolation
    annotations:
      description: |
        Write requests (PUT or PATCH) for the registry blob upload endpoints.

        PUT is used to complete the upload specified by uuid, optionally appending the body as the final chunk.

        PATCH is used to upload a chunk of data for the specified upload.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: registry-main/registry-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/registry-main/registry-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2199109348"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (le) (
            rate(registry_http_request_duration_seconds_bucket{method=~"put|patch|post",route="/v2/{name}/blobs/uploads/{uuid}"}[5m])
          )
        )
      runbook: docs/registry/README.md
      title: The server_route_blob_upload_uuid_writes SLI of the registry service
        has an apdex violating SLO
    expr: |
      (
        (
          gitlab_component_apdex:ratio_6h{component="server_route_blob_upload_uuid_writes",type="registry"}
          < (1 - 6 * 0.030000)
        )
        and
        (
          gitlab_component_apdex:ratio_30m{component="server_route_blob_upload_uuid_writes",type="registry"}
          < (1 - 6 * 0.030000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_6h{component="server_route_blob_upload_uuid_writes",type="registry"}) >= 0.16667
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "yes"
      window: 6h
  - alert: RegistryServiceServerRouteBlobUploadUuidWritesTrafficCessation
    annotations:
      description: |
        Write requests (PUT or PATCH) for the registry blob upload endpoints.

        PUT is used to complete the upload specified by uuid, optionally appending the body as the final chunk.

        PATCH is used to upload a chunk of data for the specified upload.

        This alert signifies that the SLI is reporting a cessation of traffic; the signal is present, but is zero.
      grafana_dashboard_id: registry-main/registry-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/registry-main/registry-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3946104260"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(registry_http_request_duration_seconds_count{method=~"put|patch|post",route="/v2/{name}/blobs/uploads/{uuid}"}[5m])
        )
      runbook: docs/registry/README.md
      title: The server_route_blob_upload_uuid_writes SLI of the registry service
        has not received any traffic in the past 30m
    expr: |
      gitlab_component_ops:rate_30m{component="server_route_blob_upload_uuid_writes",type="registry"} == 0
      and
      gitlab_component_ops:rate_30m{component="server_route_blob_upload_uuid_writes",type="registry"} offset 1h >= 0.16666666666666666
    for: 5m
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
  - alert: RegistryServiceServerRouteBlobUploadUuidWritesTrafficAbsent
    annotations:
      description: |
        Write requests (PUT or PATCH) for the registry blob upload endpoints.

        PUT is used to complete the upload specified by uuid, optionally appending the body as the final chunk.

        PATCH is used to upload a chunk of data for the specified upload.

        This alert signifies that the SLI was previously reporting traffic, but is no longer - the signal is absent.

        This could be caused by a change to the metrics used in the SLI, or by the service not receiving traffic.
      grafana_dashboard_id: registry-main/registry-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/registry-main/registry-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3946104260"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(registry_http_request_duration_seconds_count{method=~"put|patch|post",route="/v2/{name}/blobs/uploads/{uuid}"}[5m])
        )
      runbook: docs/registry/README.md
      title: The server_route_blob_upload_uuid_writes SLI of the registry service
        has not reported any traffic in the past 30m
    expr: |
      gitlab_component_ops:rate_5m{component="server_route_blob_upload_uuid_writes",type="registry"} offset 1h
      unless
      gitlab_component_ops:rate_5m{component="server_route_blob_upload_uuid_writes",type="registry"}
    for: 30m
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
- interval: 1m
  name: 'Service Component Alerts: sidekiq'
  rules:
  - alert: SidekiqServiceEmailReceiverErrorSLOViolation
    annotations:
      description: |
        Monitors ratio between all received emails and received emails which could not be processed for some reason.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: sidekiq-main/sidekiq-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/sidekiq-main/sidekiq-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "212053046"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(gitlab_transaction_event_email_receiver_error_total{error!="Gitlab::Email::AutoGeneratedEmailError"}[5m])
        )
      runbook: docs/sidekiq/README.md
      title: The email_receiver SLI of the sidekiq service has an error rate violating
        SLO
    expr: |
      (
        (
          gitlab_component_errors:ratio_1h{component="email_receiver",type="sidekiq"}
          > (14.4 * 0.300000)
        )
        and
        (
          gitlab_component_errors:ratio_5m{component="email_receiver",type="sidekiq"}
          > (14.4 * 0.300000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_1h{component="email_receiver",type="sidekiq"}) >= 1
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      rules_domain: general
      severity: s3
      sli_type: error
      slo_alert: "yes"
      user_impacting: "yes"
      window: 1h
  - alert: SidekiqServiceEmailReceiverErrorSLOViolation
    annotations:
      description: |
        Monitors ratio between all received emails and received emails which could not be processed for some reason.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: sidekiq-main/sidekiq-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/sidekiq-main/sidekiq-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "212053046"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(gitlab_transaction_event_email_receiver_error_total{error!="Gitlab::Email::AutoGeneratedEmailError"}[5m])
        )
      runbook: docs/sidekiq/README.md
      title: The email_receiver SLI of the sidekiq service has an error rate violating
        SLO
    expr: |
      (
        (
          gitlab_component_errors:ratio_6h{component="email_receiver",type="sidekiq"}
          > (6 * 0.300000)
        )
        and
        (
          gitlab_component_errors:ratio_30m{component="email_receiver",type="sidekiq"}
          > (6 * 0.300000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_6h{component="email_receiver",type="sidekiq"}) >= 0.16667
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      rules_domain: general
      severity: s3
      sli_type: error
      slo_alert: "yes"
      user_impacting: "yes"
      window: 6h
  - alert: SidekiqServiceEmailReceiverTrafficCessation
    annotations:
      description: |
        Monitors ratio between all received emails and received emails which could not be processed for some reason.

        This alert signifies that the SLI is reporting a cessation of traffic; the signal is present, but is zero.
      grafana_dashboard_id: sidekiq-main/sidekiq-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/sidekiq-main/sidekiq-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1545803555"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(sidekiq_jobs_completion_seconds_count{worker=~"EmailReceiverWorker|ServiceDeskEmailReceiverWorker"}[5m])
        )
      runbook: docs/sidekiq/README.md
      title: The email_receiver SLI of the sidekiq service has not received any traffic
        in the past 30m
    expr: |
      gitlab_component_ops:rate_30m{component="email_receiver",type="sidekiq"} == 0
      and
      gitlab_component_ops:rate_30m{component="email_receiver",type="sidekiq"} offset 1h >= 0.16666666666666666
    for: 5m
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      rules_domain: general
      severity: s3
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
  - alert: SidekiqServiceEmailReceiverTrafficAbsent
    annotations:
      description: |
        Monitors ratio between all received emails and received emails which could not be processed for some reason.

        This alert signifies that the SLI was previously reporting traffic, but is no longer - the signal is absent.

        This could be caused by a change to the metrics used in the SLI, or by the service not receiving traffic.
      grafana_dashboard_id: sidekiq-main/sidekiq-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/sidekiq-main/sidekiq-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1545803555"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(sidekiq_jobs_completion_seconds_count{worker=~"EmailReceiverWorker|ServiceDeskEmailReceiverWorker"}[5m])
        )
      runbook: docs/sidekiq/README.md
      title: The email_receiver SLI of the sidekiq service has not reported any traffic
        in the past 30m
    expr: |
      gitlab_component_ops:rate_5m{component="email_receiver",type="sidekiq"} offset 1h
      unless
      gitlab_component_ops:rate_5m{component="email_receiver",type="sidekiq"}
    for: 30m
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      rules_domain: general
      severity: s3
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
  - alert: SidekiqServiceShardCatchallApdexSLOViolation
    annotations:
      description: |
        All Sidekiq jobs

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: sidekiq-main/sidekiq-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/sidekiq-main/sidekiq-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "4125397901"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (le) (
            sli_aggregations:sidekiq_jobs_completion_seconds_bucket_rate5m{urgency="high"}
            or
            sli_aggregations:sidekiq_jobs_queue_duration_seconds_bucket_rate5m{urgency="high"}
            or
            sli_aggregations:sidekiq_jobs_completion_seconds_bucket_rate5m{urgency="low"}
            or
            sli_aggregations:sidekiq_jobs_queue_duration_seconds_bucket_rate5m{urgency="low"}
            or
            sli_aggregations:sidekiq_jobs_completion_seconds_bucket_rate5m{urgency="throttled"}
            or
            sli_aggregations:sidekiq_jobs_completion_seconds_bucket_rate5m{urgency=""}
            or
            sli_aggregations:sidekiq_jobs_queue_duration_seconds_bucket_rate5m{urgency=""}
          )
        )
      runbook: docs/sidekiq/README.md
      title: The shard_catchall SLI of the sidekiq service has an apdex violating
        SLO
    expr: |
      (
        (
          gitlab_component_apdex:ratio_1h{component="shard_catchall",type="sidekiq"}
          < (1 - 14.4 * 0.005000)
        )
        and
        (
          gitlab_component_apdex:ratio_5m{component="shard_catchall",type="sidekiq"}
          < (1 - 14.4 * 0.005000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_1h{component="shard_catchall",type="sidekiq"}) >= 1
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "yes"
      window: 1h
  - alert: SidekiqServiceShardCatchallApdexSLOViolation
    annotations:
      description: |
        All Sidekiq jobs

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: sidekiq-main/sidekiq-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/sidekiq-main/sidekiq-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "4125397901"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (le) (
            sli_aggregations:sidekiq_jobs_completion_seconds_bucket_rate5m{urgency="high"}
            or
            sli_aggregations:sidekiq_jobs_queue_duration_seconds_bucket_rate5m{urgency="high"}
            or
            sli_aggregations:sidekiq_jobs_completion_seconds_bucket_rate5m{urgency="low"}
            or
            sli_aggregations:sidekiq_jobs_queue_duration_seconds_bucket_rate5m{urgency="low"}
            or
            sli_aggregations:sidekiq_jobs_completion_seconds_bucket_rate5m{urgency="throttled"}
            or
            sli_aggregations:sidekiq_jobs_completion_seconds_bucket_rate5m{urgency=""}
            or
            sli_aggregations:sidekiq_jobs_queue_duration_seconds_bucket_rate5m{urgency=""}
          )
        )
      runbook: docs/sidekiq/README.md
      title: The shard_catchall SLI of the sidekiq service has an apdex violating
        SLO
    expr: |
      (
        (
          gitlab_component_apdex:ratio_6h{component="shard_catchall",type="sidekiq"}
          < (1 - 6 * 0.005000)
        )
        and
        (
          gitlab_component_apdex:ratio_30m{component="shard_catchall",type="sidekiq"}
          < (1 - 6 * 0.005000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_6h{component="shard_catchall",type="sidekiq"}) >= 0.16667
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "yes"
      window: 6h
  - alert: SidekiqServiceShardCatchallErrorSLOViolation
    annotations:
      description: |
        All Sidekiq jobs

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: sidekiq-main/sidekiq-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/sidekiq-main/sidekiq-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "532783926"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          sli_aggregations:sidekiq_jobs_failed_total_rate5m{}
        )
      runbook: docs/sidekiq/README.md
      title: The shard_catchall SLI of the sidekiq service has an error rate violating
        SLO
    expr: |
      (
        (
          gitlab_component_errors:ratio_1h{component="shard_catchall",type="sidekiq"}
          > (14.4 * 0.005000)
        )
        and
        (
          gitlab_component_errors:ratio_5m{component="shard_catchall",type="sidekiq"}
          > (14.4 * 0.005000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_1h{component="shard_catchall",type="sidekiq"}) >= 1
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "yes"
      window: 1h
  - alert: SidekiqServiceShardCatchallErrorSLOViolation
    annotations:
      description: |
        All Sidekiq jobs

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: sidekiq-main/sidekiq-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/sidekiq-main/sidekiq-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "532783926"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          sli_aggregations:sidekiq_jobs_failed_total_rate5m{}
        )
      runbook: docs/sidekiq/README.md
      title: The shard_catchall SLI of the sidekiq service has an error rate violating
        SLO
    expr: |
      (
        (
          gitlab_component_errors:ratio_6h{component="shard_catchall",type="sidekiq"}
          > (6 * 0.005000)
        )
        and
        (
          gitlab_component_errors:ratio_30m{component="shard_catchall",type="sidekiq"}
          > (6 * 0.005000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_6h{component="shard_catchall",type="sidekiq"}) >= 0.16667
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "yes"
      window: 6h
  - alert: SidekiqServiceShardCatchallTrafficCessation
    annotations:
      description: |
        All Sidekiq jobs

        This alert signifies that the SLI is reporting a cessation of traffic; the signal is present, but is zero.
      grafana_dashboard_id: sidekiq-main/sidekiq-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/sidekiq-main/sidekiq-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2518816313"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          sli_aggregations:sidekiq_jobs_completion_seconds_bucket_rate5m{le="+Inf"}
        )
      runbook: docs/sidekiq/README.md
      title: The shard_catchall SLI of the sidekiq service has not received any traffic
        in the past 30m
    expr: |
      gitlab_component_ops:rate_30m{component="shard_catchall",type="sidekiq"} == 0
      and
      gitlab_component_ops:rate_30m{component="shard_catchall",type="sidekiq"} offset 1h >= 0.16666666666666666
    for: 5m
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
  - alert: SidekiqServiceShardCatchallTrafficAbsent
    annotations:
      description: |
        All Sidekiq jobs

        This alert signifies that the SLI was previously reporting traffic, but is no longer - the signal is absent.

        This could be caused by a change to the metrics used in the SLI, or by the service not receiving traffic.
      grafana_dashboard_id: sidekiq-main/sidekiq-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/sidekiq-main/sidekiq-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2518816313"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          sli_aggregations:sidekiq_jobs_completion_seconds_bucket_rate5m{le="+Inf"}
        )
      runbook: docs/sidekiq/README.md
      title: The shard_catchall SLI of the sidekiq service has not reported any traffic
        in the past 30m
    expr: |
      gitlab_component_ops:rate_5m{component="shard_catchall",type="sidekiq"} offset 1h
      unless
      gitlab_component_ops:rate_5m{component="shard_catchall",type="sidekiq"}
    for: 30m
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
- interval: 1m
  name: 'Service Component Alerts: webservice'
  rules:
  - alert: WebserviceServicePumaApdexSLOViolation
    annotations:
      description: |
        Aggregation of most web requests that pass through the puma to the GitLab rails monolith. Healthchecks are excluded.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: webservice-main/webservice-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/webservice-main/webservice-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "561966508"
      grafana_variables: environment,stage
      runbook: docs/webservice/README.md
      title: The puma SLI of the webservice service has an apdex violating SLO
    expr: |
      (
        (
          gitlab_component_apdex:ratio_1h{component="puma",type="webservice"}
          < (1 - 14.4 * 0.002000)
        )
        and
        (
          gitlab_component_apdex:ratio_5m{component="puma",type="webservice"}
          < (1 - 14.4 * 0.002000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_1h{component="puma",type="webservice"}) >= 1
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "yes"
      window: 1h
  - alert: WebserviceServicePumaApdexSLOViolation
    annotations:
      description: |
        Aggregation of most web requests that pass through the puma to the GitLab rails monolith. Healthchecks are excluded.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: webservice-main/webservice-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/webservice-main/webservice-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "561966508"
      grafana_variables: environment,stage
      runbook: docs/webservice/README.md
      title: The puma SLI of the webservice service has an apdex violating SLO
    expr: |
      (
        (
          gitlab_component_apdex:ratio_6h{component="puma",type="webservice"}
          < (1 - 6 * 0.002000)
        )
        and
        (
          gitlab_component_apdex:ratio_30m{component="puma",type="webservice"}
          < (1 - 6 * 0.002000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_6h{component="puma",type="webservice"}) >= 0.16667
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "yes"
      window: 6h
  - alert: WebserviceServicePumaErrorSLOViolation
    annotations:
      description: |
        Aggregation of most web requests that pass through the puma to the GitLab rails monolith. Healthchecks are excluded.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: webservice-main/webservice-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/webservice-main/webservice-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2425637513"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(http_requests_total{job="gitlab-rails",status=~"5.."}[5m])
        )
      runbook: docs/webservice/README.md
      title: The puma SLI of the webservice service has an error rate violating SLO
    expr: |
      (
        (
          gitlab_component_errors:ratio_1h{component="puma",type="webservice"}
          > (14.4 * 0.000100)
        )
        and
        (
          gitlab_component_errors:ratio_5m{component="puma",type="webservice"}
          > (14.4 * 0.000100)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_1h{component="puma",type="webservice"}) >= 1
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "yes"
      window: 1h
  - alert: WebserviceServicePumaErrorSLOViolation
    annotations:
      description: |
        Aggregation of most web requests that pass through the puma to the GitLab rails monolith. Healthchecks are excluded.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: webservice-main/webservice-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/webservice-main/webservice-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2425637513"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(http_requests_total{job="gitlab-rails",status=~"5.."}[5m])
        )
      runbook: docs/webservice/README.md
      title: The puma SLI of the webservice service has an error rate violating SLO
    expr: |
      (
        (
          gitlab_component_errors:ratio_6h{component="puma",type="webservice"}
          > (6 * 0.000100)
        )
        and
        (
          gitlab_component_errors:ratio_30m{component="puma",type="webservice"}
          > (6 * 0.000100)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_6h{component="puma",type="webservice"}) >= 0.16667
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "yes"
      window: 6h
  - alert: WebserviceServicePumaTrafficCessation
    annotations:
      description: |
        Aggregation of most web requests that pass through the puma to the GitLab rails monolith. Healthchecks are excluded.

        This alert signifies that the SLI is reporting a cessation of traffic; the signal is present, but is zero.
      grafana_dashboard_id: webservice-main/webservice-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/webservice-main/webservice-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "110019945"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(http_requests_total{job="gitlab-rails"}[5m])
        )
      runbook: docs/webservice/README.md
      title: The puma SLI of the webservice service has not received any traffic in
        the past 30m
    expr: |
      gitlab_component_ops:rate_30m{component="puma",type="webservice"} == 0
      and
      gitlab_component_ops:rate_30m{component="puma",type="webservice"} offset 1h >= 0.16666666666666666
    for: 5m
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
  - alert: WebserviceServicePumaTrafficAbsent
    annotations:
      description: |
        Aggregation of most web requests that pass through the puma to the GitLab rails monolith. Healthchecks are excluded.

        This alert signifies that the SLI was previously reporting traffic, but is no longer - the signal is absent.

        This could be caused by a change to the metrics used in the SLI, or by the service not receiving traffic.
      grafana_dashboard_id: webservice-main/webservice-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/webservice-main/webservice-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "110019945"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(http_requests_total{job="gitlab-rails"}[5m])
        )
      runbook: docs/webservice/README.md
      title: The puma SLI of the webservice service has not reported any traffic in
        the past 30m
    expr: |
      gitlab_component_ops:rate_5m{component="puma",type="webservice"} offset 1h
      unless
      gitlab_component_ops:rate_5m{component="puma",type="webservice"}
    for: 30m
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
  - alert: WebserviceServiceWorkhorseApdexSLOViolation
    annotations:
      description: |
        Aggregation of most rails requests that pass through workhorse, monitored via the HTTP interface. Excludes API requests health, readiness and liveness requests. Some known slow requests, such as HTTP uploads, are excluded from the apdex score.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: webservice-main/webservice-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/webservice-main/webservice-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2586818196"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (le) (
            rate(gitlab_workhorse_http_request_duration_seconds_bucket{route!="^/-/health$",route!="^/-/(readiness|liveness)$",route!="^/([^/]+/){1,}[^/]+/uploads\\z",route!="^/([^/]+/){1,}[^/]+\\.git/git-receive-pack\\z",route!="^/([^/]+/){1,}[^/]+\\.git/git-upload-pack\\z",route!="^/([^/]+/){1,}[^/]+\\.git/info/refs\\z",route!="^/([^/]+/){1,}[^/]+\\.git/gitlab-lfs/objects/([0-9a-f]{64})/([0-9]+)\\z",route!="^/-/cable\\z",route!~"^\\^/api/.*"}[5m])
          )
        )
      runbook: docs/webservice/README.md
      title: The workhorse SLI of the webservice service has an apdex violating SLO
    expr: |
      (
        (
          gitlab_component_apdex:ratio_1h{component="workhorse",type="webservice"}
          < (1 - 14.4 * 0.002000)
        )
        and
        (
          gitlab_component_apdex:ratio_5m{component="workhorse",type="webservice"}
          < (1 - 14.4 * 0.002000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_1h{component="workhorse",type="webservice"}) >= 1
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "yes"
      window: 1h
  - alert: WebserviceServiceWorkhorseApdexSLOViolation
    annotations:
      description: |
        Aggregation of most rails requests that pass through workhorse, monitored via the HTTP interface. Excludes API requests health, readiness and liveness requests. Some known slow requests, such as HTTP uploads, are excluded from the apdex score.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: webservice-main/webservice-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/webservice-main/webservice-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2586818196"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (le) (
            rate(gitlab_workhorse_http_request_duration_seconds_bucket{route!="^/-/health$",route!="^/-/(readiness|liveness)$",route!="^/([^/]+/){1,}[^/]+/uploads\\z",route!="^/([^/]+/){1,}[^/]+\\.git/git-receive-pack\\z",route!="^/([^/]+/){1,}[^/]+\\.git/git-upload-pack\\z",route!="^/([^/]+/){1,}[^/]+\\.git/info/refs\\z",route!="^/([^/]+/){1,}[^/]+\\.git/gitlab-lfs/objects/([0-9a-f]{64})/([0-9]+)\\z",route!="^/-/cable\\z",route!~"^\\^/api/.*"}[5m])
          )
        )
      runbook: docs/webservice/README.md
      title: The workhorse SLI of the webservice service has an apdex violating SLO
    expr: |
      (
        (
          gitlab_component_apdex:ratio_6h{component="workhorse",type="webservice"}
          < (1 - 6 * 0.002000)
        )
        and
        (
          gitlab_component_apdex:ratio_30m{component="workhorse",type="webservice"}
          < (1 - 6 * 0.002000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_6h{component="workhorse",type="webservice"}) >= 0.16667
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "yes"
      window: 6h
  - alert: WebserviceServiceWorkhorseErrorSLOViolation
    annotations:
      description: |
        Aggregation of most rails requests that pass through workhorse, monitored via the HTTP interface. Excludes API requests health, readiness and liveness requests. Some known slow requests, such as HTTP uploads, are excluded from the apdex score.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: webservice-main/webservice-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/webservice-main/webservice-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2047984749"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(gitlab_workhorse_http_requests_total{code=~"^5.*",route!="^/-/health$",route!="^/-/(readiness|liveness)$",route!~"^\\^/api/.*"}[5m])
        )
      runbook: docs/webservice/README.md
      title: The workhorse SLI of the webservice service has an error rate violating
        SLO
    expr: |
      (
        (
          gitlab_component_errors:ratio_1h{component="workhorse",type="webservice"}
          > (14.4 * 0.000100)
        )
        and
        (
          gitlab_component_errors:ratio_5m{component="workhorse",type="webservice"}
          > (14.4 * 0.000100)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_1h{component="workhorse",type="webservice"}) >= 1
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "yes"
      window: 1h
  - alert: WebserviceServiceWorkhorseErrorSLOViolation
    annotations:
      description: |
        Aggregation of most rails requests that pass through workhorse, monitored via the HTTP interface. Excludes API requests health, readiness and liveness requests. Some known slow requests, such as HTTP uploads, are excluded from the apdex score.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: webservice-main/webservice-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/webservice-main/webservice-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2047984749"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(gitlab_workhorse_http_requests_total{code=~"^5.*",route!="^/-/health$",route!="^/-/(readiness|liveness)$",route!~"^\\^/api/.*"}[5m])
        )
      runbook: docs/webservice/README.md
      title: The workhorse SLI of the webservice service has an error rate violating
        SLO
    expr: |
      (
        (
          gitlab_component_errors:ratio_6h{component="workhorse",type="webservice"}
          > (6 * 0.000100)
        )
        and
        (
          gitlab_component_errors:ratio_30m{component="workhorse",type="webservice"}
          > (6 * 0.000100)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_6h{component="workhorse",type="webservice"}) >= 0.16667
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "yes"
      window: 6h
  - alert: WebserviceServiceWorkhorseTrafficCessation
    annotations:
      description: |
        Aggregation of most rails requests that pass through workhorse, monitored via the HTTP interface. Excludes API requests health, readiness and liveness requests. Some known slow requests, such as HTTP uploads, are excluded from the apdex score.

        This alert signifies that the SLI is reporting a cessation of traffic; the signal is present, but is zero.
      grafana_dashboard_id: webservice-main/webservice-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/webservice-main/webservice-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "831392945"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(gitlab_workhorse_http_requests_total{route!="^/-/health$",route!="^/-/(readiness|liveness)$",route!~"^\\^/api/.*"}[5m])
        )
      runbook: docs/webservice/README.md
      title: The workhorse SLI of the webservice service has not received any traffic
        in the past 30m
    expr: |
      gitlab_component_ops:rate_30m{component="workhorse",type="webservice"} == 0
      and
      gitlab_component_ops:rate_30m{component="workhorse",type="webservice"} offset 1h >= 0.16666666666666666
    for: 5m
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
  - alert: WebserviceServiceWorkhorseTrafficAbsent
    annotations:
      description: |
        Aggregation of most rails requests that pass through workhorse, monitored via the HTTP interface. Excludes API requests health, readiness and liveness requests. Some known slow requests, such as HTTP uploads, are excluded from the apdex score.

        This alert signifies that the SLI was previously reporting traffic, but is no longer - the signal is absent.

        This could be caused by a change to the metrics used in the SLI, or by the service not receiving traffic.
      grafana_dashboard_id: webservice-main/webservice-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/webservice-main/webservice-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "831392945"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(gitlab_workhorse_http_requests_total{route!="^/-/health$",route!="^/-/(readiness|liveness)$",route!~"^\\^/api/.*"}[5m])
        )
      runbook: docs/webservice/README.md
      title: The workhorse SLI of the webservice service has not reported any traffic
        in the past 30m
    expr: |
      gitlab_component_ops:rate_5m{component="workhorse",type="webservice"} offset 1h
      unless
      gitlab_component_ops:rate_5m{component="workhorse",type="webservice"}
    for: 30m
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
  - alert: WebserviceServiceWorkhorseApiApdexSLOViolation
    annotations:
      description: |
        Aggregation of most API requests that pass through workhorse, monitored via the HTTP interface.

        The workhorse API apdex has a longer apdex latency than the web to allow for slow API requests.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: webservice-main/webservice-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/webservice-main/webservice-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1061500963"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (le) (
            rate(gitlab_workhorse_http_request_duration_seconds_bucket{route!="^/-/health$",route!="^/-/(readiness|liveness)$",route=~"^\\^/api/.*"}[5m])
          )
        )
      runbook: docs/webservice/README.md
      title: The workhorse_api SLI of the webservice service has an apdex violating
        SLO
    expr: |
      (
        (
          gitlab_component_apdex:ratio_1h{component="workhorse_api",type="webservice"}
          < (1 - 14.4 * 0.002000)
        )
        and
        (
          gitlab_component_apdex:ratio_5m{component="workhorse_api",type="webservice"}
          < (1 - 14.4 * 0.002000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_1h{component="workhorse_api",type="webservice"}) >= 1
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "yes"
      window: 1h
  - alert: WebserviceServiceWorkhorseApiApdexSLOViolation
    annotations:
      description: |
        Aggregation of most API requests that pass through workhorse, monitored via the HTTP interface.

        The workhorse API apdex has a longer apdex latency than the web to allow for slow API requests.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: webservice-main/webservice-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/webservice-main/webservice-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1061500963"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (le) (
            rate(gitlab_workhorse_http_request_duration_seconds_bucket{route!="^/-/health$",route!="^/-/(readiness|liveness)$",route=~"^\\^/api/.*"}[5m])
          )
        )
      runbook: docs/webservice/README.md
      title: The workhorse_api SLI of the webservice service has an apdex violating
        SLO
    expr: |
      (
        (
          gitlab_component_apdex:ratio_6h{component="workhorse_api",type="webservice"}
          < (1 - 6 * 0.002000)
        )
        and
        (
          gitlab_component_apdex:ratio_30m{component="workhorse_api",type="webservice"}
          < (1 - 6 * 0.002000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_6h{component="workhorse_api",type="webservice"}) >= 0.16667
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "yes"
      window: 6h
  - alert: WebserviceServiceWorkhorseApiErrorSLOViolation
    annotations:
      description: |
        Aggregation of most API requests that pass through workhorse, monitored via the HTTP interface.

        The workhorse API apdex has a longer apdex latency than the web to allow for slow API requests.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: webservice-main/webservice-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/webservice-main/webservice-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1957610638"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(gitlab_workhorse_http_requests_total{code=~"^5.*",route!="^/-/health$",route!="^/-/(readiness|liveness)$",route=~"^\\^/api/.*"}[5m])
        )
      runbook: docs/webservice/README.md
      title: The workhorse_api SLI of the webservice service has an error rate violating
        SLO
    expr: |
      (
        (
          gitlab_component_errors:ratio_1h{component="workhorse_api",type="webservice"}
          > (14.4 * 0.000100)
        )
        and
        (
          gitlab_component_errors:ratio_5m{component="workhorse_api",type="webservice"}
          > (14.4 * 0.000100)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_1h{component="workhorse_api",type="webservice"}) >= 1
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "yes"
      window: 1h
  - alert: WebserviceServiceWorkhorseApiErrorSLOViolation
    annotations:
      description: |
        Aggregation of most API requests that pass through workhorse, monitored via the HTTP interface.

        The workhorse API apdex has a longer apdex latency than the web to allow for slow API requests.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: webservice-main/webservice-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/webservice-main/webservice-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1957610638"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(gitlab_workhorse_http_requests_total{code=~"^5.*",route!="^/-/health$",route!="^/-/(readiness|liveness)$",route=~"^\\^/api/.*"}[5m])
        )
      runbook: docs/webservice/README.md
      title: The workhorse_api SLI of the webservice service has an error rate violating
        SLO
    expr: |
      (
        (
          gitlab_component_errors:ratio_6h{component="workhorse_api",type="webservice"}
          > (6 * 0.000100)
        )
        and
        (
          gitlab_component_errors:ratio_30m{component="workhorse_api",type="webservice"}
          > (6 * 0.000100)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_6h{component="workhorse_api",type="webservice"}) >= 0.16667
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "yes"
      window: 6h
  - alert: WebserviceServiceWorkhorseApiTrafficCessation
    annotations:
      description: |
        Aggregation of most API requests that pass through workhorse, monitored via the HTTP interface.

        The workhorse API apdex has a longer apdex latency than the web to allow for slow API requests.

        This alert signifies that the SLI is reporting a cessation of traffic; the signal is present, but is zero.
      grafana_dashboard_id: webservice-main/webservice-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/webservice-main/webservice-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3800674929"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(gitlab_workhorse_http_requests_total{route!="^/-/health$",route!="^/-/(readiness|liveness)$",route=~"^\\^/api/.*"}[5m])
        )
      runbook: docs/webservice/README.md
      title: The workhorse_api SLI of the webservice service has not received any
        traffic in the past 30m
    expr: |
      gitlab_component_ops:rate_30m{component="workhorse_api",type="webservice"} == 0
      and
      gitlab_component_ops:rate_30m{component="workhorse_api",type="webservice"} offset 1h >= 0.16666666666666666
    for: 5m
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
  - alert: WebserviceServiceWorkhorseApiTrafficAbsent
    annotations:
      description: |
        Aggregation of most API requests that pass through workhorse, monitored via the HTTP interface.

        The workhorse API apdex has a longer apdex latency than the web to allow for slow API requests.

        This alert signifies that the SLI was previously reporting traffic, but is no longer - the signal is absent.

        This could be caused by a change to the metrics used in the SLI, or by the service not receiving traffic.
      grafana_dashboard_id: webservice-main/webservice-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/webservice-main/webservice-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3800674929"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(gitlab_workhorse_http_requests_total{route!="^/-/health$",route!="^/-/(readiness|liveness)$",route=~"^\\^/api/.*"}[5m])
        )
      runbook: docs/webservice/README.md
      title: The workhorse_api SLI of the webservice service has not reported any
        traffic in the past 30m
    expr: |
      gitlab_component_ops:rate_5m{component="workhorse_api",type="webservice"} offset 1h
      unless
      gitlab_component_ops:rate_5m{component="workhorse_api",type="webservice"}
    for: 30m
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
