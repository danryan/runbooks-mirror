## Minimum activity to be considered "available"

ALERT PostgresSQL_XIDConsumptionTooLow
  IF rate(pg_txid_current{environment="prd"}[1m]) < 5
  FOR 1m
  LABELS {severity="warn"}
  ANNOTATIONS {
    title="Postgres seems to be consuming transaction IDs very slowly",
	description="TXID/s is {{ $value | printf \"%.1f\" }} on {{$labels.instance}} which is unusually low. Perhaps the application is unable to connect",
    runbook="troubleshooting/postgresql.md#availability"
  }
  
ALERT PostgreSQL_XLOGConsumptionTooLow
  IF rate(pg_xlog_position_bytes{environment="prd"}[1m]) < 200000
  FOR 2m
  LABELS {severity="warn"}
  ANNOTATIONS {
    title="Postgres seems to be consuming XLOG very slowly",
	description="XLOG throughput is {{ $value | humanize1024 }}B/s on {{$labels.instance}} which is unusually low. Perhaps the application is unable to connect",
    runbook="troubleshooting/postgresql.md#availability"
  }

ALERT PostgreSQL_CommitRateTooLow
  IF rate(pg_stat_database_xact_commit{environment="prd",datname="gitlabhq_production"}[1m]) < 1000
  FOR 2m
  LABELS {severity="warn"}
  ANNOTATIONS {
    title="Postgres seems to be processing very few transactions",
	description="Commits/s on {{$labels.instance}} database {{$labels.datname}} is {{$value | printf \"%.0f\" }} which is implausibly low. Perhaps the application is unable to connect",
    runbook="troubleshooting/postgresql.md#availability"
  }

## High number of connections

ALERT PostgreSQL_ConnectionsTooHigh
  IF sum by (environment,fqdn) (pg_stat_activity_count) > on (fqdn) pg_settings_max_connections * 0.75
  FOR 10m
  LABELS {severity="warn"}
  ANNOTATIONS {
    title="Postgres has {{$value}} connections on {{$labels.fqdn}} which is close to the maximum",
	runbook="troubleshooting/postgresql.md#connections"
  }

## Transaction error rate

ALERT PostgreSQL_RollbackRateTooHigh
  IF rate(pg_stat_database_xact_rollback{environment="prd",datname="gitlabhq_production"}[1m])
     / on (instance,datname)
	 rate(pg_stat_database_xact_commit{environment="prd",datname="gitlabhq_production"}[1m])
	 > 0.01
  LABELS {severity="warn"}
  ANNOTATIONS {
    title="Postgres transaction rollback rate is high",
	description="Ratio of transactions being aborted compared to committed is {{ $value | printf \"%.2f\" }} on {{$labels.instance}}",
    runbook="troubleshooting/postgresql.md#errors"
  }

## Replication Status

ALERT PostgreSQL_ReplicationStopped
  # By floating point rules, NaN != NaN, so we can't just apply "== NaN" as a filter,
  # so we use "x != x" instead to get all elements that are NaN.
  IF pg_stat_replication_pg_xlog_location_diff{job="gitlab-cluster-db"} != pg_stat_replication_pg_xlog_location_diff{job="gitlab-cluster-db"}
  FOR 5m
  LABELS {severity="critical", channel="production", pager="pagerduty"}
  ANNOTATIONS {
    title="PostgreSQL replication has stopped",
    description="PostgreSQL replication has stopped on {{$labels.instance}}.",
    runbook="troubleshooting/postgres.md#replication-is-lagging-or-has-stopped"
  }

# This measure dependso on the replica to report the lag based on the
# data the primary has fed to it.

ALERT PostgreSQL_ReplicationLagTooLarge
  IF (pg_replication_lag{environment="prd"} > 120) and on (instance) (pg_replication_is_replica == 1.0)
  FOR 5m
  LABELS {severity="critical", pager="pagerduty"}
  ANNOTATIONS {
    title="Postgres Replication lag is over 2 minutes",
    description="Replication lag on server {{$labels.instance}} is currently {{ $value | humanizeDuration }}",
    runbook="troubleshooting/postgres.md#replication-is-lagging-or-has-stopped"
  }

# This measure does not depend on the replica having up-to-date info
# from the primary. It compares the Prometheus metrics for xlog
# position from the primary and the replica.

ALERT PostgreSQL_ReplicationLagBytesTooLarge
  IF (pg_xlog_position_bytes and pg_replication_is_replica == 0.0)
     - on (environment) group_right(instance)
     (pg_xlog_position_bytes and pg_replication_is_replica == 1.0)
     > 200000000
  FOR 5m
  LABELS {severity="critical", pager="pagerduty"}
  ANNOTATIONS {
    title="Postgres Replication lag is over 200MB",
	description="Replication lag on server {{$labels.instance} is currently {{ $value | humanize1024}}B",
	runbook="troubleshooting/postgres.md#replication-is-lagging-or-has-stopped"
  }

ALERT PostgreSQL_XLOGConsumptionTooHigh
  IF rate(pg_xlog_position_bytes{environment="prd"}[1m]) > 12000000 and on (instance) (pg_replication_is_replica == 0.0)
  FOR 5m
  LABELS {severity="critical", pager="pagerduty"}
  ANNOTATIONS {
    title="Postgres is generating XLOG too fast, expect this to cause replication lag",
	description="XLOG is being generated at a rate of {{ $value | humanize1024 }}B/s on {{$labels.instance}}",
	runbook="troubleshooting/postgres.md#xlog-consumption-is-very-high"
  }

# Group these by database host
ALERT PostgreSQL_UnusedReplicationSlot
  IF pg_replication_slots_active == 0.0
  FOR 30m
  LABELS {severity="warn"}
  ANNOTATIONS {
    title="Unused Replication Slots for {{$labels.fqdn}}",
  	description="Unused {{$labels.slot_type}} slot \"{{$labels.slot_name}}\" on {{$labels.fqdn}}",
 	runbook="troubleshooting/postgres.md#replication_slots"
  }

ALERT PostgreSQL_ReplicaStaleXmin
  IF pg_replication_slots_xmin_age > 10000
  FOR 5m
  LABELS {severity="warn", pager="pagerduty"}
  ANNOTATIONS {
    title="PostgreSQL replication slot with an stale xmin which can cause bloat on the primary",
	description="PostgreSQL slot {{$labels.slot_name}} xmin age is {{$value}} which means there's a long-running query or transaction on the database. This can cause bloat on the *primary* due to hot standby feedback.",
    runbook="troubleshooting/postgres.md#tables-with-a-large-amount-of-dead-tuples",
  }

## Miscellaneous Database Alerts

ALERT PostgreSQL_LongLivedTransaction
  IF pg_stat_activity_max_tx_duration > 900
  LABELS {severity="warn"}
  ANNOTATIONS {
    title="There's a Postgres transaction older than 30 minutes",
	description="Postgres server {{$labels.fqdn}} has a transaction in state \"{{$labels.state}}\" that started {{ $value | humanizeDuration }} ago",
    runbook="troubleshooting/postgres.md#tables-with-a-large-amount-of-dead-tuples",
  }

ALERT PostgreSQL_TooManyRowExclusiveLocks
  IF sum(pg_locks_count{mode="rowexclusivelock"}) by (environment) > 100
  FOR 60m
  LABELS {severity="info"}
  ANNOTATIONS {
    title="Too many row exclusive locks: {{ $value }}",
    description="Large numbers of ROW EXCLUSIVE locks (used by SQL UPDATEs for example) can be a symptom of a blocking DDL, long lived transaction holding locks, or system performance degradation. See http://performance.gitlab.net/dashboard/db/postgres-stats for more information.",
    runbook="troubleshooting/postgres.md#locks"
  }

ALERT PostgreSQL_DBHeavyLoad
  IF node_load1{environment="prd",type="postgres"} > 200
  FOR 5m
  LABELS {severity="critical", channel="production"}
  ANNOTATIONS {
    title="High load in database {{ $labels.fqdn }}: {{$value}}",
    description="Really high load in the batabase for the last minute, there are {{ query \"sum(pg_slow_queries_total)\" }} slow queries, {{ query \"sum(pg_blocked_queries_total)\" }} and {{ query \"sum(pg_locks_count{datname='gitlabhq_production'})\" }} locks. Check http://performance.gitlab.net/dashboard/db/postgres-stats and http://performance.gitlab.net/dashboard/db/postgres-queries to get more data.",
    runbook="troubleshooting/postgres.md#load"
  }

## PostgreSQL dead tuples
ALERT PostgreSQL_TooManyDeadTuples
  IF pg_stat_table_n_dead_tup{environment="prd"} > 50000 unless on (instance) (pg_replication_is_replica == 1.0)
  FOR 1h
  LABELS {severity="critical", channel="production"}
  ANNOTATIONS {
    title="PostgreSQL dead tuples is too large",
    description="PostgreSQL table {{$labels.table_name}} has {{$value}} dead tuples on {{$labels.instance}}",
    runbook="troubleshooting/postgres.md#tables-with-a-large-amount-of-dead-tuples"
  }


## Meta Monitoring

ALERT PostgreSQL_FleetSizeChange
  IF postgres:databases != postgres:databases offset 2m
  FOR 1m
  LABELS {severity="info"}
  ANNOTATIONS {
    title="Number of PostgreSQL Databases in {{$labels.environment}} has changed in the past minute",
	description="There are now {{$value}} databases in environment \"{{$labels.environment}}\"",
  }

ALERT PostgreSQL_RoleChange
  IF pg_replication_is_replica and changes(pg_replication_is_replica[1m]) > 0
  LABELS {severity="info"}
  ANNOTATIONS {
    title="Postgres Database replica promotion occurred in environment \"{{$labels.environment}}\"",
	description="Database on {{$labels.fqdn}} changed role to {{if eq $value 1.0}} *replica* {{else}} *primary* {{end}}",
  }

ALERT PostgreSQL_ConfigurationChange
  IF {__name__=~"pg_settings_.*"} != on (__name__,fqdn) {__name__=~"pg_settings_.*"} offset 10m
  LABELS {severity="info"}
  ANNOTATIONS {
    title="Postgres Database configuration change has occured on \"{{$labels.fqdn}}\"",
	description="Database on {{$labels.fqdn}} setting now {{$labels.__name__}}={{$value}}",
  }

# This alert detects replicas that have different configuration values
# than the primary. It only fires after 60m to avoid spamming alert
# channels when changes are rolled out normally (the above rule will
# still send an alert). But after 60m all the servers should be
# updated and if they're still inconsistent then something has gone
# wrong with the config rollout.

# The bizarre regexp here is to work around the restrictions of Go's
# Regexp library's limited RE2 syntax. It's basically implementing
# /pg_settings_(?!transaction_read_only).*/ (because
# transaction_read_only is true on standbys).

ALERT PostgreSQL_ConfigurationInconsistent
  IF (
	  {__name__=~"pg_settings_.*"}
	  and on (instance) pg_replication_is_replica == 1.0
	 )
     != on (__name__) group_left()
     (
	  {__name__=~"pg_settings_([^t]|t[^r]|tr[^a]|tra[^n]|tran[^s]|trans[^a]|transa[^c]|transac[^t]|transact[^i]|transacti[^o]|transactio[^n]|transaction[^_]|transaction_[^r]|transaction_r[^e]|transaction_re[^a]|transaction_rea[^d]|transaction_read[^_]|transaction_read_[^o]|transaction_read_o[^n]|transaction_read_on[^l]|transaction_read_onl[^y]).*"}
      and on (instance) pg_replication_is_replica == 0.0
	 )
  FOR 60m
  LABELS {severity="info"}
  ANNOTATIONS {
    title="Postgres server configurations inconsistent across fleet",
	description="The Postges server {{$labels.fqdn}} has {{$labels.__name__}}={{$value}} which doesn't match the current production primary",
    runbook="troubleshooting/postgres.md",
  }

ALERT PostgreSQL_TooFewPrometheusScrapes
  IF rate(pg_exporter_scrapes_total[1m]) < 1/60
  FOR 5m
  LABELS {severity="warn"}
  ANNOTATIONS {
    title="PostgreSQL Exporter not being scraped",
	description="{{$labels.fqdn}} is showing only {{$value}} scrapes per second which should be > 0.2",
  }

ALERT PostgreSQL_ExporterErrors
  IF pg_exporter_last_scrape_error == 1
  FOR 1h
  LABELS {severity = "warn"}
  ANNOTATIONS {
    title="Postgres exporter is showing errors for the last hour",
	description="This may indicate postgres_exporter is not running or a buggy query in query.yaml on {{$labels.fqdn}}",
	runbook="troubleshooting/prometheus-exporter-scrape-errors.md",
  }

ALERT PostgreSQL_DiskUtilizationMaxedOut
  IF rate(node_disk_io_time_ms{tier="db",type="postgres",device="dm-0",environment="prd"}[1m])/10 > 96
  FOR 2m
  LABELS {severity="warn"}
  ANNOTATIONS {
    title="Postgres server's disk utilization over 95% for over a minute. Expect this to cause replication lag or user visible latency",
	description="Server {{$labels.fqdn}} disk {{$labels.device}} utilization is {{$value | printf \"%.1f\"}}%",
	runbook="troubleshooting/postgres.md#replication-is-lagging-or-has-stopped",
  }
