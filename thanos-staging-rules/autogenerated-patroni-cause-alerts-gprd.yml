---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: autogenerated-patroni-cause-alerts-gprd
  labels:
    ruler: thanos
    shard: '3'
  annotations: {}
spec:
  # WARNING. DO NOT EDIT THIS FILE BY HAND. USE ./thanos-staging-rules-jsonnet/patroni-cause-alerts.jsonnet TO GENERATE IT
  # YOUR CHANGES WILL BE OVERRIDDEN
  groups:
  - name: patroni_cause_alerts
    rules:
    - alert: PostgreSQL_HotSpotTupleFetchingPrimary
      for: 10m
      annotations:
        title: Hot spot tuple fetches on the postgres primary `{{ $labels.fqdn }}` in
          the `{{ $labels.relname }}` table, `{{ $labels.relname }}`.
        description: |
          More than 50% of all tuple fetches on postgres primary `{{ $labels.fqdn }}` are for a single table.

          This may indicate that the query optimizer is using incorrect statistics to execute a query.

          This could be due to vacuum and analyze commands (issued either automatically or manually) against this table or closely related table. As a new step, check which tables were analyzed and vacuumed immediately prior to this incident.

          <https://log.gprd.gitlab.net/app/kibana#/discover?_a=(columns:!(json.hostname,json.endpoint_id,json.error_severity,json.message,json.session_start_time,json.sql_state_code,json.duration_s,json.sql),filters:!((meta:(index:'97f04200-024b-11eb-81e5-155ba78758d4',key:json.sql,params:'{{$labels.relname}}',type:phrase),query:(match:(json.sql:(query:'{{$labels.relname}}',type:phrase)))),(meta:(index:'97f04200-024b-11eb-81e5-155ba78758d4',key:json.fqdn,params:'{{$labels.fqdn}}',type:phrase),query:(match:(json.fqdn:(query:'{{$labels.fqdn}}',type:phrase))))),index:'97f04200-024b-11eb-81e5-155ba78758d4')|postgres slowlog in Kibana>

          Previous incidents of this type include <https://gitlab.com/gitlab-com/gl-infra/production/-/issues/2885> and <https://gitlab.com/gitlab-com/gl-infra/production/-/issues/3875>.
        grafana_dashboard_id: alerts-pg_user_tables_primary/alerts-pg-user-table-alerts-primary
        grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-pg_user_tables_primary/alerts-pg-user-table-alerts-primary?from=now-6h/m&to=now-1m/m&var-environment={{
          $labels.environment }}&var-tier={{ $labels.tier }}&var-type={{ $labels.type
          }}&var-fqdn={{ $labels.fqdn }}&var-relname={{ $labels.relname }}
        grafana_min_zoom_hours: "6"
        grafana_panel_id: "2"
        grafana_variables: environment,tier,type,fqdn,relname
        runbook: docs/patroni/rails-sql-apdex-slow.md
      labels:
        alert_type: cause
        severity: s4
      expr: |
        (
          sum by (environment,tier,type,fqdn,relname) (
            rate(pg_stat_user_tables_idx_tup_fetch{env="gprd",type="patroni"}[5m])
            and on(job, instance)
            pg_replication_is_replica == 0
          )
          / ignoring(relname) group_left()
            sum by (environment,tier,type,fqdn) (
              rate(pg_stat_user_tables_idx_tup_fetch{env="gprd",type="patroni"}[5m])
              and on(job, instance)
              pg_replication_is_replica == 0
          )
        ) > 0.5
    - alert: PostgreSQL_HotSpotTupleFetchingReplicas
      for: 10m
      annotations:
        title: Hot spot tuple fetches on the postgres postgres replicas in the `{{ $labels.relname
          }}` table, `{{ $labels.relname }}`.
        description: |
          More than 50% of all tuple fetches on postgres postgres replicas are for a single table.

          This may indicate that the query optimizer is using incorrect statistics to execute a query.

          This could be due to vacuum and analyze commands (issued either automatically or manually) against this table or closely related table. As a new step, check which tables were analyzed and vacuumed immediately prior to this incident.

          <https://log.gprd.gitlab.net/app/kibana#/discover?_a=(columns:!(json.hostname,json.endpoint_id,json.error_severity,json.message,json.session_start_time,json.sql_state_code,json.duration_s,json.sql),filters:!((meta:(index:'97f04200-024b-11eb-81e5-155ba78758d4',key:json.sql,params:'{{$labels.relname}}',type:phrase),query:(match:(json.sql:(query:'{{$labels.relname}}',type:phrase))))),index:'97f04200-024b-11eb-81e5-155ba78758d4')|postgres slowlog in Kibana>

          Previous incidents of this type include <https://gitlab.com/gitlab-com/gl-infra/production/-/issues/2885> and <https://gitlab.com/gitlab-com/gl-infra/production/-/issues/3875>.
        grafana_dashboard_id: alerts-pg_user_tables_replica/alerts-pg-user-table-alerts-replicas
        grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-pg_user_tables_replica/alerts-pg-user-table-alerts-replicas?from=now-6h/m&to=now-1m/m&var-environment={{
          $labels.environment }}&var-tier={{ $labels.tier }}&var-type={{ $labels.type
          }}&var-relname={{ $labels.relname }}
        grafana_min_zoom_hours: "6"
        grafana_panel_id: "2"
        grafana_variables: environment,tier,type,relname
        runbook: docs/patroni/rails-sql-apdex-slow.md
      labels:
        alert_type: cause
        severity: s4
      expr: |
        (
          sum by (environment,tier,type,relname) (
            rate(pg_stat_user_tables_idx_tup_fetch{env="gprd",type="patroni"}[5m])
            and on(job, instance)
            pg_replication_is_replica == 1
          )
          / ignoring(relname) group_left()
            sum by (environment,tier,type) (
              rate(pg_stat_user_tables_idx_tup_fetch{env="gprd",type="patroni"}[5m])
              and on(job, instance)
              pg_replication_is_replica == 1
          )
        ) > 0.5
    - alert: PostgreSQLAccessGroupTupleFetchesWarningTrigger
      for: 1h
      annotations:
        title: Average fetches on the postgres primary in the project_authorizations
          table exceeds 5% of total
        description: |
          More than 5% of all tuple fetches on the postgres primary are for the `project_authorizations` table.

          This work was previously addressed through the epic https://gitlab.com/groups/gitlab-org/-/epics/3343#note_652970688.

          The Authentication and authorization team should work to understand why this is happening and look to address the problem.
        grafana_dashboard_id: alerts-pg_user_tables_primary/alerts-pg-user-table-alerts-primary
        grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-pg_user_tables_primary/alerts-pg-user-table-alerts-primary?from=now-24h/m&to=now-1m/m&var-environment={{
          $labels.environment }}&var-tier={{ $labels.tier }}&var-type={{ $labels.type
          }}&var-fqdn={{ $labels.fqdn }}&var-relname={{ $labels.relname }}
        grafana_min_zoom_hours: "24"
        grafana_panel_id: "2"
        grafana_variables: environment,tier,type,fqdn,relname
        runbook: docs/patroni/rails-sql-apdex-slow.md
      labels:
        alert_type: cause
        severity: s4
        team: authentication
      expr: |
        (
          sum by (environment,tier,type,fqdn,relname) (
            rate(pg_stat_user_tables_idx_tup_fetch{env="gprd",relname="project_authorizations",type="patroni"}[1d])
            and on(job, instance)
            pg_replication_is_replica == 0
          )
          / ignoring(relname) group_left()
            sum by (environment,tier,type,fqdn) (
              rate(pg_stat_user_tables_idx_tup_fetch{env="gprd",type="patroni"}[1d])
              and on(job, instance)
              pg_replication_is_replica == 0
          )
        ) > 0.05
    - alert: PatroniSubtransControlLocksDetected
      for: 5m
      annotations:
        title: Subtransactions wait events have been detected in the database in the
          last 5 minutes
        description: |
          Wait events related to subtransactions locking have been detected in the database in the last 5 minutes.

          This can eventually saturate entire database cluster if this situation continues for a longer period of time.
        runbook: docs/patroni/postgresql-subtransactions.md
      labels:
        alert_type: cause
        severity: s3
        team: subtransaction_troubleshooting
      expr: |
        sum by (environment) (
          sum_over_time(pg_stat_activity_marginalia_sampler_active_count{env="gprd",wait_event=~"[Ss]ubtrans.*"}[10m])
        ) > 10
    - alert: PatroniLongRunningTransactionDetected
      for: 1m
      annotations:
        title: Transactions detected that have been running on `{{ $labels.fqdn }}`
          for more than 10m
        description: |
          Endpoint `{{ $labels.endpoint }}` on `{{ $labels.application }}` is executing a transaction that has been running for more than 10m. This could lead to dead-tuples and performance degradation in our Patroni fleet.

          Ideally, no transaction should remain open for more than a few seconds.

          <https://log.gprd.gitlab.net/app/kibana#/visualize/create?type=line&indexPattern=97f04200-024b-11eb-81e5-155ba78758d4&_a=(filters:!(),query:(language:kuery,query:''),vis:(aggs:!((enabled:!t,id:'1',params:(),schema:metric,type:count),(enabled:!t,id:'2',params:(drop_partials:!t,extended_bounds:(),field:'@timestamp',interval:auto,min_doc_count:1,scaleMetricValues:!f,timeRange:(from:'${__from:date:iso}',to:'${__to:date:iso}'),useNormalizedEsInterval:!t),schema:segment,type:date_histogram),(enabled:!t,id:'3',params:(field:json.fingerprint.keyword,missingBucket:!f,missingBucketLabel:Missing,order:desc,orderBy:'1',otherBucket:!t,otherBucketLabel:Other,size:5),schema:group,type:terms))))|Check the slowlog for changes in the usual trends>.
        grafana_dashboard_id: alerts-long_running_transactions/alerts-long-running-transactions
        grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-long_running_transactions/alerts-long-running-transactions?from=now-6h/m&to=now-1m/m&var-environment={{
          $labels.environment }}
        grafana_min_zoom_hours: "6"
        grafana_variables: environment
        runbook: docs/patroni/postgres-long-running-transaction.md
      labels:
        alert_type: cause
        pager: pagerduty
        severity: s2
      expr: |
        topk by (environment, type, stage, shard) (1,
          max by (environment, type, stage, shard, application, endpoint, fqdn) (
            pg_stat_activity_marginalia_sampler_max_tx_age_in_seconds{command!="vacuum",command!="autovacuum",command!~"[cC][rR][eE][aA][tT][eE]",command!~"[aA][nN][aA][lL][yY][zZ][eE]",command!~"[aA][lL][tT][eE][rR]",command!~"[dD][rR][oO][pP]",env="gprd",type="patroni"}
          )
          > 540
        )
