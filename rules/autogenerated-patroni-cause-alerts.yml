# WARNING. DO NOT EDIT THIS FILE BY HAND. USE ./rules-jsonnet/patroni-cause-alerts.jsonnet TO GENERATE IT
# YOUR CHANGES WILL BE OVERRIDDENgroups:
- name: patroni_cause_alerts
  rules:
  - alert: PostgreSQL_HotSpotTupleFetchingPrimary
    for: 10m
    annotations:
      title: Hot spot tuple fetches on the postgres primary `{{ $labels.fqdn }}` in
        the `{{ $labels.relname }}` table, `{{ $labels.relname }}`.
      description: |
        More than 50% of all tuple fetches on postgres primary `{{ $labels.fqdn }}` are for a single table.

        This may indicate that the query optimizer is using incorrect statistics to execute a query.

        This could be due to vacuum and analyze commands (issued either automatically or manually) against this table or closely related table. As a new step, check which tables were analyzed and vacuumed immediately prior to this incident.

        <https://log.gprd.gitlab.net/app/kibana#/discover?_a=(columns:!(json.hostname,json.endpoint_id,json.error_severity,json.message,json.session_start_time,json.sql_state_code,json.duration_s,json.sql),filters:!((query:(match:(json.sql:(query:'{{$labels.relname}}',type:phrase)))),(query:(match:(json.fqdn:(query:'{{$labels.fqdn}}',type:phrase))))),index:'97f04200-024b-11eb-81e5-155ba78758d4')|postgres slowlog in Kibana>

        Previous incidents of this type include <https://gitlab.com/gitlab-com/gl-infra/production/-/issues/2885> and <https://gitlab.com/gitlab-com/gl-infra/production/-/issues/3875>.
      grafana_dashboard_id: alerts-pg_user_tables_primary/alerts-pg-user-table-alerts-primary
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-pg_user_tables_primary/alerts-pg-user-table-alerts-primary?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-tier={{ $labels.tier }}&var-type={{ $labels.type
        }}&var-fqdn={{ $labels.fqdn }}&var-relname={{ $labels.relname }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2"
      grafana_variables: environment,tier,type,fqdn,relname
    labels:
      alert_type: cause
      runbook: docs/patroni/rails-sql-apdex-slow.md
      severity: s4
      team: rapid-action-intercom
    expr: |
      (
        sum by (environment,tier,type,fqdn,relname) (
          rate(pg_stat_user_tables_idx_tup_fetch{type="patroni"}[5m])
          and on(job, instance)
          pg_replication_is_replica == 0
        )
        / ignoring(relname) group_left()
          sum by (environment,tier,type,fqdn) (
            rate(pg_stat_user_tables_idx_tup_fetch{type="patroni"}[5m])
            and on(job, instance)
            pg_replication_is_replica == 0
        )
      ) > 0.5
  - alert: PostgreSQL_HotSpotTupleFetchingReplicas
    for: 10m
    annotations:
      title: Hot spot tuple fetches on the postgres postgres replicas in the `{{ $labels.relname
        }}` table, `{{ $labels.relname }}`.
      description: |
        More than 50% of all tuple fetches on postgres postgres replicas are for a single table.

        This may indicate that the query optimizer is using incorrect statistics to execute a query.

        This could be due to vacuum and analyze commands (issued either automatically or manually) against this table or closely related table. As a new step, check which tables were analyzed and vacuumed immediately prior to this incident.

        <https://log.gprd.gitlab.net/app/kibana#/discover?_a=(columns:!(json.hostname,json.endpoint_id,json.error_severity,json.message,json.session_start_time,json.sql_state_code,json.duration_s,json.sql),filters:!((query:(match:(json.sql:(query:'{{$labels.relname}}',type:phrase))))),index:'97f04200-024b-11eb-81e5-155ba78758d4')|postgres slowlog in Kibana>

        Previous incidents of this type include <https://gitlab.com/gitlab-com/gl-infra/production/-/issues/2885> and <https://gitlab.com/gitlab-com/gl-infra/production/-/issues/3875>.
      grafana_dashboard_id: alerts-pg_user_tables_replica/alerts-pg-user-table-alerts-replicas
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-pg_user_tables_replica/alerts-pg-user-table-alerts-replicas?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-tier={{ $labels.tier }}&var-type={{ $labels.type
        }}&var-relname={{ $labels.relname }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2"
      grafana_variables: environment,tier,type,relname
    labels:
      alert_type: cause
      runbook: docs/patroni/rails-sql-apdex-slow.md
      severity: s4
      team: rapid-action-intercom
    expr: |
      (
        sum by (environment,tier,type,relname) (
          rate(pg_stat_user_tables_idx_tup_fetch{type="patroni"}[5m])
          and on(job, instance)
          pg_replication_is_replica == 1
        )
        / ignoring(relname) group_left()
          sum by (environment,tier,type) (
            rate(pg_stat_user_tables_idx_tup_fetch{type="patroni"}[5m])
            and on(job, instance)
            pg_replication_is_replica == 1
        )
      ) > 0.5
  - alert: PostgreSQLAccessGroupTupleFetchesWarningTrigger
    for: 1h
    annotations:
      title: Average fetches on the postgres primary in the project_authorizations
        table exceeds 5% of total
      description: |
        More than 5% of all tuple fetches on the postgres primary are for the `project_authorizations` table.

        This work was previously addressed through the epic https://gitlab.com/groups/gitlab-org/-/epics/3343#note_652970688.

        The Access team should work to understand why this is happening and look to address the problem.
      grafana_dashboard_id: alerts-pg_user_tables_primary/alerts-pg-user-table-alerts-primary
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-pg_user_tables_primary/alerts-pg-user-table-alerts-primary?from=now-24h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-tier={{ $labels.tier }}&var-type={{ $labels.type
        }}&var-fqdn={{ $labels.fqdn }}&var-relname={{ $labels.relname }}
      grafana_min_zoom_hours: "24"
      grafana_panel_id: "2"
      grafana_variables: environment,tier,type,fqdn,relname
    labels:
      alert_type: cause
      runbook: docs/patroni/rails-sql-apdex-slow.md
      severity: s4
      team: access
    expr: |
      (
        sum by (environment,tier,type,fqdn,relname) (
          rate(pg_stat_user_tables_idx_tup_fetch{relname="project_authorizations",type="patroni"}[1d])
          and on(job, instance)
          pg_replication_is_replica == 0
        )
        / ignoring(relname) group_left()
          sum by (environment,tier,type,fqdn) (
            rate(pg_stat_user_tables_idx_tup_fetch{type="patroni"}[1d])
            and on(job, instance)
            pg_replication_is_replica == 0
        )
      ) > 0.05
  - alert: PatroniLongRunningTransactionDetected
    for: 1m
    annotations:
      title: Transactions detected that have been running on `{{ $labels.fqdn }}`
        for more than 10m
      description: |
        Endpoint `{{ $labels.endpoint }}` on `{{ $labels.application }}` is executing a transaction that has been running for more than 10m. This could lead to dead-tuples and performance degradation in our Patroni fleet.

        Ideally, no transaction should remain open for more than a few seconds.

        <https://log.gprd.gitlab.net/app/kibana#/visualize/create?type=line&indexPattern=97f04200-024b-11eb-81e5-155ba78758d4&_a=(filters:!(),query:(language:kuery,query:''),vis:(aggs:!((enabled:!t,id:'1',params:(),schema:metric,type:count),(enabled:!t,id:'2',params:(drop_partials:!t,extended_bounds:(),field:'@timestamp',interval:auto,min_doc_count:1,scaleMetricValues:!f,timeRange:(from:'${__from:date:iso}',to:'${__to:date:iso}'),useNormalizedEsInterval:!t),schema:segment,type:date_histogram),(enabled:!t,id:'3',params:(field:json.fingerprint.keyword,missingBucket:!f,missingBucketLabel:Missing,order:desc,orderBy:'1',otherBucket:!t,otherBucketLabel:Other,size:5),schema:group,type:terms))))|Check the slowlog for changes in the usual trends>.
      grafana_dashboard_id: alerts-long_running_transactions/alerts-long-running-transactions
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-long_running_transactions/alerts-long-running-transactions?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}
      grafana_min_zoom_hours: "6"
      grafana_variables: environment
    labels:
      alert_type: cause
      pager: pagerduty
      runbook: docs/patroni/postgres.md#tables-with-a-large-amount-of-dead-tuples
      severity: s2
      team: sre_datastores
    expr: |
      topk by (environment, type, stage, shard) (1,
        max by (environment, type, stage, shard, application, endpoint, fqdn) (
          pg_stat_activity_marginalia_sampler_max_tx_age_in_seconds{
            type="patroni",
            command!="vacuum",
            command!="autovacuum",
            command!~"[cC][rR][eE][aA][tT][eE]",
            command!~"[aA][nN][aA][lL][yY][zZ][eE]",
            command!~"[rR][eE][iI][nN][dD][eE][xX]",
            command!~"[aA][lL][tT][eE][rR]",
          }
        )
        > 540
      )
