# WARNING. DO NOT EDIT THIS FILE BY HAND. USE ./rules-jsonnet/patroni-cause-alerts.jsonnet TO GENERATE IT
# YOUR CHANGES WILL BE OVERRIDDEN
groups:
- name: patroni_cause_alerts
  rules:
  - alert: PostgreSQL_HotSpotTupleFetchingPrimary
    for: 10m
    annotations:
      title: >-
        Hot spot tuple fetches on the postgres primary `{{ $labels.fqdn }}` in the `{{ $labels.relname
        }}` table, `{{ $labels.relname }}`.
      description: |
        More than 50% of all tuple fetches on postgres primary `{{ $labels.fqdn }}` are for a single table.

        This may indicate that the query optimizer is using incorrect statistics to execute a query.

        This could be due to vacuum and analyze commands (issued either automatically or manually) against this table or closely related table. As a new step, check which tables were analyzed and vacuumed immediately prior to this incident.

        Check the [postgres slowlog in Kibana](https://log.gprd.gitlab.net/app/kibana#/discover?_a=(columns:!(json.hostname,json.endpoint_id,json.error_severity,json.message,json.session_start_time,json.sql_state_code,json.duration_s,json.sql),filters:!((query:(match:(json.sql:(query:'{{$labels.relname}}',type:phrase)))),(query:(match:(json.fqdn:(query:'{{$labels.fqdn}}',type:phrase))))),index:'97f04200-024b-11eb-81e5-155ba78758d4',query:(language:kuery,query:''))&_=_).

        Previous incidents of this type include https://gitlab.com/gitlab-com/gl-infra/production/-/issues/2885 and https://gitlab.com/gitlab-com/gl-infra/production/-/issues/3875.
      grafana_dashboard_id: alerts-pg_user_tables_primary/alerts-pg-user-table-alerts-primary
      grafana_dashboard_link: >-
        https://dashboards.gitlab.net/d/alerts-pg_user_tables_primary/alerts-pg-user-table-alerts-primary?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-tier={{ $labels.tier }}&var-type={{ $labels.type }}&var-fqdn={{ $labels.fqdn
        }}
      grafana_min_zoom_hours: '6'
      grafana_panel_id: '2'
      grafana_variables: environment,tier,type,fqdn
    labels:
      alert_type: cause
      pager: pagerduty
      severity: s1
      team: rapid-action-intercom
    expr: |
      (
        sum by (environment,tier,type,fqdn,relname) (
          rate(pg_stat_user_tables_idx_tup_fetch{type="patroni"}[5m])
          and on(job, instance)
          pg_replication_is_replica == 0
        )
        / ignoring(relname) group_left()
          sum by (environment,tier,type,fqdn) (
            rate(pg_stat_user_tables_idx_tup_fetch{type="patroni"}[5m])
            and on(job, instance)
            pg_replication_is_replica == 0
        )
      ) > 0.5
  - alert: PostgreSQL_HotSpotTupleFetchingReplicas
    for: 10m
    annotations:
      title: >-
        Hot spot tuple fetches on the postgres replicas in the `{{ $labels.relname }}` table, `{{ $labels.relname
        }}`.
      description: |
        More than 50% of all tuple fetches on postgres replicas are for a single table.

        This may indicate that the query optimizer is using incorrect statistics to execute a query.

        This could be due to vacuum and analyze commands (issued either automatically or manually) against this table or closely related table. As a new step, check which tables were analyzed and vacuumed immediately prior to this incident.

        Check the [postgres slowlog in Kibana](https://log.gprd.gitlab.net/app/kibana#/discover?_a=(columns:!(json.hostname,json.endpoint_id,json.error_severity,json.message,json.session_start_time,json.sql_state_code,json.duration_s,json.sql),filters:!((query:(match:(json.sql:(query:'{{$labels.relname}}',type:phrase)))),!n),index:'97f04200-024b-11eb-81e5-155ba78758d4',query:(language:kuery,query:''))&_=_).

        Previous incidents of this type include https://gitlab.com/gitlab-com/gl-infra/production/-/issues/2885 and https://gitlab.com/gitlab-com/gl-infra/production/-/issues/3875.
      grafana_dashboard_id: alerts-pg_user_tables_replica/alerts-pg-user-table-alerts-replicas
      grafana_dashboard_link: >-
        https://dashboards.gitlab.net/d/alerts-pg_user_tables_replica/alerts-pg-user-table-alerts-replicas?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-tier={{ $labels.tier }}&var-type={{ $labels.type }}
      grafana_min_zoom_hours: '6'
      grafana_panel_id: '2'
      grafana_variables: environment,tier,type
    labels:
      alert_type: cause
      pager: pagerduty
      severity: s1
      team: rapid-action-intercom
    expr: |
      (
        sum by (environment,tier,type,relname) (
          rate(pg_stat_user_tables_idx_tup_fetch{type="patroni"}[5m])
          and on(job, instance)
          pg_replication_is_replica == 1
        )
        / ignoring(relname) group_left()
          sum by (environment,tier,type) (
            rate(pg_stat_user_tables_idx_tup_fetch{type="patroni"}[5m])
            and on(job, instance)
            pg_replication_is_replica == 1
        )
      ) > 0.5
  - alert: PostgreSQL_HotSpotTupleFetchingPrimaryWarning
    for: 3m
    annotations:
      title: >-
        Hot spot tuple fetches on the postgres replicas in the `{{ $labels.relname }}` table, `{{ $labels.relname
        }}`.
      description: |
        More than 50% of all tuple fetches on postgres replicas are for a single table.

        This may indicate that the query optimizer is using incorrect statistics to execute a query.

        This could be due to vacuum and analyze commands (issued either automatically or manually) against this table or closely related table. As a new step, check which tables were analyzed and vacuumed immediately prior to this incident.

        Check the [postgres slowlog in Kibana](https://log.gprd.gitlab.net/app/kibana#/discover?_a=(columns:!(json.hostname,json.endpoint_id,json.error_severity,json.message,json.session_start_time,json.sql_state_code,json.duration_s,json.sql),filters:!((query:(match:(json.sql:(query:'{{$labels.relname}}',type:phrase)))),!n),index:'97f04200-024b-11eb-81e5-155ba78758d4',query:(language:kuery,query:''))&_=_).

        Previous incidents of this type include https://gitlab.com/gitlab-com/gl-infra/production/-/issues/2885 and https://gitlab.com/gitlab-com/gl-infra/production/-/issues/3875.
      grafana_dashboard_id: alerts-pg_user_tables_replica/alerts-pg-user-table-alerts-replicas
      grafana_dashboard_link: >-
        https://dashboards.gitlab.net/d/alerts-pg_user_tables_replica/alerts-pg-user-table-alerts-replicas?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-tier={{ $labels.tier }}&var-type={{ $labels.type }}
      grafana_min_zoom_hours: '6'
      grafana_panel_id: '2'
      grafana_variables: environment,tier,type
    labels:
      alert_type: cause
      severity: s4
      team: rapid-action-intercom
    expr: |
      (
        sum by (environment,tier,type,relname) (
          rate(pg_stat_user_tables_idx_tup_fetch{type="patroni"}[5m])
          and on(job, instance)
          pg_replication_is_replica == 1
        )
        / ignoring(relname) group_left()
          sum by (environment,tier,type) (
            rate(pg_stat_user_tables_idx_tup_fetch{type="patroni"}[5m])
            and on(job, instance)
            pg_replication_is_replica == 1
        )
      ) > 0.5
  - alert: PostgreSQL_HotSpotTupleFetchingReplicasWarning
    for: 5m
    annotations:
      title: >-
        Hot spot tuple fetches on the postgres replicas in the `{{ $labels.relname }}` table, `{{ $labels.relname
        }}`.
      description: |
        More than 50% of all tuple fetches on postgres replicas are for a single table.

        This may indicate that the query optimizer is using incorrect statistics to execute a query.

        This could be due to vacuum and analyze commands (issued either automatically or manually) against this table or closely related table. As a new step, check which tables were analyzed and vacuumed immediately prior to this incident.

        Check the [postgres slowlog in Kibana](https://log.gprd.gitlab.net/app/kibana#/discover?_a=(columns:!(json.hostname,json.endpoint_id,json.error_severity,json.message,json.session_start_time,json.sql_state_code,json.duration_s,json.sql),filters:!((query:(match:(json.sql:(query:'{{$labels.relname}}',type:phrase)))),!n),index:'97f04200-024b-11eb-81e5-155ba78758d4',query:(language:kuery,query:''))&_=_).

        Previous incidents of this type include https://gitlab.com/gitlab-com/gl-infra/production/-/issues/2885 and https://gitlab.com/gitlab-com/gl-infra/production/-/issues/3875.
      grafana_dashboard_id: alerts-pg_user_tables_replica/alerts-pg-user-table-alerts-replicas
      grafana_dashboard_link: >-
        https://dashboards.gitlab.net/d/alerts-pg_user_tables_replica/alerts-pg-user-table-alerts-replicas?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-tier={{ $labels.tier }}&var-type={{ $labels.type }}
      grafana_min_zoom_hours: '6'
      grafana_panel_id: '2'
      grafana_variables: environment,tier,type
    labels:
      alert_type: cause
      severity: s4
      team: rapid-action-intercom
    expr: |
      (
        sum by (environment,tier,type,relname) (
          rate(pg_stat_user_tables_idx_tup_fetch{type="patroni"}[5m])
          and on(job, instance)
          pg_replication_is_replica == 1
        )
        / ignoring(relname) group_left()
          sum by (environment,tier,type) (
            rate(pg_stat_user_tables_idx_tup_fetch{type="patroni"}[5m])
            and on(job, instance)
            pg_replication_is_replica == 1
        )
      ) > 0.5
