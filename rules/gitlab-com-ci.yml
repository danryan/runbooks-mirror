groups:
- name: gitlab-com-ci.rules
  rules:
  - alert: CICDTooManyPendingBuildsOnSharedRunnerProject
    expr: (ci_pending_builds{has_minutes="yes",shared_runners="yes"} > 500) and (topk(1,
      predict_linear(ci_pending_builds{has_minutes="yes",shared_runners="yes"}[15m], 3600)) > 1000)
    for: 5m
    labels:
      channel: ci-cd
      severity: warn
    annotations:
      title: 'The number of pending builds for projects with shared runners will be
        too high in 1h: {{$value | printf "%.2f" }}'
      description: "Hey <!subteam^S940BK2TV|cicdops>! The number of pending builds for projects with shared runners is
        increasing and will be too high in 1h ({{$value}}). This may suggest problems
        with auto-scaling provider or Runner stability. You should check Runner's
        logs. Check http://dashboards.gitlab.net/dashboard/db/ci."

  - alert: CICDTooManyPendingJobsPerNamespace
    expr: max(ci_pending_builds{has_minutes="yes",namespace!="",namespace!="9970",shared_runners="yes"}) by (namespace) > 350
    for: 1m
    labels:
      channel: ci-cd
      severity: warn
    annotations:
      title: 'Number of pending jobs per namespace too high: {{$value}}'
      description: 'Hey <!subteam^S940BK2TV|cicdops>! Number of pending jobs for namespace {{$labels.namespace}} is too high: {{$value}}.
        Check https://dashboards.gitlab.net/dashboard/db/ci?panelId=33&fullscreen'
      runbook: troubleshooting/ci_pending_builds.md#2-verify-graphs-and-potential-outcomes-out-of-the-graphs-as-described-in-ci-graphsci_graphsmd

  - alert: CICDTooManyRunningJobsPerNamespaceOnSharedRunners
    expr: max(sum(ci_running_builds{has_minutes="yes",namespace!="",namespace!="9970",shared_runner="yes",runner=~"40786|40788|44028|44949|380986|380987"}) by (fqdn,namespace)) by (namespace) > 350
    for: 5m
    labels:
      channel: ci-cd
      severity: warn
    annotations:
      title: 'Number of running jobs per namespace too high: {{$value}}'
      description: 'Hey <!subteam^S940BK2TV|cicdops>! Number of running jobs for namespace {{$labels.namespace}} running on regular Shared Runners is too high: {{$value}}.
        Check https://dashboards.gitlab.net/dashboard/db/ci?panelId=60&fullscreen'
      runbook: troubleshooting/ci_pending_builds.md#2-verify-graphs-and-potential-outcomes-out-of-the-graphs-as-described-in-ci-graphsci_graphsmd

  - alert: CICDTooManyRunningJobsPerNamespaceOnSharedRunnersGitLabOrg
    expr: max(sum(ci_running_builds{has_minutes="yes",namespace!="",namespace!="9970",shared_runner="yes",runner=~"37397|37398|157328|157329|380989|380990"}) by (fqdn,namespace)) by (namespace) > 350
    for: 10m
    labels:
      channel: ci-cd
      severity: warn
    annotations:
      title: 'Number of running jobs per namespace too high: {{$value}}'
      description: 'Hey <!subteam^S940BK2TV|cicdops>! Number of running jobs for namespace {{$labels.namespace}} running on gitlab-org Shared Runners is too high: {{$value}}.
        Check https://dashboards.gitlab.net/dashboard/db/ci?panelId=60&fullscreen'
      runbook: troubleshooting/ci_pending_builds.md#2-verify-graphs-and-potential-outcomes-out-of-the-graphs-as-described-in-ci-graphsci_graphsmd

  - alert: CICDNoJobsOnSharedRunners
    expr: sum(gitlab_runner_jobs{job="shared-runners"}) == 0
    for: 5m
    labels:
      channel: ci-cd
      severity: warn
    annotations:
      title: 'Number of builds running on shared runners is too low: {{$value}}'
      description: "Hey <!subteam^S940BK2TV|cicdops>! Number of builds running on shared runners for the last 5 minutes
        is 0. This may suggest problems with auto-scaling provider or Runner stability.
        You should check Runner's logs. Check http://dashboards.gitlab.net/dashboard/db/ci."

  - alert: CICDRunnersConcurrentLimitHigh
    expr: (sum(gitlab_runner_jobs) by (job) / sum(gitlab_runner_concurrent) by (job)) > 0.85
    for: 5m
    labels:
      channel: ci-cd
      severity: warn
    annotations:
      title: "{{ $labels.job }} runners are using 85% of concurrent limit for more than 5 minutes."
      description: 'Hey <!subteam^S940BK2TV|cicdops>! This may suggest problems with our autoscaled machines fleet OR
        abusive usage of Runners. Check https://dashboards.gitlab.net/dashboard/db/ci'

  - alert: CICDRunnersConcurrentLimitCritical
    expr: (sum(gitlab_runner_jobs) by (job) / sum(gitlab_runner_concurrent) by (job)) > 0.95
    for: 5m
    labels:
      channel: ci-cd
      severity: error
    annotations:
      title: "{{ $labels.job }} runners are using 95% of concurrent limit for more than 5 minutes."
      description: 'Hey <!subteam^S940BK2TV|cicdops>! This may suggest problems with our autoscaled machines fleet OR
        abusive usage of Runners. Check https://dashboards.gitlab.net/dashboard/db/ci'

  - alert: CICDRunnersManagerDown
    expr: up{job=~"private-runners|shared-runners|shared-runners-gitlab-org|staging-shared-runners"} == 0
    for: 5m
    labels:
      channel: ci-cd
      severity: error
    annotations:
      title: Runners manager is down on {{ $labels.instance }}
      description: 'Hey <!subteam^S940BK2TV|cicdops>! This impacts CI execution builds, consider tweeting: !tweet ''Builds
        are being delayed due to our shared runners manager being non responsive.
        We are restarting it to restore the service and then investigating the root
        cause''. Hosts impacted: {{ $labels.instance }}'
      runbook: troubleshooting/runners_manager_is_down.md

  - alert: CICDRunnerMachineCreationRateHigh
    expr: sum(gitlab_runner_autoscaling_machine_states{executor="docker+machine", state="creating"}) / (sum(gitlab_runner_autoscaling_machine_states{executor="docker+machine", state="idle"}) + 1) > 100
    for: 1m
    labels:
      channel: ci-cd
      severity: warn
    annotations:
      title: 'Machine creation rate for runners is too high: {{$value | printf "%.2f" }}'
      description: 'Hey <!subteam^S940BK2TV|cicdops>! Machine creation rate for the last 1 minute is at least {{$value}}
        times greater than machines idle rate. This may by a symptom of problems with
        the auto-scaling provider. Check http://dashboards.gitlab.net/dashboard/db/ci.'
      runbook: troubleshooting/ci_graphs.md#runners-manager-auto-scaling

  - alert: CICDRunnersCacheDown
    expr: (probe_success{instance=~"runners-cache.*:(5000/v2|9000/minio/login)"} == 0)
    for: 5m
    labels:
      channel: ci-cd
      severity: error
    annotations:
      title: Runners cache service {{ $labels.instance }} on {{ $labels.fqdn }} has been down for more than 5 minutes.
      description: 'Hey <!subteam^S940BK2TV|cicdops>! This impacts CI execution builds, consider tweeting: !tweet ''CI
        executions are being delayed due to our runners cache being down at GitLab.com,
        we are investigating the root cause'''
      runbook: troubleshooting/runners_cache_is_down.md

  - alert: CICDRunnersCacheNginxDown
    expr: up{job="runners-cache-server"} == 0
    for: 5m
    labels:
      channel: ci-cd
      severity: error
    annotations:
      title: Runners cache nginx service on {{ $labels.fqdn }} has been down for more than 5 minutes.
      description: 'Hey <!subteam^S940BK2TV|cicdops>! This impacts CI execution builds, consider tweeting: !tweet ''CI
        executions are being delayed due to our runners cache being down at GitLab.com,
        we are investigating the root cause'''
      runbook: troubleshooting/runners_cache_is_down.md

  - alert: CICDRunnersCacheServerHasTooManyConnections
    expr: node_netstat_Tcp_CurrEstab{fqdn=~"runners-cache-.*.gitlab.com"} > 500
    for: 20m
    labels:
      channel: ci-cd
      severity: error
    annotations:
      title: Number of established connections for {{ $labels.instance }} is too high
      description: 'Hey <!subteam^S940BK2TV|cicdops>! This impacts CI execution builds, consider tweeting: !tweet ''CI
        jobs are being delayed due to issues with Shared Runners cache server. We
        are restarting it to restore the service.''. Hosts impacted - {{ $labels.instance }}'
      runbook: troubleshooting/ci_too_many_connections_on_runners_cache_server.md

  - alert: CICDTooManyUsedFDs
    expr: process_open_fds{job=~"(omnibus-runners|private-runners|shared-runners|shared-runners-(gitlab-org|high-cpu))"}
      / process_max_fds{job=~"(omnibus-runners|private-runners|shared-runners|shared-runners-(gitlab-org|high-cpu))"} > 0.8
    for: 10m
    labels:
      channel: ci-cd
      severity: warn
    annotations:
      title: Number of used file descriptors on {{ $labels.instance }} is too high
      description: 'Hey <!subteam^S940BK2TV|cicdops>! {{ $labels.instance }} is using more than 80% of available FDs
        since 10 minutes. This may affect Runner''s stability. Please look at https://dashboards.gitlab.net/dashboard/db/ci
        for more data.'

  - alert: CICDDegradatedCIConsulPrometheusCluster
    expr: up{job="ci-node",fqdn=~"(prometheus|consul)-.*(nyc1|us-east1-(c|d)).*"} == 0
    for: 5m
    labels:
      channel: ci-cd
      severity: warn
    annotations:
      title: CI Consul+Prometheus cluster is degradated, {{ $labels.instance }} is down
      description: 'Hey <!subteam^S940BK2TV|cicdops>! One or more of hosts from CI Consul+Prometheus cluster is down: {{ $labels.instance }}.'

  - alert: CICDGCPQuotaHighUsage
    expr: |
      max(
        gcp_exporter_region_quota_usage{project="gitlab-ci-155816"} / gcp_exporter_region_quota_limit{project="gitlab-ci-155816"} * 100
      ) by (quota, region) > 85
    for: 5m
    labels:
      channel: ci-cd
      severity: warn
    annotations:
      title: GCP Quota usage of {{ $labels.quota }} is too high
      description: |
        Hey <!subteam^S940BK2TV|cicdops>! Quota usage of {{ $labels.quota }} is at the level of {{ $value }} for more than 5 minutes.
        Quota limit breach is coming!
        See https://dashboards.gitlab.net/dashboard/db/ci-autoscaling-providers

  - alert: CICDGCPQuotaCriticalUsage
    expr: |
      max(
        gcp_exporter_region_quota_usage{project="gitlab-ci-155816"} / gcp_exporter_region_quota_limit{project="gitlab-ci-155816"} * 100
      ) by (quota, region) > 95
    for: 5m
    labels:
      channel: ci-cd
      severity: error
    annotations:
      title: GCP Quota usage of {{ $labels.quota }} is near limit
      description: |
        Hey <!subteam^S940BK2TV|cicdops>! Quota usage of {{ $labels.quota }} is at the level of {{ $value }} for more than 5 minutes.
        It's less than 5% to reach quota limits!
        See https://dashboards.gitlab.net/dashboard/db/ci-autoscaling-providers

  - alert: CICDNamespaceWithConstantNumberOfLongRunningRepeatedJobs
    expr: |
      max(sum(ci_repeated_commands_builds{shared_runners="yes",has_minutes="yes",status="running"}) by (fqdn,  namespace)) by (namespace) > 5
      and
      sum(changes(ci_repeated_commands_builds{shared_runners="yes",has_minutes="yes",status="running"}[45m])) by (namespace) == 0
    for: 45m
    labels:
      channel: ci-cd
      severity: warn
    annotations:
      title: Namespace with constant number of long running jobs with repeated commands
      description: |
        Hey <!subteam^S940BK2TV|cicdops>! Namespace {{ $labels.namespace }} is running a constant number of jobs with
        repeated commands for more than 45 minutes.

  - alert: CICDJobQueueDurationUnderperformant
    expr: |
      (
        sum(increase(job_queue_duration_seconds_bucket{shared_runner="true", jobs_running_for_project="0", le="10"}[5m]))
        /
        sum(increase(job_queue_duration_seconds_count{shared_runner="true", jobs_running_for_project="0"}[5m]))
      ) < 0.8
    for: 5m
    labels:
      channel: ci-cd
      severity: error
    annotations:
      title: Job queue duration performance for '0 - 10s' is to low
      description: |
        Hey <!subteam^S940BK2TV|cicdops>! The job queue duration performance for 10s bucket for projects with 0 running
        jobs is lower than 80% for more than 5 minutes.

        This should be considered as service degradation and the reason should be investigated.

        This would exceed our future SLA on GitLab.com!

  - alert: CICDWorkhorseQueuingUnderperformant
    expr: |
      histogram_quantile(
        0.90,
        sum(rate(gitlab_workhorse_queueing_waiting_time_bucket{queue_name="ci_api_job_requests",job="gitlab-workhorse-api"}[5m])) by (le)
      ) >= 30
    for: 5m
    labels:
      channel: ci-cd
      severity: error
    annotations:
      title: 90% of request queued on Workhorse is longer than 30s
      description: |
        Hey <!subteam^S940BK2TV|cicdops>! 90% of requests queued on Workhorse are longer than 30s for last 5 minutes.

        This should be considered as service degradation and the reason should be investigated.

  - alert: CICDTooManyArchivingTraceFailures
    expr: (sum (rate(job_trace_archive_failed_total[5m])) by (job)) > 10
    for: 5m
    labels:
      channel: ci-cd
      severity: warn
    annotations:
      title: 'Too big number of traces archiving failures: {{$value}}'
      description: |
        Hey <!subteam^S940BK2TV|cicdops>! Traces archiving keeps failing for more than 5 minutes.
        Plese check https://dashboards.gitlab.net/d/000000159/ci?refresh=5m&orgId=1&panelId=153&fullscreen and
        https://sentry.gitlab.net/gitlab/gitlabcom/?query=BuildTraceChunk&statsPeriod=14d to find out more details

  - alert: CICDSidekiqQueuesTooBig
    expr: (sidekiq_queue_size{environment=~"gprd",name=~"pipeline.*"} and on (fqdn) gitlab:redis_master) > 1000
    for: 5m
    labels:
      channel: ci-cd
      severity: error
    annotations:
      title: 'Sidekiq queues for CI/CD are growing: {{$value}}'
      description: |
        Hey <!subteam^S940BK2TV|cicdops>! Sidekiq queues for CI/CD are growing and are over 1000 for
        more than last 5 minutes.
        Plese check https://dashboards.gitlab.net/d/000000159/ci?refresh=5m&from=now-1h&to=now&orgId=1&panelId=85&fullscreen

