groups:
- name: gitlab-com-ci.rules
  rules:
  - alert: CICDRunnerMachineCreationRateHigh
    expr: |
      (
        sum by(environment, tier, type, stage, shard) (gitlab_runner_autoscaling_machine_states{executor="docker+machine", state="creating"})
        /
        sum by(environment, tier, type, stage, shard) (gitlab_runner_autoscaling_machine_states{executor="docker+machine", state="idle"})
      ) > 1
    for: 5m
    labels:
      team: runner
      severity: s3
      alert_type: cause
    annotations:
      title: 'Machine creation rate for runners is too high: {{$value | printf "%.2f" }}'
      description: 'Machine creation rate for the last 1 minute is at least {{$value}}
        times greater than machines idle rate. This may by a symptom of problems with
        the auto-scaling provider. Check https://dashboards.gitlab.net/d/ci-runners-incident-autoscaling/ci-runners-incident-support-autoscaling.'
      runbook: docs/ci-runners/linux/autoscaling.md

  - record: quota_region:quota_usage:ratio
    expr: |
      max by (quota, region) (
        gcp_exporter_region_quota_usage{project="gitlab-ci-155816"}
        /
        gcp_exporter_region_quota_limit{project="gitlab-ci-155816"}
      )

  - alert: CICDGCPQuotaExceedingSoon
    expr: predict_linear(quota_region:quota_usage:ratio[6h], 3600) >= 1.0
    for: 5m
    labels:
      team: runner
      severity: s4
      alert_type: cause
    annotations:
      title: 'GCP Quota usage may exceed capacity'
      description: |
        Quota usage of {{ $labels.quota }} in {{ $labels.region }} is going to exceed capacity soon.
        Quota limit breach is coming!
        See https://dashboards.gitlab.net/dashboard/db/ci-autoscaling-providers

  - alert: CICDGCPQuotaCriticalUsage
    expr: quota_region:quota_usage:ratio * 100 > 95
    for: 5m
    labels:
      team: runner
      severity: s3
      alert_type: cause
    annotations:
      title: 'GCP Quota usage of is near limit'
      description: |
        Quota usage of {{ $labels.quota }} in {{ $labels.region }} is at the level of {{ $value }} for more than 5 minutes.
        It's less than 5% to reach quota limits!
        See https://dashboards.gitlab.net/dashboard/db/ci-autoscaling-providers

  - alert: CICDWorkhorseQueuingUnderperformant
    expr: |
      histogram_quantile(
        0.90,
        sum by(environment, tier, type, stage, shard, le) (
          rate(gitlab_workhorse_queueing_waiting_time_bucket{queue_name="ci_api_job_requests",job="gitlab-workhorse"}[5m])
        )
      ) >= 30
    for: 5m
    labels:
      team: runner
      severity: s3
      alert_type: cause
    annotations:
      title: '90% of request queued on Workhorse is longer than 30s'
      description: |
        90% of requests queued on Workhorse are longer than 30s for last 5 minutes.

        This should be considered as service degradation and the reason should be investigated.
      runbook: docs/ci-runners/ci_workhorse-queuing.md

  - alert: CICDTooManyArchivingTraceFailures
    expr: |
      sum by (environment, tier, type, stage, shard) (
        rate(job_trace_archive_failed_total[5m])
      ) > 10
    for: 5m
    labels:
      team: runner
      severity: s4
      alert_type: cause
    annotations:
      title: 'Too big number of traces archiving failures: {{$value}}'
      description: |
        Traces archiving keeps failing for more than 5 minutes.
        Plese check https://dashboards.gitlab.net/d/000000159/ci?refresh=5m&orgId=1&panelId=153&fullscreen,
        https://sentry.gitlab.net/gitlab/gitlabcom/?query=ArchiveTraceWorker, and
        https://sentry.gitlab.net/gitlab/gitlabcom/?query=ArchiveTracesCronWorker to find out more details
      runbook: docs/ci-runners/ci_too_many_archiving_trace_failures.md

  - alert: CICDSidekiqQueuesTooBig
    expr: |
      sum by (environment, tier, type, stage, shard, worker) (
        sidekiq_jobs_inflight:irate1m{feature_category=~"(continuous_integration|runner)"}
      ) > 100
    for: 5m
    labels:
      team: runner
      severity: s3
      alert_type: cause
    annotations:
      title: 'Sidekiq queues for CI/CD are growing: {{$value}}'
      description: |
        Sidekiq queues for CI/CD are growing and are over 1000 for
        more than last 5 minutes.
        Please check https://dashboards.gitlab.net/d/ci-runners-incident-gitlab-application/ci-runners-incident-support-gitlab-application?viewPanel=20&orgId=1&refresh=1m&from=now-3h&to=now

  - alert: CICDJobsStuckInDockerPull
    expr: |
      (
        sum by (environment, tier, type, stage, shard) (gitlab_runner_jobs{executor_stage="docker_pulling_image"})
        /
        sum by (environment, tier, type, stage, shard) (gitlab_runner_jobs{executor_stage="docker_run"})
      ) > 1
    for: 5m
    labels:
      team: runner
      severity: s4
      alert_type: cause
    annotations:
      title: 'More CI Jobs are in state docker_pulling_image than state docker_run'
      description: |
        More CI Jobs are in state docker_pulling_image than are in state docker_run
        for over 5 minutes. This could mean connectivity to docker hub is broken.
        Plese check https://dashboards.gitlab.net/d/000000159/ci?refresh=5m&viewPanel=46&orgId=1&from=now-1h&to=now&fullscreen

  - alert: GitLabRailsRepoMasterArchiveIsStale
    expr: |
      (
        time() - probe_http_last_modified_timestamp_seconds{job="gitlab-ci-git-repo-cache-blackbox"}
      ) > 86400 # 1 day
    for: 3m
    labels:
      team: create
      severity: s3
      alert_type: cause
    annotations:
      title: 'gitlab/gitlab-org repo master archive is stale'
      description: |
        The repo archive has not been updated in GCS in one day so it could mean that
        the mechanism that uploads the archive is failing. The archive is used to speed up
        fetches for CI jobs and not to overload the Gitaly node that hosts the repo.
        Try reaching out to the backend team to check the correctness of the upload mechanism.
