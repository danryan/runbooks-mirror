groups:
- name: gitlab-com-ci.rules
  rules:
  - alert: CICDNoJobsOnSharedRunners
    expr: sum(gitlab_runner_jobs{job="shared-runners"}) == 0
    for: 5m
    labels:
      team: runner
      severity: s4
      alert_type: cause
    annotations:
      title: 'Number of builds running on shared runners is too low: {{$value}}'
      description: "Number of builds running on shared runners for the last 5 minutes
        is 0. This may suggest problems with auto-scaling provider or Runner stability.
        You should check Runner's logs. Check http://dashboards.gitlab.net/dashboard/db/ci."

  - alert: CICDRunnersConcurrentLimitHigh
    expr: (sum(gitlab_runner_jobs) by (job) / sum(gitlab_runner_concurrent) by (job)) > 0.85
    for: 5m
    labels:
      team: runner
      severity: s4
      alert_type: cause
    annotations:
      title: "{{ $labels.job }} runners are using 85% of concurrent limit for more than 5 minutes."
      description: 'This may suggest problems with our autoscaled machines fleet OR
        abusive usage of Runners. Check https://dashboards.gitlab.net/dashboard/db/ci'

  - alert: CICDRunnersConcurrentLimitCritical
    expr: (sum(gitlab_runner_jobs) by (job) / sum(gitlab_runner_concurrent) by (job)) > 0.95
    for: 5m
    labels:
      team: runner
      severity: s3
      alert_type: cause
    annotations:
      title: "{{ $labels.job }} runners are using 95% of concurrent limit for more than 5 minutes."
      description: 'This may suggest problems with our autoscaled machines fleet OR
        abusive usage of Runners. Check https://dashboards.gitlab.net/dashboard/db/ci'

  - alert: CICDRunnerMachineCreationRateHigh
    expr: sum(gitlab_runner_autoscaling_machine_states{executor="docker+machine", state="creating"}) / (sum(gitlab_runner_autoscaling_machine_states{executor="docker+machine", state="idle"}) + 1) > 100
    for: 1m
    labels:
      team: runner
      severity: s3
      alert_type: cause
    annotations:
      title: 'Machine creation rate for runners is too high: {{$value | printf "%.2f" }}'
      description: 'Machine creation rate for the last 1 minute is at least {{$value}}
        times greater than machines idle rate. This may by a symptom of problems with
        the auto-scaling provider. Check https://dashboards.gitlab.net/d/ci-runners-incident-autoscaling/ci-runners-incident-support-autoscaling.'
      runbook: docs/ci-runners/linux/autoscaling.md

  - alert: CICDPrometheusDown
    expr: up{job="ci-node",fqdn=~"prometheus-.*us-east1-(c|d).*"} == 0
    for: 5m
    labels:
      team: runner
      severity: s4
      alert_type: cause
    annotations:
      title: CI Prometheus server {{ $labels.instance }} is down
      description: 'One or more of CI Prometheus servers is down: {{ $labels.instance }}.'

  - record: quota_region:quota_usage:ratio
    expr: |
      max by (quota, region) (
        gcp_exporter_region_quota_usage{project="gitlab-ci-155816"}
        /
        gcp_exporter_region_quota_limit{project="gitlab-ci-155816"}
      )

  - alert: CICDGCPQuotaExceedingSoon
    expr: predict_linear(quota_region:quota_usage:ratio[6h], 3600) >= 1.0
    for: 5m
    labels:
      team: runner
      severity: s4
      alert_type: cause
    annotations:
      title: GCP Quota usage may exceed capacity
      description: >
        Quota usage of {{ $labels.quota }} in {{ $labels.region }} is going to exceed capacity soon.
        Quota limit breach is coming!
        See https://dashboards.gitlab.net/dashboard/db/ci-autoscaling-providers

  - alert: CICDGCPQuotaCriticalUsage
    expr: quota_region:quota_usage:ratio * 100 > 95
    for: 5m
    labels:
      team: runner
      severity: s3
      alert_type: cause
    annotations:
      title: GCP Quota usage of is near limit
      description: >
        Quota usage of {{ $labels.quota }} in {{ $labels.region }} is at the level of {{ $value }} for more than 5 minutes.
        It's less than 5% to reach quota limits!
        See https://dashboards.gitlab.net/dashboard/db/ci-autoscaling-providers

  - alert: CICDWorkhorseQueuingUnderperformant
    expr: |
      histogram_quantile(
        0.90,
        sum(rate(gitlab_workhorse_queueing_waiting_time_bucket{queue_name="ci_api_job_requests",job="gitlab-workhorse-api"}[5m])) by (le)
      ) >= 30
    for: 5m
    labels:
      team: runner
      severity: s3
      alert_type: cause
    annotations:
      title: 90% of request queued on Workhorse is longer than 30s
      description: >
        90% of requests queued on Workhorse are longer than 30s for last 5 minutes.

        This should be considered as service degradation and the reason should be investigated.
      runbook: docs/ci-runners/ci_workhorse-queuing.md

  - alert: CICDTooManyArchivingTraceFailures
    expr: (sum (rate(job_trace_archive_failed_total[5m])) by (job)) > 10
    for: 5m
    labels:
      team: runner
      severity: s4
      alert_type: cause
    annotations:
      title: 'Too big number of traces archiving failures: {{$value}}'
      description: >
        Traces archiving keeps failing for more than 5 minutes.
        Plese check https://dashboards.gitlab.net/d/000000159/ci?refresh=5m&orgId=1&panelId=153&fullscreen,
        https://sentry.gitlab.net/gitlab/gitlabcom/?query=ArchiveTraceWorker, and
        https://sentry.gitlab.net/gitlab/gitlabcom/?query=ArchiveTracesCronWorker to find out more details
      runbook: docs/ci-runners/ci_too_many_archiving_trace_failures.md

  - alert: CICDSidekiqQueuesTooBig
    expr: (sidekiq_queue_size{environment=~"gprd",name=~"pipeline.*"} and on (fqdn) gitlab:redis_master) > 1000
    for: 5m
    labels:
      team: runner
      severity: s3
      alert_type: cause
    annotations:
      title: 'Sidekiq queues for CI/CD are growing: {{$value}}'
      description: >
        Sidekiq queues for CI/CD are growing and are over 1000 for
        more than last 5 minutes.
        Plese check https://dashboards.gitlab.net/d/000000159/ci?refresh=5m&from=now-1h&to=now&orgId=1&panelId=85&fullscreen

  - alert: CICDJobsStuckInDockerPull
    expr: >
      (sum by (environment, tier, type, stage, shard) (gitlab_runner_jobs{executor_stage="docker_pulling_image"})
      /
      sum by (environment, tier, type, stage, shard) (gitlab_runner_jobs{executor_stage="docker_run"})) > 1
    for: 5m
    labels:
      team: runner
      severity: s4
      alert_type: cause
    annotations:
      title: 'More CI Jobs are in state docker_pulling_image than state docker_run'
      description: >
        More CI Jobs are in state docker_pulling_image than are in state docker_run
        for over 5 minutes. This could mean connectivity to docker hub is broken.
        Plese check https://dashboards.gitlab.net/d/000000159/ci?refresh=5m&viewPanel=46&orgId=1&from=now-1h&to=now&fullscreen

  - alert: GitLabRailsRepoMasterArchiveIsStale
    expr: >
      (time() - probe_http_last_modified_timestamp_seconds{job="gitlab-ci-git-repo-cache-blackbox"}) > 86400 # 1 day
    for: 3m
    labels:
      team: create
      severity: s3
      alert_type: cause
    annotations:
      title: 'gitlab/gitlab-org repo master archive is stale'
      description: >
        The repo archive has not been updated in GCS in one day so it could mean that
        the mechanism that uploads the archive is failing. The archive is used to speed up
        fetches for CI jobs and not to overload the Gitaly node that hosts the repo.
        Try reaching out to the backend team to check the correctness of the upload mechanism.
