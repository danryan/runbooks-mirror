groups:
- name: haproxy availability
  interval: 60s
  rules:
  - record: instance:haproxy_availability:ratio
    expr: >
      avg_over_time(haproxy_up[1m])
  - record: backend:haproxy_backend_availability:ratio
    expr: >
      avg without (fqdn, instance) (
        avg_over_time(haproxy_backend_up[1m])
      )
  - record: server:haproxy_server_availability:ratio
    expr: >
      avg without (fqdn, instance, backend) (
        avg_over_time(haproxy_server_up[1m])
      )

- name: haproxy traffic
  rules:
  - record: backend_code:haproxy_server_http_responses_total:irate1m
    expr: >
      sum without (fqdn, instance, server) (
        irate(haproxy_server_http_responses_total[1m])
      )
  - record: backend_code:haproxy_server_http_responses_total:rate5m
    expr: >
      sum without (fqdn, instance, server) (
        rate(haproxy_server_http_responses_total[5m])
      )
  - record: frontend_code:haproxy_frontend_http_responses_total:irate1m
    expr: >
      sum without (fqdn, instance) (
        irate(haproxy_frontend_http_responses_total[1m])
      )

  - record: job_frontend:haproxy_frontend_bytes_in_total:irate1m
    expr: >
      sum without (fqdn, instance) (
        irate(haproxy_frontend_bytes_in_total[1m])
      )
  - record: job_frontend:haproxy_frontend_bytes_out_total:irate1m
    expr: >
      sum without (fqdn, instance) (
        irate(haproxy_frontend_bytes_out_total[1m])
      )

  - record: job_backend:haproxy_backend_response_errors_total:irate1m
    expr: >
      sum without (fqdn, instance) (
        irate(haproxy_backend_response_errors_total[1m])
      )
  - record: job_frontend:haproxy_frontend_request_errors_total:irate1m
    expr: >
      sum without (fqdn, instance) (
        irate(haproxy_frontend_request_errors_total[1m])
      )

  - alert: HighWebErrorRate
    expr: sum(backend_code:haproxy_server_http_responses_total:irate1m{backend="web",code="5xx",tier="lb"}) by (environment)
      - sum(backend_code:haproxy_server_http_responses_total:irate1m{backend="web",code!="5xx",tier="lb"}) by (environment)
      > 0
    for: 15s
    labels:
      pager: pagerduty
      severity: s1
      alert_type: symptom
    annotations:
      description: We are having more 5xx returns than any other reply. Web traffic
        is being impacted and the service is probably down. Have you thought about
        turning it off and on again?
      runbook: docs/frontend/haproxy.md
      title: High Error Rate on Front End Web
  - alert: High4xxApiRateLimit
    expr: |
      sum(backend_code:haproxy_server_http_responses_total:irate1m{backend="api_rate_limit",code="4xx",tier="lb"}) by (environment)
      / sum(backend_code:haproxy_server_http_responses_total:irate1m{tier="lb"}) by (environment)
      > 0.1
    for: 5m
    labels:
      pager: pagerduty
      severity: s1
      alert_type: cause
    annotations:
      description: We are seeing an increase of 4xx errors on the load balancing api
        rate limiting backend, more than 10% of http requests for at least 5 minutes.
      runbook: docs/frontend/haproxy.md
      title: High 4xx Error Rate on Front End Web on backend api_rate_limit
  - alert: High4xxRateLimit
    expr: |
      sum(backend_code:haproxy_server_http_responses_total:irate1m{backend!="registry",code="4xx",tier="lb"}) by (environment)
      / sum(backend_code:haproxy_server_http_responses_total:irate1m{backend!="registry",tier="lb"}) by (environment)
      > 0.25
    for: 5m
    labels:
      pager: pagerduty
      severity: s1
      alert_type: cause
    annotations:
      description: We are seeing an increase of 4xx errors on the load balancing across
        all backends, more than 25% of http requests for at least 5 minutes.
      runbook: docs/frontend/haproxy.md
      title: High 4xx Error Rate on Front End Web
  - alert: IncreasedErrorRateHTTPSGit
    expr: sum(backend_code:haproxy_server_http_responses_total:irate1m{code="5xx",tier="lb",backend="https_git"}) by (environment) > 20
    for: 15s
    labels:
      pager: pagerduty
      severity: s1
      alert_type: cause
    annotations:
      description: We are having a high rate of 5xx on https_git backend. It's likely that customers are impacted.
      runbook: docs/frontend/high-error-rate.md
      title: Increased Error Rate Across Fleet
  - alert: IncreasedErrorRateOtherBackends
    expr: sum(backend_code:haproxy_server_http_responses_total:irate1m{code="5xx",tier="lb",backend!="https_git"}) by (backend, environment) > 20
    for: 15s
    labels:
      pager: pagerduty
      severity: s1
      alert_type: cause
    annotations:
      description: We are having a high rate of 5xx across other backends (web(sockets)?/api/registry/etc, anything except https_git). It's likely that customers are impacted.
      runbook: docs/frontend/high-error-rate.md
      title: Increased Error Rate Across Fleet
  - alert: IncreasedBackendConnectionErrors
    expr: rate(haproxy_backend_connection_errors_total[1m]) > .1
    for: 2m
    labels:
      pager: pagerduty
      severity: s1
      alert_type: cause
    annotations:
      description: We are seeing an increase in backend connection errors on {{$labels.fqdn}} for backend {{$labels.backend}}.
        This likely indicates that requests are being sent to servers in a backend that are unable to fulfil them which will
        result in connection errors.
      runbook: docs/frontend/haproxy.md
      title: Increased HAProxy Backend Connection Errors
  - alert: IncreasedServerResponseErrors
    expr: rate(haproxy_server_response_errors_total[1m]) > .5
    for: 2m
    labels:
      pager: pagerduty
      severity: s1
      alert_type: cause
    annotations:
      description: We are seeing an increase in server response errors on {{$labels.fqdn}} for backend/server {{$labels.backend}}/{{$labels.server}}.
        This likely indicates that requests are being sent to servers and there are errors reported to users.
      runbook: docs/frontend/haproxy.md
      title: Increased Server Response Errors
  - alert: IncreasedServerConnectionErrors
    expr: rate(haproxy_server_connection_errors_total[1m]) > .1
    for: 2m
    labels:
      pager: pagerduty
      severity: s1
      alert_type: cause
    annotations:
      description: We are seeing an increase in server connection errors on {{$labels.fqdn}} for backend/server {{$labels.backend}}/{{$labels.server}}.
        This likely indicates that requests are being sent to servers and there are errors reported to users.
      runbook: docs/frontend/haproxy.md
      title: Increased Server Connection Errors
  - alert: HAProxyHighCPU
    expr: rate(haproxy_process_cpu_seconds_total[30s]) > 0.95
    for: 15m
    labels:
      pager: pagerduty
      severity: s1
      alert_type: cause
    annotations:
      runbook: docs/frontend/haproxy.md
      title: HAProxy process high CPU usage on {{ $labels.fqdn }}
  - alert: SSHMaxStartupsMaybeBreached
    # About the numbers here:
    # I know that "for: 30s" and the actual threshold (0.25) look low, but this
    # alert is meant to detect very brief spikes that occur for a few seconds at
    # the top of the minute.  They absolutely should not occur at all; if they
    # do then either the MaxStartups is being breached again, or something else
    # is exploding.  See the runbook for more details
    expr: sum(rate(haproxy_ssh_max_startups_breached_total[1m])) > 0.25
    for: 30s
    labels:
      severity: s4
      alert_type: cause
    annotations:
      description: We have detected conditions which suggest SSH's MaxStartups limit is being breached for git+ssh connections
      runbook: docs/frontend/ssh-maxstartups-breach.md
      title: Possible breach of SSH MaxStartups limit
  - alert: HAProxyExtraneousProcesses
    # DR environment is excluded here due to https://gitlab.com/gitlab-com/gl-infra/infrastructure/issues/8897
    expr: sum(namedprocess_namegroup_num_procs{fqdn=~"fe-[0-9].*", groupname="haproxy", env!="dr"}) by (fqdn) > 2
    for: 2h
    labels:
      severity: s3
      alert_type: cause
    annotations:
      title: Extraneous processes detected on haproxy front-end
      runbook: docs/frontend/haproxy.md
      description: >
        We are seeing extraneous processes on {{$labels.fqdn}}. These processes can interfere with state
        changes and may cause issues with deployments. Normally on reload old HAProxy processes with active
        connections should only remain active for 5 minutes. See the runbook for how to remove the extraneous
        processes
