groups:
- name: SnitchHeartBeat
  interval: 1m
  rules:
  - alert: SnitchHeartBeat
    expr: vector(1)
    labels:
      severity: s4
      alert_type: heartbeat
    annotations:
      description: SnitchHeartBeat
      runbook: docs/monitoring/prometheus-snitch.md
      title: SnitchHeartBeat
- name: Prometheus Metamon
  rules:
  - alert: PrometheusUnreachable
    expr: avg_over_time(up{job=~"prometheus.*"}[5m]) * 100 < 50
    for: 10m
    labels:
      pager: pagerduty
      severity: s1
      alert_type: cause
    annotations:
      description: '{{$labels.job}} at {{$labels.instance}} {{$labels.pod}} could not be scraped for
        over 10 minutes.'
      runbook: docs/monitoring/prometheus-is-down.md
      title: '{{$labels.job}} is unreachable'

  - alert: PrometheusNotConnectedToAlertmanagers
    expr: >
      sum without (fqdn,instance,pod) (
        prometheus_notifications_alertmanagers_discovered
      ) < 1
    for: 10m
    labels:
      pager: pagerduty
      severity: s1
      alert_type: cause
    annotations:
      description: >
        Prometheus {{$labels.shard}} is not connected to any Alertmanagers
      runbook: docs/monitoring/prometheus-is-down.md
      title: 'Prometheus not connected to any Alertmanagers'

  - alert: PrometheusManyRestarts
    expr: round(changes(process_start_time_seconds{job=~"(alertmanager|prometheus|thanos)"}[1h])) > 2
    for: 30m
    labels:
      pager: pagerduty
      severity: s1
      alert_type: cause
    annotations:
      description: '{{$labels.job}} at {{$labels.instance}} {{$labels.pod}} has restarted more than
        {{ $value }} times in the last hour. It may be crashlooping.'
      runbook: docs/monitoring/prometheus-is-down.md#thanos-compact
      title: '{{$labels.job}} is restarting frequently'

  - alert: PrometheusManyFileSDReadErrors
    expr: >
      rate(prometheus_sd_file_read_errors_total[5m]) /
      rate(prometheus_sd_file_scan_duration_seconds_count[5m])
      * 100 > 5
    for: 10m
    labels:
      severity: s4
      alert_type: cause
    annotations:
      description: '{{$labels.job}} at {{$labels.instance}} {{$labels.pod}} has {{$value}}% of DNS-SD
        requests failing.'
      runbook: docs/monitoring/prometheus-filesd-errors.md
      title: '{{$labels.job}} has many DNS-SD errors'
  - alert: PrometheusRuleEvaluationSlow
    expr: prometheus_rule_evaluation_duration_seconds{quantile="0.9"} > 60
    for: 10m
    labels:
      severity: s4
      alert_type: cause
    annotations:
      description: '{{$labels.job}} at {{$labels.instance}} {{$labels.pod}} has a 90th percentile
        latency of {{$value}}s completing rule evaluation cycles.'
      runbook: docs/monitoring/prometheus-slow-rule-eval.md
      title: '{{$labels.job}} is evaluating rules too slowly'
  - alert: PrometheusNotificationsBacklog
    expr: prometheus_notifications_queue_length > 0
    for: 10m
    labels:
      pager: pagerduty
      severity: s1
      alert_type: cause
    annotations:
      description: '{{$labels.job}} at {{$labels.instance}} {{$labels.pod}} is backlogging on the
        notifications queue. The queue has not been empty for 10 minutes. Current
        queue length: {{$value}}.'
      runbook: docs/monitoring/prometheus-notifications-backlog.md
      title: '{{$labels.job}} is backlogging on the notifications queue'

  - alert: PrometheusScrapingSlowly
    expr: |
      prometheus_target_interval_length_seconds{interval!~".*m.*",quantile="0.9"}
      > 2 * 60
    for: 10m
    labels:
      severity: s4
      alert_type: cause
    annotations:
      description: '{{$labels.job}} at {{$labels.instance}} {{$labels.pod}} has a 90th percentile
        latency of {{$value}}s for scraping targets in the {{$labels.interval}} target
        pool.'
      runbook: docs/monitoring/prometheus-slow-scrapes.md
      title: '{{$labels.job}} is scraping targets slowly'
  - alert: PrometheusInvalidConfigFile
    expr: prometheus_config_last_reload_successful == 0
    for: 30m
    labels:
      pager: pagerduty
      severity: s1
      alert_type: cause
    annotations:
      description: The configuration file for {{$labels.job}} at {{$labels.instance}} {{$labels.pod}}
        is invalid and was therefore not reloaded.
      runbook: docs/monitoring/prometheus-invalid-config.md
      title: '{{$labels.job}} has an invalid config'
  - alert: PrometheusSlowRuleEvaluation
    expr: >
      (prometheus_rule_group_last_duration_seconds /
      prometheus_rule_group_interval_seconds) * 100 > 70
    for: 30m
    labels:
      severity: s4
      alert_type: cause
    annotations:
      description: '{{$labels.job}} at {{$labels.instance}} {{$labels.pod}} rule group {{$labels.rule_group}}
        is taking more than 70% of the evaluation over the last 30 minutes.'
      runbook: docs/monitoring/prometheus-slow-rule-eval.md
      title: 'Prometheus has slow rule evaluations'
  - alert: PrometheusFailedCompactions
    expr: increase(prometheus_tsdb_compactions_failed_total[6h]) > 0
    labels:
      severity: s4
      alert_type: cause
    annotations:
      description: '{{$labels.job}} at {{$labels.instance}} {{$labels.pod}} has failed compactions in the last 6 hours'
      runbook: docs/monitoring/prometheus-failed-compactions.md
      title: 'Prometheus has failed compactions'

  - alert: PrometheusTSDBReloadsFailing
    expr: increase(prometheus_tsdb_reloads_failures_total[2h]) > 0
    for: 12h
    labels:
      severity: s4
      alert_type: cause
    annotations:
      description: 'Prometheus {{$labels.pod}} {{$labels.fqdn}} had {{$value | humanize}} reload failures over the last four hours.'
      title: Prometheus has issues reloading data blocks from disk

  - alert: PrometheusNotIngestingSamples
    expr: rate(prometheus_tsdb_head_samples_appended_total[5m]) <= 0
    for: 10m
    labels:
      severity: s4
      alert_type: cause
    annotations:
      description: 'Prometheus {{$labels.pod }} {{$labels.fqdn}} is not ingesting samples.'
      title: Prometheus isn't ingesting samples

  - alert: PrometheusTargetScrapesDuplicate
    expr: increase(prometheus_target_scrapes_sample_duplicate_timestamp_total[5m]) > 0
    for: 10m
    labels:
      severity: s4
      alert_type: cause
    annotations:
      description: 'Prometheus {{$labels.pod }} {{$labels.fqdn}} has many samples rejected due to duplicate timestamps but different values'
      title: Prometheus has many samples rejected

  - alert: PrometheusLargeScrapes
    expr: increase(prometheus_target_scrapes_exceeded_sample_limit_total[30m]) > 60
    labels:
      severity: s4
      alert_type: cause
    annotations:
      description: '{{$labels.job}} at {{$labels.instance}} {{$labels.pod}} has many scrapes that exceed the sample limit'
      runbook: docs/monitoring/prometheus-scrape-errors.md
      title: 'Prometheus has large scrape errors'
  - alert: PrometheusFailedCheckpoints
    expr: increase(prometheus_tsdb_checkpoint_creations_failed_total[5m]) > 0
    labels:
      severity: s4
      alert_type: cause
    annotations:
      description: '{{$labels.job}} at {{$labels.instance}} {{$labels.pod}} has failed to create checkpoints.'
      runbook: docs/monitoring/prometheus-failed-checkpoints.md
      title: 'Prometheus has failed checkpoints'
  - alert: PrometheusFailedDeletingCheckpoints
    expr: increase(prometheus_tsdb_checkpoint_deletions_failed_total[5m]) > 0
    labels:
      severity: s4
      alert_type: cause
    annotations:
      description: '{{$labels.job}} at {{$labels.instance}} {{$labels.pod}} has failed to delete checkpoints.'
      runbook: docs/monitoring/prometheus-failed-checkpoints.md
      title: 'Prometheus has failed deleting checkpoints'
  - alert: PrometheusWALTruncationsFailed
    expr: increase(prometheus_tsdb_wal_truncations_failed_total[5m]) > 0
    labels:
      severity: s4
      alert_type: cause
    annotations:
      description: '{{$labels.job}} at {{$labels.instance}} {{$labels.pod}} has failed to wal truncations.'
      runbook: docs/monitoring/prometheus-failed-wal-truncations.md
      title: 'Prometheus has failed wal truncations'
  - alert: PrometheusRuleEvalFailures
    expr: increase(prometheus_rule_evaluation_failures_total[5m]) > 0
    labels:
      severity: s4
      alert_type: cause
    annotations:
      description: '{{$labels.job}} at {{$labels.instance}} {{$labels.pod}} has failing rule evaluations.'
      runbook: docs/monitoring/prometheus-failing-rule-evaluations.md
      title: 'Prometheus has failing rule evaluations'
  - alert: PrometheusEmptyJobs
    expr: prometheus_sd_discovered_targets == 0
    for: 5m
    labels:
      severity: s4
      alert_type: cause
    annotations:
      description: '{{$labels.config}} at {{$labels.instance}} {{$labels.pod}} has no targets in service discovery.'
      runbook: docs/monitoring/prometheus-empty-sd.md
      title: 'Prometheus has no targets'
  - alert: PrometheusHighMemoryUtilization
    expr: go_memstats_alloc_bytes{fqdn!="",job="prometheus"} / on (fqdn) node_memory_MemTotal_bytes{fqdn!=""} * 100 > 90
    for: 30m
    labels:
      severity: s4
      alert_type: cause
    annotations:
      description: 'Prometheus at {{$labels.instance}} {{$labels.pod}} has very high ({{$value}}%) system memory utilization.'
      runbook: docs/monitoring/prometheus-high-memory.md
      title: 'Prometheus High Memory Utilization'
  - alert: PrometheusWALCorruption
    expr: increase(prometheus_tsdb_wal_corruptions_total[5m]) > 0
    labels:
      severity: s4
      alert_type: cause
    annotations:
      description: 'Prometheus at {{$labels.instance}} {{$labels.pod}} has {{$value}} WAL Corruptions.'
      runbook: docs/monitoring/prometheus-wal-corruption.md
      title: 'Prometheus WAL Corruption'

- name: Thanos Metamon
  rules:
  - alert: ThanosBucketHighOperationFailures
    expr: |
      (
        sum by (instance) (rate(thanos_objstore_bucket_operation_failures_total[5m]))
      /
        sum by (instance) (rate(thanos_objstore_bucket_operations_total[5m]))
      * 100 > 5
      )
    for: 15m
    labels:
      severity: s4
      alert_type: cause
    annotations:
      title: Thanos has failing storage operations
      description: Thanos {{ $labels.instance }} has failing storage bucket operations.
      runbook: docs/monitoring/thanos.md

- name: Thanos Compact Metamon
  rules:
  - alert: ThanosCompactHalted
    expr: thanos_compactor_halted == 1
    for: 5m
    labels:
      severity: s4
      alert_type: cause
    annotations:
      title: Thanos compaction halted
      description: Thanos compact {{ $labels.instance }} has failed to run and is now halted.
      runbook: docs/monitoring/thanos-compact.md
  - alert: ThanosCompactHighCompactionFailures
    expr: |
      (
        sum by (instance) (rate(thanos_compact_group_compactions_failures_total[5m]))
      /
        sum by (instance) (rate(thanos_compact_group_compactions_total[5m]))
      * 100 > 5
      )
    for: 15m
    labels:
      severity: s4
      alert_type: cause
    annotations:
      title: Thanos Compact is failing to execute compactions
      description: Thanos Compact {{$labels.instance}} is failing to execute {{ $value | humanize
        }}% of compactions.
      runbook: docs/monitoring/thanos-compact.md
  - alert: ThanosCompactCompactionsFailed
    expr: rate(prometheus_tsdb_compactions_failed_total[5m]) > 0
    labels:
      severity: s4
      alert_type: cause
    annotations:
      title: Thanos compaction is failing
      description: Thanos compact {{ $labels.instance }} has failed compactions.
      runbook: docs/monitoring/thanos-compact.md
  - alert: ThanosCompactNotRunIn24Hours
    expr: (time() - (thanos_objstore_bucket_last_successful_upload_time > 0)) > 86400
    labels:
      severity: s4
      alert_type: cause
    annotations:
      title: Thanos compaction has not run in 24 hours.
      description: Thanos compact {{ $labels.instance }} has not uploaded any blocks in 24 hours.
      runbook: docs/monitoring/thanos-compact.md

- name: Thanos Rule Metamon
  rules:
  - alert: ThanosRuleQueueIsDroppingAlerts
    expr: rate(thanos_alert_queue_alerts_dropped_total[5m]) > 0
    for: 5m
    labels:
      severity: s4
      alert_type: cause
    annotations:
      title: Thanos Rule is failing to queue alerts
      description: Thanos Rule {{$labels.instance}} is failing to queue alerts.
      runbook: docs/monitoring/thanos-rule.md
  - alert: ThanosRuleSenderIsFailingAlerts
    expr: rate(thanos_alert_sender_alerts_dropped_total[5m]) > 0
    for: 5m
    labels:
      severity: s4
      alert_type: cause
    annotations:
      title: Thanos Rule is failing to send alerts
      description: Thanos Rule {{$labels.instance}} is failing to send alerts
        to alertmanager.
      runbook: docs/monitoring/thanos-rule.md
  - alert: ThanosRuleHighRuleEvaluationFailures
    expr: |
      (
        rate(prometheus_rule_evaluation_failures_total{job="thanos"}[5m])
      /
        rate(prometheus_rule_evaluations_total{job="thanos"}[5m])
      * 100 > 0
      )
    for: 5m
    labels:
      severity: s4
      alert_type: cause
    annotations:
      title: Thanos Rule is failing to evaluate rules
      description: Thanos Rule {{$labels.instance}} is failing to evaluate {{ $value }}% rules.
      runbook: docs/monitoring/thanos-rule.md
  - alert: ThanosRuleHighRuleEvaluationWarnings
    expr: rate(thanos_rule_evaluation_with_warnings_total[5m]) > 0
    for: 15m
    labels:
      severity: s4
      alert_type: cause
    annotations:
      title: Thanos Rule has high number of evaluation warnings
      description: Thanos Rule {{$labels.instance}} has high number of evaluation
        warnings.
      runbook: docs/monitoring/thanos-rule.md
  - alert: ThanosRuleRuleEvaluationLatencyHigh
    expr: |
      (
        sum by (instance, rule_group) (prometheus_rule_group_last_duration_seconds{job="thanos"})
      >
        sum by (instance, rule_group) (prometheus_rule_group_interval_seconds{job="thanos"})
      )
    for: 5m
    labels:
      severity: s4
      alert_type: cause
    annotations:
      title: Thanos Rule has high evaluation latency
      description: Thanos Rule {{$labels.instance}} has higher evaluation latency
        than eval interval for {{$labels.rule_group}}.
      runbook: docs/monitoring/thanos-rule.md
  - alert: ThanosRuleConfigReloadFailure
    expr: thanos_rule_config_last_reload_successful != 1
    for: 5m
    labels:
      severity: s4
      alert_type: cause
    annotations:
      title: Thanos Rule has not been able to reload its configuration
      description: Thanos Rule {{$labels.instance}} has not been able to reload its configuration.
      runbook: docs/monitoring/thanos-rule.md
  - alert: ThanosRuleQueryHighDNSFailures
    expr: |
      (
        rate(thanos_ruler_query_apis_dns_failures_total[5m])
      /
        rate(thanos_ruler_query_apis_dns_lookups_total[5m]) > 0
      * 100 > 1
      )
    for: 15m
    labels:
      severity: s4
      alert_type: cause
    annotations:
      title: Thanos Rule has failing DNS queries
      description: Thanos Rule {{$labels.instance}} has {{ $value | humanize }}% of failing DNS
        queries for query endpoints.
      runbook: docs/monitoring/thanos-rule.md
  - alert: ThanosRuleAlertmanagerHighDNSFailures
    expr: |
      (
        rate(thanos_ruler_alertmanagers_dns_failures_total[5m])
      /
        rate(thanos_ruler_alertmanagers_dns_lookups_total[5m]) > 0
      * 100 > 1
      )
    for: 15m
    labels:
      severity: s4
      alert_type: cause
    annotations:
      title: Thanos Rule has failing DNS queries
      description: Thanos Rule {{$labels.instance}} has {{ $value | humanize }}% of failing DNS
        queries for Alertmanager endpoints.
      runbook: docs/monitoring/thanos-rule.md
  # TODO: Needs investigation to see why this is failing/noisy before we enable it.
  # - alert: ThanosRuleNoEvaluationFor10Intervals
  #   expr: |
  #     time() - prometheus_rule_group_last_evaluation_timestamp_seconds{job="thanos"}
  #     >
  # #     10 * prometheus_rule_group_interval_seconds{job="thanos"}
  #   for: 5m
  #   labels:
  #     severity: s4
  #     alert_type: cause
  #   annotations:
  #     title: Thanos Rule is not evalutiang rules.
  #     description: Thanos Rule {{$labels.instance}} has {{ $value | humanize }}% rule groups
  #       that did not evaluate for at least 10x of their expected interval.
  #     runbook: docs/monitoring/thanos-rule.md
  - alert: ThanosNoRuleEvaluations
    expr: |
      sum by (instance) (rate(prometheus_rule_evaluations_total[2m])) <= 0
        and
      sum by (instance) (thanos_rule_loaded_rules) > 0
    labels:
      severity: s4
      alert_type: cause
    annotations:
      title: Thanos Rule is not evaluating rules
      description: Thanos Rule {{$labels.instance}} did not perform any rule evaluations in the
        past 2 minutes.
      runbook: docs/monitoring/thanos-rule.md

- name: Thanos Store Metamon
  rules:
  - alert: ThanosStoreSeriesGateLatencyHigh
    expr: |
      (
        histogram_quantile(0.9, sum by (instance, le) (rate(thanos_bucket_store_series_gate_duration_seconds_bucket[5m]))) > 2
      and
        sum by (instance) (rate(thanos_bucket_store_series_gate_duration_seconds_count[5m])) > 0
      )
    for: 10m
    labels:
      severity: s4
      alert_type: cause
    annotations:
      title: Thanos Store has high latency
      description: Thanos Store {{$labels.instance}} has a 99th percentile latency of {{ $value
        }} seconds for store series gate requests.
      runbook: docs/monitoring/thanos-store.md
  - alert: ThanosStoreObjstoreOperationLatencyHigh
    expr: |
      (
        histogram_quantile(0.9, sum by (instance, le) (rate(thanos_objstore_bucket_operation_duration_seconds_bucket[5m]))) > 2
      and
        sum by (instance) (rate(thanos_objstore_bucket_operation_duration_seconds_count[5m])) > 0
      )
    for: 10m
    labels:
      severity: s4
      alert_type: cause
    annotations:
      title: Thanos Store Bucket has high latency
      description: Thanos Store {{$labels.instance}} Bucket has a 99th percentile latency of
        {{ $value }} seconds for the bucket operations.
      runbook: docs/monitoring/thanos-store.md
