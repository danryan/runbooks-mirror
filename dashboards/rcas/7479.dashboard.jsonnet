local grafana = import 'grafonnet/grafana.libsonnet';
local dashboard = grafana.dashboard;

// local seriesOverrides = import 'series_overrides.libsonnet';
local commonAnnotations = import 'common_annotations.libsonnet';
// local promQuery = import 'prom_query.libsonnet';
local templates = import 'templates.libsonnet';
// local colors = import 'colors.libsonnet';
// local platformLinks = import 'platform_links.libsonnet';
// local capacityPlanning = import 'capacity_planning.libsonnet';
local layout = import 'layout.libsonnet';
local basic = import 'basic.libsonnet';
// local redisCommon = import 'redis_common_graphs.libsonnet';
// local nodeMetrics = import 'node_metrics.libsonnet';
// local keyMetrics = import 'key_metrics.libsonnet';
// local serviceCatalog = import 'service_catalog.libsonnet';
// local row = grafana.row;
// local template = grafana.template;
// local graphPanel = grafana.graphPanel;
// local annotation = grafana.annotation;
local text = grafana.text;

dashboard.new(
  'Gitaly n+1 calls causing bad latency and sidekiq queues to grow',
  schemaVersion=16,
  tags=['rca'],
  timezone='UTC',
  graphTooltip='shared_crosshair',
  time_from='2019-08-07 22:00:00',
  time_to='2019-08-08 18:00:00',
)
.addAnnotation(commonAnnotations.deploymentsForEnvironment)
.addAnnotation(commonAnnotations.deploymentsForEnvironmentCanary)
.addTemplate(templates.ds)
.addTemplate(templates.environment)
.addPanels(layout.grid([
  text.new(title='CPU on Gitaly',
  mode='markdown',
  content='
The RCA issue for this incident is at https://gitlab.com/gitlab-com/gl-infra/infrastructure/issues/7479.

"Group Zero" is a pseudonym for the group who pushed the extremely large tag sets to GitLab.com.

"Group Zero" seems to start generating push activity at about 23h00 hours.

The first sign of a problem was CPU spikes on some Gitaly nodes.
This occurred as Gitaly struggled with _O(n^2)_ `FindAllTag` queries,
generated by `PostReceiveWorkers`.
  '),
  basic.saturationTimeseries(
    title="Gitaly Node CPU",
    description="",
    query='
      avg(instance_cpu:node_cpu_seconds_not_idle:rate1m{type="gitaly", environment="gprd"}) by (fqdn)
    ',
    legendFormat='{{ fqdn }}',
    yAxisLabel='CPU',
    interval="1m",
    intervalFactor=3,
    linewidth=1
),

  // --------------------------------------------------------------------

  text.new(title='FindAllTags',
  mode='markdown',
  content='
The `FindAllTags` will return multiple messages for each request made.
This is to ensure that each messages is kept reasonably small.
By calculating the average message size, we can detect if Gitaly is having
to deal with very large tag payloads.
  '),
  basic.timeseries(
    title="FindAllTags Response Size",
    description="",
    query='
      grpc_server_msg_sent_total{grpc_method="FindAllTags", environment="gprd"}
      /
      grpc_server_started_total{grpc_method="FindAllTags", environment="gprd"}
    ',
    legendFormat='{{ fqdn }}',
    yAxisLabel='Average messages per FindAllTags request',
    interval="1m",
    intervalFactor=3,
    linewidth=1
),

  // --------------------------------------------------------------------

  text.new(title='Queue Length',
  mode='markdown',
  content='
Queue lengths increased for several reasons:

1. "Group Zero" created `post_receive` jobs taking several hours, which starved the Sidekiq workers from performing other tasks.
1. `post_receive` jobs calling `FindAllTags` put additional load on Gitaly servers which slowed other background tasks down
  '),
  basic.timeseries(
    title="Queue Length",
    description="",
    query='
      sidekiq_queue_size{environment="gprd", name=~"web_hook|project_service|post_receive", type="redis-sidekiq"} and on(fqdn) (redis_connected_slaves != 0)
    ',
    legendFormat='{{ name }}',
    yAxisLabel='Queue Length',
    interval="1m",
    intervalFactor=3,
    linewidth=1
),

  // --------------------------------------------------------------------

  text.new(title='Processing Time',
  mode='markdown',
  content='
This chart shows the sum total amount of time spent processing jobs in each queue.
The `PostReceive` queue timings appear to be tied to the `FindAllTags` issue, but it
appears that there may still be a pathologic repository causing high processing time.
  '),
  basic.timeseries(
    title="Processing Time",
    description="",
    query='
      sum(rate(sidekiq_jobs_completion_time_seconds_sum{environment="gprd", worker=~"WebHookWorker|ProjectServiceWorker|PostReceive"}[1m])) by (worker)
    ',
    legendFormat='{{ worker }}',
    yAxisLabel='Total Processing Time',
    interval="1m",
    intervalFactor=3,
    linewidth=1
),

  // --------------------------------------------------------------------

  text.new(title='PostReceive Latency',
  mode='markdown',
  content='
`PostRecieve` latency. Lower is better. As a retrospective item, [we have now added](https://gitlab.com/gitlab-com/runbooks/merge_requests/1318) per-worker latency monitoring.
this shows the p80 latency of the `PostReceive` worker. This rose due to Gitaly latency caused by the `FindAllTags` issue and the huge payloads being processes.
  '),
  basic.timeseries(
    title="p80 Latency for PostReceive jobs",
    description="",
    query='
      histogram_quantile(0.8, sum(rate(sidekiq_jobs_completion_time_seconds_bucket{environment="gprd", worker="PostReceive"}[$__interval])) by (le, environment, stage, tier, type, worker))
    ',
    legendFormat='{{ worker }}',
    yAxisLabel='PostReceive latency',
    interval="1m",
    intervalFactor=3,
    linewidth=1
),

], cols=2, rowHeight=10, startRow=1))
+ {
  annotations: {
    list+: [
{
      datasource: "Pagerduty",
      enable: true,
      hide: false,
      iconColor: "#F2495C",
      limit: 100,
      name: "GitLab Production Pagerduty",
      serviceId: "PATDFCE",
      showIn: 0,
      tags: [],
      type: "tags",
      urgency: "high",
    },
    {
      datasource: "Pagerduty",
      enable: true,
      hide: false,
      iconColor: "#C4162A",
      limit: 100,
      name: "GitLab Production SLO",
      serviceId: "P7Q44DU",
      showIn: 0,
      tags: [],
      type: "tags",
      urgency: "high",
    },
    {
      datasource: "Simple Annotations",
      enable: true,
      hide: false,
      iconColor: "#5794F2",
      limit: 100,
      name: "Key Events",
      queries: [
        { date: "2019-08-07T23:00:00Z", text: "Group Zero generates large amount of WebHook, ProjectServer activity" },
        { date: "2019-08-08T00:30:00Z", text: "First CPU Spikes" },
        { date: "2019-08-08T00:30:00Z", text: "CPU usage on file-33,34,35 starts going up" },
        { date: "2019-08-08T01:47:00Z", text: "cache_vulnerability_history feature flag enabled" },
        { date: "2019-08-08T04:24:00Z", text: "gitaly latency APDEX starts going down, CPU usage on file-33,34,35 reaches 80%" },
        { date: "2019-08-08T05:10:00Z", text: "web_hook queue starts growing" },
        { date: "2019-08-08T05:29:00Z", text: "gitaly latency APDEX alert in #alerts-general" },
        { date: "2019-08-08T06:28:00Z", text: "Incident opened by EOC" },
        { date: "2019-08-08T06:31:00Z", text: "#backend and #g_gitaly pinged for support" },
        { date: "2019-08-08T06:41:00Z", text: "IMOC pinged via /pd-mgr" },

        { date: "2019-08-08T07:21:00Z", text: "status.io post" },
        { date: "2019-08-08T08:03:00Z", text: "users librideploy and dann30 blocked by abuse team - but it didn't help" },
        { date: "2019-08-08T08:16:00Z", text: "gitaly on file-33 restarted, post receive queue starting to go down a bit, but then rises again" },
        { date: "2019-08-08T08:25:00Z", text: "gitaly on file-34 restarted" },
        { date: "2019-08-08T08:29:00Z", text: "gitaly on file-35 restarted" },
        { date: "2019-08-08T08:10:00Z", text: "cache_vulnerability_history feature flag disabled" },
        { date: "2019-08-08T11:08:00Z", text: "starting to deploy hot patch for Gitaly n+1 calls to prod" },
        { date: "2019-08-08T11:14:00Z", text: "post_receive queue starting to decline" },
        { date: "2019-08-08T11:20:00Z", text: "deployment of hot patch finished" },
        { date: "2019-08-08T11:45:00Z", text: "problematic jobs manually killed" },
        { date: "2019-08-08T13:16:00Z", text: "hot patch reverted" },
        { date: "2019-08-08T14:30:00Z", text: "manually added web_hook to queue groups on the pages sidekiq nodes to accelerate queue processing" },
        { date: "2019-08-08T14:55:00Z", text: "queues down to zero" },
      ],
      showIn: 0,
      tags: [],
      type: "tags",
    },
],
  },
}
