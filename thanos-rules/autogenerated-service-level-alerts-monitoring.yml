# WARNING. DO NOT EDIT THIS FILE BY HAND. USE ./thanos-rules-jsonnet/service-component-alerts.jsonnet TO GENERATE IT
# YOUR CHANGES WILL BE OVERRIDDEN
groups:
- name: 'Service Component Alerts: monitoring'
  interval: 1m
  partial_response_strategy: warn
  rules:
  - alert: MonitoringServiceGrafanaCloudsqlErrorSLOViolation
    for: 2m
    annotations:
      title: The grafana_cloudsql SLI of the monitoring service (`{{ $labels.stage
        }}` stage) has an error rate violating SLO
      description: |
        Grafana uses a GCP CloudSQL instance. This SLI represents SQL transactions to that service.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "738880335"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          avg_over_time(stackdriver_cloudsql_database_cloudsql_googleapis_com_database_postgresql_transaction_count{database="grafana",environment="{{ $labels.environment }}",job="stackdriver",stage="{{ $labels.stage }}",transaction_type="rollback"}[5m])
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "yes"
      window: 1h
    expr: |
      (
        (
          gitlab_component_errors:ratio_1h{component="grafana_cloudsql",monitor="global",type="monitoring"}
          > (14.4 * 0.001000)
        )
        and
        (
          gitlab_component_errors:ratio_5m{component="grafana_cloudsql",monitor="global",type="monitoring"}
          > (14.4 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_1h{component="grafana_cloudsql",monitor="global",type="monitoring"}) >= 1
      )
  - alert: MonitoringServiceGrafanaCloudsqlErrorSLOViolation
    for: 2m
    annotations:
      title: The grafana_cloudsql SLI of the monitoring service (`{{ $labels.stage
        }}` stage) has an error rate violating SLO
      description: |
        Grafana uses a GCP CloudSQL instance. This SLI represents SQL transactions to that service.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "738880335"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          avg_over_time(stackdriver_cloudsql_database_cloudsql_googleapis_com_database_postgresql_transaction_count{database="grafana",environment="{{ $labels.environment }}",job="stackdriver",stage="{{ $labels.stage }}",transaction_type="rollback"}[5m])
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "yes"
      window: 6h
    expr: |
      (
        (
          gitlab_component_errors:ratio_6h{component="grafana_cloudsql",monitor="global",type="monitoring"}
          > (6 * 0.001000)
        )
        and
        (
          gitlab_component_errors:ratio_30m{component="grafana_cloudsql",monitor="global",type="monitoring"}
          > (6 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_6h{component="grafana_cloudsql",monitor="global",type="monitoring"}) >= 1
      )
  - alert: MonitoringServiceGrafanaCloudsqlTrafficCessation
    for: 5m
    annotations:
      title: The grafana_cloudsql SLI of the monitoring service (`{{ $labels.stage
        }}` stage) has not received any traffic in the past 30 minutes
      description: |
        Grafana uses a GCP CloudSQL instance. This SLI represents SQL transactions to that service.

        This alert signifies that the SLI is reporting a cessation of traffic, but the signal is not absent.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "159285256"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          avg_over_time(stackdriver_cloudsql_database_cloudsql_googleapis_com_database_postgresql_transaction_count{database="grafana",environment="{{ $labels.environment }}",job="stackdriver",stage="{{ $labels.stage }}"}[5m])
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
    expr: |
      gitlab_component_ops:rate_30m{type="monitoring", component="grafana_cloudsql", stage="main", monitor="global"} == 0
  - alert: MonitoringServiceGrafanaCloudsqlTrafficAbsent
    for: 30m
    annotations:
      title: The grafana_cloudsql SLI of the monitoring service (`{{ $labels.stage
        }}` stage) has not reported any traffic in the past 30 minutes
      description: |
        Grafana uses a GCP CloudSQL instance. This SLI represents SQL transactions to that service.

        This alert signifies that the SLI was previously reporting traffic, but is no longer - the signal is absent.

        This could be caused by a change to the metrics used in the SLI, or by the service not receiving traffic.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "159285256"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          avg_over_time(stackdriver_cloudsql_database_cloudsql_googleapis_com_database_postgresql_transaction_count{database="grafana",environment="{{ $labels.environment }}",job="stackdriver",stage="{{ $labels.stage }}"}[5m])
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
    expr: |
      gitlab_component_ops:rate_5m{type="monitoring", component="grafana_cloudsql", stage="main", monitor="global"} offset 1h
      unless
      gitlab_component_ops:rate_5m{type="monitoring", component="grafana_cloudsql", stage="main", monitor="global"}
  - alert: MonitoringServiceGrafanaGoogleLbErrorSLOViolation
    for: 2m
    annotations:
      title: The grafana_google_lb SLI of the monitoring service (`{{ $labels.stage
        }}` stage) has an error rate violating SLO
      description: |
        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "665917726"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(stackdriver_https_lb_rule_loadbalancing_googleapis_com_https_request_count{environment="{{ $labels.environment }}",project_id="gitlab-ops",response_code_class="500",stage="{{ $labels.stage }}",target_proxy_name="k8s2-ts-4zodnh0s-monitoring-grafana-lhbkv8d3"}[5m])
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "no"
      window: 1h
    expr: |
      (
        (
          gitlab_component_errors:ratio_1h{component="grafana_google_lb",monitor="global",type="monitoring"}
          > (14.4 * 0.001000)
        )
        and
        (
          gitlab_component_errors:ratio_5m{component="grafana_google_lb",monitor="global",type="monitoring"}
          > (14.4 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_1h{component="grafana_google_lb",monitor="global",type="monitoring"}) >= 1
      )
  - alert: MonitoringServiceGrafanaGoogleLbErrorSLOViolation
    for: 2m
    annotations:
      title: The grafana_google_lb SLI of the monitoring service (`{{ $labels.stage
        }}` stage) has an error rate violating SLO
      description: |
        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "665917726"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(stackdriver_https_lb_rule_loadbalancing_googleapis_com_https_request_count{environment="{{ $labels.environment }}",project_id="gitlab-ops",response_code_class="500",stage="{{ $labels.stage }}",target_proxy_name="k8s2-ts-4zodnh0s-monitoring-grafana-lhbkv8d3"}[5m])
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "no"
      window: 6h
    expr: |
      (
        (
          gitlab_component_errors:ratio_6h{component="grafana_google_lb",monitor="global",type="monitoring"}
          > (6 * 0.001000)
        )
        and
        (
          gitlab_component_errors:ratio_30m{component="grafana_google_lb",monitor="global",type="monitoring"}
          > (6 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_6h{component="grafana_google_lb",monitor="global",type="monitoring"}) >= 1
      )
  - alert: MonitoringServiceMemcachedThanosQfeLabelsApdexSLOViolation
    for: 2m
    annotations:
      title: The memcached_thanos_qfe_labels SLI of the monitoring service (`{{ $labels.stage
        }}` stage) has an apdex violating SLO
      description: |
        Various memcached instances support our thanos infrastructure, for the store and query-frontend components.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3835305006"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (env,environment,tier,stage,le) (
            rate(thanos_memcached_operation_duration_seconds_bucket{environment="{{ $labels.environment }}",job="memcached-thanos-qfe-labels-metrics",stage="{{ $labels.stage }}",type="monitoring"}[5m])
          )
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "no"
      window: 1h
    expr: |
      (
        (
          gitlab_component_apdex:ratio_1h{component="memcached_thanos_qfe_labels",monitor="global",type="monitoring"}
          < (1 - 14.4 * 0.001000)
        )
        and
        (
          gitlab_component_apdex:ratio_5m{component="memcached_thanos_qfe_labels",monitor="global",type="monitoring"}
          < (1 - 14.4 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_1h{component="memcached_thanos_qfe_labels",monitor="global",type="monitoring"}) >= 1
      )
  - alert: MonitoringServiceMemcachedThanosQfeLabelsApdexSLOViolation
    for: 2m
    annotations:
      title: The memcached_thanos_qfe_labels SLI of the monitoring service (`{{ $labels.stage
        }}` stage) has an apdex violating SLO
      description: |
        Various memcached instances support our thanos infrastructure, for the store and query-frontend components.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3835305006"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (env,environment,tier,stage,le) (
            rate(thanos_memcached_operation_duration_seconds_bucket{environment="{{ $labels.environment }}",job="memcached-thanos-qfe-labels-metrics",stage="{{ $labels.stage }}",type="monitoring"}[5m])
          )
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "no"
      window: 6h
    expr: |
      (
        (
          gitlab_component_apdex:ratio_6h{component="memcached_thanos_qfe_labels",monitor="global",type="monitoring"}
          < (1 - 6 * 0.001000)
        )
        and
        (
          gitlab_component_apdex:ratio_30m{component="memcached_thanos_qfe_labels",monitor="global",type="monitoring"}
          < (1 - 6 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_6h{component="memcached_thanos_qfe_labels",monitor="global",type="monitoring"}) >= 1
      )
  - alert: MonitoringServiceMemcachedThanosQfeLabelsErrorSLOViolation
    for: 2m
    annotations:
      title: The memcached_thanos_qfe_labels SLI of the monitoring service (`{{ $labels.stage
        }}` stage) has an error rate violating SLO
      description: |
        Various memcached instances support our thanos infrastructure, for the store and query-frontend components.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1168951987"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(thanos_memcached_operation_failures_total{environment="{{ $labels.environment }}",job="memcached-thanos-qfe-labels-metrics",stage="{{ $labels.stage }}",type="monitoring"}[5m])
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "no"
      window: 1h
    expr: |
      (
        (
          gitlab_component_errors:ratio_1h{component="memcached_thanos_qfe_labels",monitor="global",type="monitoring"}
          > (14.4 * 0.001000)
        )
        and
        (
          gitlab_component_errors:ratio_5m{component="memcached_thanos_qfe_labels",monitor="global",type="monitoring"}
          > (14.4 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_1h{component="memcached_thanos_qfe_labels",monitor="global",type="monitoring"}) >= 1
      )
  - alert: MonitoringServiceMemcachedThanosQfeLabelsErrorSLOViolation
    for: 2m
    annotations:
      title: The memcached_thanos_qfe_labels SLI of the monitoring service (`{{ $labels.stage
        }}` stage) has an error rate violating SLO
      description: |
        Various memcached instances support our thanos infrastructure, for the store and query-frontend components.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1168951987"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(thanos_memcached_operation_failures_total{environment="{{ $labels.environment }}",job="memcached-thanos-qfe-labels-metrics",stage="{{ $labels.stage }}",type="monitoring"}[5m])
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "no"
      window: 6h
    expr: |
      (
        (
          gitlab_component_errors:ratio_6h{component="memcached_thanos_qfe_labels",monitor="global",type="monitoring"}
          > (6 * 0.001000)
        )
        and
        (
          gitlab_component_errors:ratio_30m{component="memcached_thanos_qfe_labels",monitor="global",type="monitoring"}
          > (6 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_6h{component="memcached_thanos_qfe_labels",monitor="global",type="monitoring"}) >= 1
      )
  - alert: MonitoringServiceMemcachedThanosQfeQueryRangeApdexSLOViolation
    for: 2m
    annotations:
      title: The memcached_thanos_qfe_query_range SLI of the monitoring service (`{{
        $labels.stage }}` stage) has an apdex violating SLO
      description: |
        Various memcached instances support our thanos infrastructure, for the store and query-frontend components.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2315412172"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (env,environment,tier,stage,le) (
            rate(thanos_memcached_operation_duration_seconds_bucket{environment="{{ $labels.environment }}",job="memcached-thanos-qfe-query-range-metrics",stage="{{ $labels.stage }}",type="monitoring"}[5m])
          )
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "no"
      window: 1h
    expr: |
      (
        (
          gitlab_component_apdex:ratio_1h{component="memcached_thanos_qfe_query_range",monitor="global",type="monitoring"}
          < (1 - 14.4 * 0.001000)
        )
        and
        (
          gitlab_component_apdex:ratio_5m{component="memcached_thanos_qfe_query_range",monitor="global",type="monitoring"}
          < (1 - 14.4 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_1h{component="memcached_thanos_qfe_query_range",monitor="global",type="monitoring"}) >= 1
      )
  - alert: MonitoringServiceMemcachedThanosQfeQueryRangeApdexSLOViolation
    for: 2m
    annotations:
      title: The memcached_thanos_qfe_query_range SLI of the monitoring service (`{{
        $labels.stage }}` stage) has an apdex violating SLO
      description: |
        Various memcached instances support our thanos infrastructure, for the store and query-frontend components.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2315412172"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (env,environment,tier,stage,le) (
            rate(thanos_memcached_operation_duration_seconds_bucket{environment="{{ $labels.environment }}",job="memcached-thanos-qfe-query-range-metrics",stage="{{ $labels.stage }}",type="monitoring"}[5m])
          )
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "no"
      window: 6h
    expr: |
      (
        (
          gitlab_component_apdex:ratio_6h{component="memcached_thanos_qfe_query_range",monitor="global",type="monitoring"}
          < (1 - 6 * 0.001000)
        )
        and
        (
          gitlab_component_apdex:ratio_30m{component="memcached_thanos_qfe_query_range",monitor="global",type="monitoring"}
          < (1 - 6 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_6h{component="memcached_thanos_qfe_query_range",monitor="global",type="monitoring"}) >= 1
      )
  - alert: MonitoringServiceMemcachedThanosQfeQueryRangeErrorSLOViolation
    for: 2m
    annotations:
      title: The memcached_thanos_qfe_query_range SLI of the monitoring service (`{{
        $labels.stage }}` stage) has an error rate violating SLO
      description: |
        Various memcached instances support our thanos infrastructure, for the store and query-frontend components.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2342545966"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(thanos_memcached_operation_failures_total{environment="{{ $labels.environment }}",job="memcached-thanos-qfe-query-range-metrics",stage="{{ $labels.stage }}",type="monitoring"}[5m])
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "no"
      window: 1h
    expr: |
      (
        (
          gitlab_component_errors:ratio_1h{component="memcached_thanos_qfe_query_range",monitor="global",type="monitoring"}
          > (14.4 * 0.001000)
        )
        and
        (
          gitlab_component_errors:ratio_5m{component="memcached_thanos_qfe_query_range",monitor="global",type="monitoring"}
          > (14.4 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_1h{component="memcached_thanos_qfe_query_range",monitor="global",type="monitoring"}) >= 1
      )
  - alert: MonitoringServiceMemcachedThanosQfeQueryRangeErrorSLOViolation
    for: 2m
    annotations:
      title: The memcached_thanos_qfe_query_range SLI of the monitoring service (`{{
        $labels.stage }}` stage) has an error rate violating SLO
      description: |
        Various memcached instances support our thanos infrastructure, for the store and query-frontend components.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2342545966"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(thanos_memcached_operation_failures_total{environment="{{ $labels.environment }}",job="memcached-thanos-qfe-query-range-metrics",stage="{{ $labels.stage }}",type="monitoring"}[5m])
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "no"
      window: 6h
    expr: |
      (
        (
          gitlab_component_errors:ratio_6h{component="memcached_thanos_qfe_query_range",monitor="global",type="monitoring"}
          > (6 * 0.001000)
        )
        and
        (
          gitlab_component_errors:ratio_30m{component="memcached_thanos_qfe_query_range",monitor="global",type="monitoring"}
          > (6 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_6h{component="memcached_thanos_qfe_query_range",monitor="global",type="monitoring"}) >= 1
      )
  - alert: MonitoringServiceMemcachedThanosStoreBucketApdexSLOViolation
    for: 2m
    annotations:
      title: The memcached_thanos_store_bucket SLI of the monitoring service (`{{
        $labels.stage }}` stage) has an apdex violating SLO
      description: |
        Various memcached instances support our thanos infrastructure, for the store and query-frontend components.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2079888397"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (env,environment,tier,stage,le) (
            rate(thanos_memcached_operation_duration_seconds_bucket{environment="{{ $labels.environment }}",job="memcached-thanos-bucket-cache-metrics",stage="{{ $labels.stage }}",type="monitoring"}[5m])
          )
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "no"
      window: 1h
    expr: |
      (
        (
          gitlab_component_apdex:ratio_1h{component="memcached_thanos_store_bucket",monitor="global",type="monitoring"}
          < (1 - 14.4 * 0.001000)
        )
        and
        (
          gitlab_component_apdex:ratio_5m{component="memcached_thanos_store_bucket",monitor="global",type="monitoring"}
          < (1 - 14.4 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_1h{component="memcached_thanos_store_bucket",monitor="global",type="monitoring"}) >= 1
      )
  - alert: MonitoringServiceMemcachedThanosStoreBucketApdexSLOViolation
    for: 2m
    annotations:
      title: The memcached_thanos_store_bucket SLI of the monitoring service (`{{
        $labels.stage }}` stage) has an apdex violating SLO
      description: |
        Various memcached instances support our thanos infrastructure, for the store and query-frontend components.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2079888397"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (env,environment,tier,stage,le) (
            rate(thanos_memcached_operation_duration_seconds_bucket{environment="{{ $labels.environment }}",job="memcached-thanos-bucket-cache-metrics",stage="{{ $labels.stage }}",type="monitoring"}[5m])
          )
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "no"
      window: 6h
    expr: |
      (
        (
          gitlab_component_apdex:ratio_6h{component="memcached_thanos_store_bucket",monitor="global",type="monitoring"}
          < (1 - 6 * 0.001000)
        )
        and
        (
          gitlab_component_apdex:ratio_30m{component="memcached_thanos_store_bucket",monitor="global",type="monitoring"}
          < (1 - 6 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_6h{component="memcached_thanos_store_bucket",monitor="global",type="monitoring"}) >= 1
      )
  - alert: MonitoringServiceMemcachedThanosStoreBucketErrorSLOViolation
    for: 2m
    annotations:
      title: The memcached_thanos_store_bucket SLI of the monitoring service (`{{
        $labels.stage }}` stage) has an error rate violating SLO
      description: |
        Various memcached instances support our thanos infrastructure, for the store and query-frontend components.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3450526005"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(thanos_memcached_operation_failures_total{environment="{{ $labels.environment }}",job="memcached-thanos-bucket-cache-metrics",stage="{{ $labels.stage }}",type="monitoring"}[5m])
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "no"
      window: 1h
    expr: |
      (
        (
          gitlab_component_errors:ratio_1h{component="memcached_thanos_store_bucket",monitor="global",type="monitoring"}
          > (14.4 * 0.001000)
        )
        and
        (
          gitlab_component_errors:ratio_5m{component="memcached_thanos_store_bucket",monitor="global",type="monitoring"}
          > (14.4 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_1h{component="memcached_thanos_store_bucket",monitor="global",type="monitoring"}) >= 1
      )
  - alert: MonitoringServiceMemcachedThanosStoreBucketErrorSLOViolation
    for: 2m
    annotations:
      title: The memcached_thanos_store_bucket SLI of the monitoring service (`{{
        $labels.stage }}` stage) has an error rate violating SLO
      description: |
        Various memcached instances support our thanos infrastructure, for the store and query-frontend components.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3450526005"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(thanos_memcached_operation_failures_total{environment="{{ $labels.environment }}",job="memcached-thanos-bucket-cache-metrics",stage="{{ $labels.stage }}",type="monitoring"}[5m])
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "no"
      window: 6h
    expr: |
      (
        (
          gitlab_component_errors:ratio_6h{component="memcached_thanos_store_bucket",monitor="global",type="monitoring"}
          > (6 * 0.001000)
        )
        and
        (
          gitlab_component_errors:ratio_30m{component="memcached_thanos_store_bucket",monitor="global",type="monitoring"}
          > (6 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_6h{component="memcached_thanos_store_bucket",monitor="global",type="monitoring"}) >= 1
      )
  - alert: MonitoringServiceMemcachedThanosStoreIndexApdexSLOViolation
    for: 2m
    annotations:
      title: The memcached_thanos_store_index SLI of the monitoring service (`{{ $labels.stage
        }}` stage) has an apdex violating SLO
      description: |
        Various memcached instances support our thanos infrastructure, for the store and query-frontend components.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "383276883"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (env,environment,tier,stage,le) (
            rate(thanos_memcached_operation_duration_seconds_bucket{environment="{{ $labels.environment }}",job="memcached-thanos-index-cache-metrics",stage="{{ $labels.stage }}",type="monitoring"}[5m])
          )
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "no"
      window: 1h
    expr: |
      (
        (
          gitlab_component_apdex:ratio_1h{component="memcached_thanos_store_index",monitor="global",type="monitoring"}
          < (1 - 14.4 * 0.001000)
        )
        and
        (
          gitlab_component_apdex:ratio_5m{component="memcached_thanos_store_index",monitor="global",type="monitoring"}
          < (1 - 14.4 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_1h{component="memcached_thanos_store_index",monitor="global",type="monitoring"}) >= 1
      )
  - alert: MonitoringServiceMemcachedThanosStoreIndexApdexSLOViolation
    for: 2m
    annotations:
      title: The memcached_thanos_store_index SLI of the monitoring service (`{{ $labels.stage
        }}` stage) has an apdex violating SLO
      description: |
        Various memcached instances support our thanos infrastructure, for the store and query-frontend components.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "383276883"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (env,environment,tier,stage,le) (
            rate(thanos_memcached_operation_duration_seconds_bucket{environment="{{ $labels.environment }}",job="memcached-thanos-index-cache-metrics",stage="{{ $labels.stage }}",type="monitoring"}[5m])
          )
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "no"
      window: 6h
    expr: |
      (
        (
          gitlab_component_apdex:ratio_6h{component="memcached_thanos_store_index",monitor="global",type="monitoring"}
          < (1 - 6 * 0.001000)
        )
        and
        (
          gitlab_component_apdex:ratio_30m{component="memcached_thanos_store_index",monitor="global",type="monitoring"}
          < (1 - 6 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_6h{component="memcached_thanos_store_index",monitor="global",type="monitoring"}) >= 1
      )
  - alert: MonitoringServiceMemcachedThanosStoreIndexErrorSLOViolation
    for: 2m
    annotations:
      title: The memcached_thanos_store_index SLI of the monitoring service (`{{ $labels.stage
        }}` stage) has an error rate violating SLO
      description: |
        Various memcached instances support our thanos infrastructure, for the store and query-frontend components.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "4083956997"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(thanos_memcached_operation_failures_total{environment="{{ $labels.environment }}",job="memcached-thanos-index-cache-metrics",stage="{{ $labels.stage }}",type="monitoring"}[5m])
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "no"
      window: 1h
    expr: |
      (
        (
          gitlab_component_errors:ratio_1h{component="memcached_thanos_store_index",monitor="global",type="monitoring"}
          > (14.4 * 0.001000)
        )
        and
        (
          gitlab_component_errors:ratio_5m{component="memcached_thanos_store_index",monitor="global",type="monitoring"}
          > (14.4 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_1h{component="memcached_thanos_store_index",monitor="global",type="monitoring"}) >= 1
      )
  - alert: MonitoringServiceMemcachedThanosStoreIndexErrorSLOViolation
    for: 2m
    annotations:
      title: The memcached_thanos_store_index SLI of the monitoring service (`{{ $labels.stage
        }}` stage) has an error rate violating SLO
      description: |
        Various memcached instances support our thanos infrastructure, for the store and query-frontend components.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "4083956997"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(thanos_memcached_operation_failures_total{environment="{{ $labels.environment }}",job="memcached-thanos-index-cache-metrics",stage="{{ $labels.stage }}",type="monitoring"}[5m])
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "no"
      window: 6h
    expr: |
      (
        (
          gitlab_component_errors:ratio_6h{component="memcached_thanos_store_index",monitor="global",type="monitoring"}
          > (6 * 0.001000)
        )
        and
        (
          gitlab_component_errors:ratio_30m{component="memcached_thanos_store_index",monitor="global",type="monitoring"}
          > (6 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_6h{component="memcached_thanos_store_index",monitor="global",type="monitoring"}) >= 1
      )
  - alert: MonitoringServicePrometheusApdexSLOViolation
    for: 2m
    annotations:
      title: The prometheus SLI of the monitoring service (`{{ $labels.stage }}` stage)
        has an apdex violating SLO
      description: |
        This SLI monitors Prometheus instances via the HTTP interface. 5xx responses are considered errors.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "4146537329"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (env,environment,tier,stage,le) (
            rate(prometheus_http_request_duration_seconds_bucket{environment="{{ $labels.environment }}",job=~"prometheus.*",job!="prometheus-metamon",stage="{{ $labels.stage }}",type="monitoring"}[5m])
          )
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "yes"
      window: 1h
    expr: |
      (
        (
          gitlab_component_apdex:ratio_1h{component="prometheus",monitor="global",type="monitoring"}
          < (1 - 14.4 * 0.001000)
        )
        and
        (
          gitlab_component_apdex:ratio_5m{component="prometheus",monitor="global",type="monitoring"}
          < (1 - 14.4 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_1h{component="prometheus",monitor="global",type="monitoring"}) >= 1
      )
  - alert: MonitoringServicePrometheusApdexSLOViolation
    for: 2m
    annotations:
      title: The prometheus SLI of the monitoring service (`{{ $labels.stage }}` stage)
        has an apdex violating SLO
      description: |
        This SLI monitors Prometheus instances via the HTTP interface. 5xx responses are considered errors.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "4146537329"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (env,environment,tier,stage,le) (
            rate(prometheus_http_request_duration_seconds_bucket{environment="{{ $labels.environment }}",job=~"prometheus.*",job!="prometheus-metamon",stage="{{ $labels.stage }}",type="monitoring"}[5m])
          )
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "yes"
      window: 6h
    expr: |
      (
        (
          gitlab_component_apdex:ratio_6h{component="prometheus",monitor="global",type="monitoring"}
          < (1 - 6 * 0.001000)
        )
        and
        (
          gitlab_component_apdex:ratio_30m{component="prometheus",monitor="global",type="monitoring"}
          < (1 - 6 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_6h{component="prometheus",monitor="global",type="monitoring"}) >= 1
      )
  - alert: MonitoringServicePrometheusErrorSLOViolation
    for: 2m
    annotations:
      title: The prometheus SLI of the monitoring service (`{{ $labels.stage }}` stage)
        has an error rate violating SLO
      description: |
        This SLI monitors Prometheus instances via the HTTP interface. 5xx responses are considered errors.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "190969910"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(prometheus_http_requests_total{code=~"^5.*",environment="{{ $labels.environment }}",job=~"prometheus.*",job!="prometheus-metamon",stage="{{ $labels.stage }}",type="monitoring"}[5m])
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "yes"
      window: 1h
    expr: |
      (
        (
          gitlab_component_errors:ratio_1h{component="prometheus",monitor="global",type="monitoring"}
          > (14.4 * 0.001000)
        )
        and
        (
          gitlab_component_errors:ratio_5m{component="prometheus",monitor="global",type="monitoring"}
          > (14.4 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_1h{component="prometheus",monitor="global",type="monitoring"}) >= 1
      )
  - alert: MonitoringServicePrometheusErrorSLOViolation
    for: 2m
    annotations:
      title: The prometheus SLI of the monitoring service (`{{ $labels.stage }}` stage)
        has an error rate violating SLO
      description: |
        This SLI monitors Prometheus instances via the HTTP interface. 5xx responses are considered errors.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "190969910"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(prometheus_http_requests_total{code=~"^5.*",environment="{{ $labels.environment }}",job=~"prometheus.*",job!="prometheus-metamon",stage="{{ $labels.stage }}",type="monitoring"}[5m])
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "yes"
      window: 6h
    expr: |
      (
        (
          gitlab_component_errors:ratio_6h{component="prometheus",monitor="global",type="monitoring"}
          > (6 * 0.001000)
        )
        and
        (
          gitlab_component_errors:ratio_30m{component="prometheus",monitor="global",type="monitoring"}
          > (6 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_6h{component="prometheus",monitor="global",type="monitoring"}) >= 1
      )
  - alert: MonitoringServicePrometheusTrafficCessation
    for: 5m
    annotations:
      title: The prometheus SLI of the monitoring service (`{{ $labels.stage }}` stage)
        has not received any traffic in the past 30 minutes
      description: |
        This SLI monitors Prometheus instances via the HTTP interface. 5xx responses are considered errors.

        This alert signifies that the SLI is reporting a cessation of traffic, but the signal is not absent.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "443859513"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(prometheus_http_requests_total{environment="{{ $labels.environment }}",job=~"prometheus.*",job!="prometheus-metamon",stage="{{ $labels.stage }}",type="monitoring"}[5m])
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
    expr: |
      gitlab_component_ops:rate_30m{type="monitoring", component="prometheus", stage="main", monitor="global"} == 0
  - alert: MonitoringServicePrometheusTrafficAbsent
    for: 30m
    annotations:
      title: The prometheus SLI of the monitoring service (`{{ $labels.stage }}` stage)
        has not reported any traffic in the past 30 minutes
      description: |
        This SLI monitors Prometheus instances via the HTTP interface. 5xx responses are considered errors.

        This alert signifies that the SLI was previously reporting traffic, but is no longer - the signal is absent.

        This could be caused by a change to the metrics used in the SLI, or by the service not receiving traffic.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "443859513"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(prometheus_http_requests_total{environment="{{ $labels.environment }}",job=~"prometheus.*",job!="prometheus-metamon",stage="{{ $labels.stage }}",type="monitoring"}[5m])
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
    expr: |
      gitlab_component_ops:rate_5m{type="monitoring", component="prometheus", stage="main", monitor="global"} offset 1h
      unless
      gitlab_component_ops:rate_5m{type="monitoring", component="prometheus", stage="main", monitor="global"}
  - alert: MonitoringServicePrometheusAlertSenderErrorSLOViolation
    for: 2m
    annotations:
      title: The prometheus_alert_sender SLI of the monitoring service (`{{ $labels.stage
        }}` stage) has an error rate violating SLO
      description: |
        This SLI monitors all prometheus alert notifications that are generated by AlertManager. Alert delivery failure is considered a service-level failure.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3098809023"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(prometheus_notifications_errors_total{environment="{{ $labels.environment }}",job="prometheus",stage="{{ $labels.stage }}",type="monitoring"}[5m])
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "yes"
      window: 1h
    expr: |
      (
        (
          gitlab_component_errors:ratio_1h{component="prometheus_alert_sender",monitor="global",type="monitoring"}
          > (14.4 * 0.001000)
        )
        and
        (
          gitlab_component_errors:ratio_5m{component="prometheus_alert_sender",monitor="global",type="monitoring"}
          > (14.4 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_1h{component="prometheus_alert_sender",monitor="global",type="monitoring"}) >= 1
      )
  - alert: MonitoringServicePrometheusAlertSenderErrorSLOViolation
    for: 2m
    annotations:
      title: The prometheus_alert_sender SLI of the monitoring service (`{{ $labels.stage
        }}` stage) has an error rate violating SLO
      description: |
        This SLI monitors all prometheus alert notifications that are generated by AlertManager. Alert delivery failure is considered a service-level failure.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3098809023"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(prometheus_notifications_errors_total{environment="{{ $labels.environment }}",job="prometheus",stage="{{ $labels.stage }}",type="monitoring"}[5m])
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "yes"
      window: 6h
    expr: |
      (
        (
          gitlab_component_errors:ratio_6h{component="prometheus_alert_sender",monitor="global",type="monitoring"}
          > (6 * 0.001000)
        )
        and
        (
          gitlab_component_errors:ratio_30m{component="prometheus_alert_sender",monitor="global",type="monitoring"}
          > (6 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_6h{component="prometheus_alert_sender",monitor="global",type="monitoring"}) >= 1
      )
  - alert: MonitoringServicePrometheusAlertSenderTrafficCessation
    for: 5m
    annotations:
      title: The prometheus_alert_sender SLI of the monitoring service (`{{ $labels.stage
        }}` stage) has not received any traffic in the past 30 minutes
      description: |
        This SLI monitors all prometheus alert notifications that are generated by AlertManager. Alert delivery failure is considered a service-level failure.

        This alert signifies that the SLI is reporting a cessation of traffic, but the signal is not absent.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2970883772"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(prometheus_notifications_sent_total{environment="{{ $labels.environment }}",job="prometheus",stage="{{ $labels.stage }}",type="monitoring"}[5m])
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
    expr: |
      gitlab_component_ops:rate_30m{type="monitoring", component="prometheus_alert_sender", stage="main", monitor="global"} == 0
  - alert: MonitoringServicePrometheusAlertSenderTrafficAbsent
    for: 30m
    annotations:
      title: The prometheus_alert_sender SLI of the monitoring service (`{{ $labels.stage
        }}` stage) has not reported any traffic in the past 30 minutes
      description: |
        This SLI monitors all prometheus alert notifications that are generated by AlertManager. Alert delivery failure is considered a service-level failure.

        This alert signifies that the SLI was previously reporting traffic, but is no longer - the signal is absent.

        This could be caused by a change to the metrics used in the SLI, or by the service not receiving traffic.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2970883772"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(prometheus_notifications_sent_total{environment="{{ $labels.environment }}",job="prometheus",stage="{{ $labels.stage }}",type="monitoring"}[5m])
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
    expr: |
      gitlab_component_ops:rate_5m{type="monitoring", component="prometheus_alert_sender", stage="main", monitor="global"} offset 1h
      unless
      gitlab_component_ops:rate_5m{type="monitoring", component="prometheus_alert_sender", stage="main", monitor="global"}
  - alert: MonitoringServicePublicDashboardsThanosQueryApdexSLOViolation
    for: 2m
    annotations:
      title: The public_dashboards_thanos_query SLI of the monitoring service (`{{
        $labels.stage }}` stage) has an apdex violating SLO
      description: |
        Thanos query gathers the data needed to evaluate Prometheus queries from multiple underlying prometheus and thanos instances. This SLI monitors the Thanos query HTTP interface for GitLab's public Thanos instance, which is used by the public Grafana instance. 5xx responses are considered failures.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1482786452"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (env,environment,tier,stage,le) (
            rate(http_request_duration_seconds_bucket{environment="{{ $labels.environment }}",job="thanos",shard="public-dashboards",stage="{{ $labels.stage }}",type="monitoring"}[5m])
          )
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "no"
      window: 1h
    expr: |
      (
        (
          gitlab_component_apdex:ratio_1h{component="public_dashboards_thanos_query",monitor="global",type="monitoring"}
          < (1 - 14.4 * 0.001000)
        )
        and
        (
          gitlab_component_apdex:ratio_5m{component="public_dashboards_thanos_query",monitor="global",type="monitoring"}
          < (1 - 14.4 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_1h{component="public_dashboards_thanos_query",monitor="global",type="monitoring"}) >= 1
      )
  - alert: MonitoringServicePublicDashboardsThanosQueryApdexSLOViolation
    for: 2m
    annotations:
      title: The public_dashboards_thanos_query SLI of the monitoring service (`{{
        $labels.stage }}` stage) has an apdex violating SLO
      description: |
        Thanos query gathers the data needed to evaluate Prometheus queries from multiple underlying prometheus and thanos instances. This SLI monitors the Thanos query HTTP interface for GitLab's public Thanos instance, which is used by the public Grafana instance. 5xx responses are considered failures.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1482786452"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (env,environment,tier,stage,le) (
            rate(http_request_duration_seconds_bucket{environment="{{ $labels.environment }}",job="thanos",shard="public-dashboards",stage="{{ $labels.stage }}",type="monitoring"}[5m])
          )
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "no"
      window: 6h
    expr: |
      (
        (
          gitlab_component_apdex:ratio_6h{component="public_dashboards_thanos_query",monitor="global",type="monitoring"}
          < (1 - 6 * 0.001000)
        )
        and
        (
          gitlab_component_apdex:ratio_30m{component="public_dashboards_thanos_query",monitor="global",type="monitoring"}
          < (1 - 6 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_6h{component="public_dashboards_thanos_query",monitor="global",type="monitoring"}) >= 1
      )
  - alert: MonitoringServicePublicDashboardsThanosQueryErrorSLOViolation
    for: 2m
    annotations:
      title: The public_dashboards_thanos_query SLI of the monitoring service (`{{
        $labels.stage }}` stage) has an error rate violating SLO
      description: |
        Thanos query gathers the data needed to evaluate Prometheus queries from multiple underlying prometheus and thanos instances. This SLI monitors the Thanos query HTTP interface for GitLab's public Thanos instance, which is used by the public Grafana instance. 5xx responses are considered failures.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2417201990"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          sli_aggregations:http_requests_total_rate5m{code=~"^5.*",environment="{{ $labels.environment }}",job="thanos",shard="public-dashboards",stage="{{ $labels.stage }}",type="monitoring"}
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "no"
      window: 1h
    expr: |
      (
        (
          gitlab_component_errors:ratio_1h{component="public_dashboards_thanos_query",monitor="global",type="monitoring"}
          > (14.4 * 0.001000)
        )
        and
        (
          gitlab_component_errors:ratio_5m{component="public_dashboards_thanos_query",monitor="global",type="monitoring"}
          > (14.4 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_1h{component="public_dashboards_thanos_query",monitor="global",type="monitoring"}) >= 1
      )
  - alert: MonitoringServicePublicDashboardsThanosQueryErrorSLOViolation
    for: 2m
    annotations:
      title: The public_dashboards_thanos_query SLI of the monitoring service (`{{
        $labels.stage }}` stage) has an error rate violating SLO
      description: |
        Thanos query gathers the data needed to evaluate Prometheus queries from multiple underlying prometheus and thanos instances. This SLI monitors the Thanos query HTTP interface for GitLab's public Thanos instance, which is used by the public Grafana instance. 5xx responses are considered failures.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2417201990"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          sli_aggregations:http_requests_total_rate5m{code=~"^5.*",environment="{{ $labels.environment }}",job="thanos",shard="public-dashboards",stage="{{ $labels.stage }}",type="monitoring"}
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "no"
      window: 6h
    expr: |
      (
        (
          gitlab_component_errors:ratio_6h{component="public_dashboards_thanos_query",monitor="global",type="monitoring"}
          > (6 * 0.001000)
        )
        and
        (
          gitlab_component_errors:ratio_30m{component="public_dashboards_thanos_query",monitor="global",type="monitoring"}
          > (6 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_6h{component="public_dashboards_thanos_query",monitor="global",type="monitoring"}) >= 1
      )
  - alert: MonitoringServicePublicGrafanaGoogleLbErrorSLOViolation
    for: 2m
    annotations:
      title: The public_grafana_google_lb SLI of the monitoring service (`{{ $labels.stage
        }}` stage) has an error rate violating SLO
      description: |
        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "327759237"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(stackdriver_https_lb_rule_loadbalancing_googleapis_com_https_request_count{environment="{{ $labels.environment }}",project_id="gitlab-ops",response_code_class="500",stage="{{ $labels.stage }}",target_proxy_name="ops-dashboards-com"}[5m])
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "no"
      window: 1h
    expr: |
      (
        (
          gitlab_component_errors:ratio_1h{component="public_grafana_google_lb",monitor="global",type="monitoring"}
          > (14.4 * 0.001000)
        )
        and
        (
          gitlab_component_errors:ratio_5m{component="public_grafana_google_lb",monitor="global",type="monitoring"}
          > (14.4 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_1h{component="public_grafana_google_lb",monitor="global",type="monitoring"}) >= 1
      )
  - alert: MonitoringServicePublicGrafanaGoogleLbErrorSLOViolation
    for: 2m
    annotations:
      title: The public_grafana_google_lb SLI of the monitoring service (`{{ $labels.stage
        }}` stage) has an error rate violating SLO
      description: |
        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "327759237"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(stackdriver_https_lb_rule_loadbalancing_googleapis_com_https_request_count{environment="{{ $labels.environment }}",project_id="gitlab-ops",response_code_class="500",stage="{{ $labels.stage }}",target_proxy_name="ops-dashboards-com"}[5m])
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "no"
      window: 6h
    expr: |
      (
        (
          gitlab_component_errors:ratio_6h{component="public_grafana_google_lb",monitor="global",type="monitoring"}
          > (6 * 0.001000)
        )
        and
        (
          gitlab_component_errors:ratio_30m{component="public_grafana_google_lb",monitor="global",type="monitoring"}
          > (6 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_6h{component="public_grafana_google_lb",monitor="global",type="monitoring"}) >= 1
      )
  - alert: MonitoringServiceRuleEvaluationErrorSLOViolation
    for: 2m
    annotations:
      title: The rule_evaluation SLI of the monitoring service (`{{ $labels.stage
        }}` stage) has an error rate violating SLO
      description: |
        This SLI monitors Prometheus recording rule evaluations. Recording rule evalution failures are considered to be service failures.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3727131931"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(prometheus_rule_evaluation_failures_total{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="monitoring"}[5m])
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "yes"
      window: 1h
    expr: |
      (
        (
          gitlab_component_errors:ratio_1h{component="rule_evaluation",monitor="global",type="monitoring"}
          > (14.4 * 0.001000)
        )
        and
        (
          gitlab_component_errors:ratio_5m{component="rule_evaluation",monitor="global",type="monitoring"}
          > (14.4 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_1h{component="rule_evaluation",monitor="global",type="monitoring"}) >= 1
      )
  - alert: MonitoringServiceRuleEvaluationErrorSLOViolation
    for: 2m
    annotations:
      title: The rule_evaluation SLI of the monitoring service (`{{ $labels.stage
        }}` stage) has an error rate violating SLO
      description: |
        This SLI monitors Prometheus recording rule evaluations. Recording rule evalution failures are considered to be service failures.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3727131931"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(prometheus_rule_evaluation_failures_total{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="monitoring"}[5m])
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "yes"
      window: 6h
    expr: |
      (
        (
          gitlab_component_errors:ratio_6h{component="rule_evaluation",monitor="global",type="monitoring"}
          > (6 * 0.001000)
        )
        and
        (
          gitlab_component_errors:ratio_30m{component="rule_evaluation",monitor="global",type="monitoring"}
          > (6 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_6h{component="rule_evaluation",monitor="global",type="monitoring"}) >= 1
      )
  - alert: MonitoringServiceRuleEvaluationTrafficCessation
    for: 5m
    annotations:
      title: The rule_evaluation SLI of the monitoring service (`{{ $labels.stage
        }}` stage) has not received any traffic in the past 30 minutes
      description: |
        This SLI monitors Prometheus recording rule evaluations. Recording rule evalution failures are considered to be service failures.

        This alert signifies that the SLI is reporting a cessation of traffic, but the signal is not absent.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2621394178"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(prometheus_rule_evaluations_total{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="monitoring"}[5m])
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
    expr: |
      gitlab_component_ops:rate_30m{type="monitoring", component="rule_evaluation", stage="main", monitor="global"} == 0
  - alert: MonitoringServiceRuleEvaluationTrafficAbsent
    for: 30m
    annotations:
      title: The rule_evaluation SLI of the monitoring service (`{{ $labels.stage
        }}` stage) has not reported any traffic in the past 30 minutes
      description: |
        This SLI monitors Prometheus recording rule evaluations. Recording rule evalution failures are considered to be service failures.

        This alert signifies that the SLI was previously reporting traffic, but is no longer - the signal is absent.

        This could be caused by a change to the metrics used in the SLI, or by the service not receiving traffic.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2621394178"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(prometheus_rule_evaluations_total{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="monitoring"}[5m])
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
    expr: |
      gitlab_component_ops:rate_5m{type="monitoring", component="rule_evaluation", stage="main", monitor="global"} offset 1h
      unless
      gitlab_component_ops:rate_5m{type="monitoring", component="rule_evaluation", stage="main", monitor="global"}
  - alert: MonitoringServiceThanosCompactorErrorSLOViolation
    for: 2m
    annotations:
      title: The thanos_compactor SLI of the monitoring service (`{{ $labels.stage
        }}` stage) has an error rate violating SLO
      description: |
        Thanos compactor is responsible for compaction of Prometheus series data into blocks, which are stored in GCS buckets. It also handles downsampling. This SLI monitors compaction operations and compaction failures.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3147757590"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(thanos_compact_group_compactions_failures_total{environment="{{ $labels.environment }}",job="thanos",stage="{{ $labels.stage }}",type="monitoring"}[5m])
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "no"
      window: 1h
    expr: |
      (
        (
          gitlab_component_errors:ratio_1h{component="thanos_compactor",monitor="global",type="monitoring"}
          > (14.4 * 0.001000)
        )
        and
        (
          gitlab_component_errors:ratio_5m{component="thanos_compactor",monitor="global",type="monitoring"}
          > (14.4 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_1h{component="thanos_compactor",monitor="global",type="monitoring"}) >= 1
      )
  - alert: MonitoringServiceThanosCompactorErrorSLOViolation
    for: 2m
    annotations:
      title: The thanos_compactor SLI of the monitoring service (`{{ $labels.stage
        }}` stage) has an error rate violating SLO
      description: |
        Thanos compactor is responsible for compaction of Prometheus series data into blocks, which are stored in GCS buckets. It also handles downsampling. This SLI monitors compaction operations and compaction failures.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3147757590"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(thanos_compact_group_compactions_failures_total{environment="{{ $labels.environment }}",job="thanos",stage="{{ $labels.stage }}",type="monitoring"}[5m])
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "no"
      window: 6h
    expr: |
      (
        (
          gitlab_component_errors:ratio_6h{component="thanos_compactor",monitor="global",type="monitoring"}
          > (6 * 0.001000)
        )
        and
        (
          gitlab_component_errors:ratio_30m{component="thanos_compactor",monitor="global",type="monitoring"}
          > (6 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_6h{component="thanos_compactor",monitor="global",type="monitoring"}) >= 1
      )
  - alert: MonitoringServiceThanosQueryApdexSLOViolation
    for: 2m
    annotations:
      title: The thanos_query SLI of the monitoring service (`{{ $labels.stage }}`
        stage) has an apdex violating SLO
      description: |
        Thanos query gathers the data needed to evaluate Prometheus queries from multiple underlying prometheus and thanos instances. This SLI monitors the Thanos query HTTP interface. 5xx responses are considered failures.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1281064991"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (env,environment,tier,stage,le) (
            rate(http_request_duration_seconds_bucket{environment="{{ $labels.environment }}",job="thanos-query",shard="default",stage="{{ $labels.stage }}",type="monitoring"}[5m])
          )
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "no"
      window: 1h
    expr: |
      (
        (
          gitlab_component_apdex:ratio_1h{component="thanos_query",monitor="global",type="monitoring"}
          < (1 - 14.4 * 0.050000)
        )
        and
        (
          gitlab_component_apdex:ratio_5m{component="thanos_query",monitor="global",type="monitoring"}
          < (1 - 14.4 * 0.050000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_1h{component="thanos_query",monitor="global",type="monitoring"}) >= 1
      )
  - alert: MonitoringServiceThanosQueryApdexSLOViolation
    for: 2m
    annotations:
      title: The thanos_query SLI of the monitoring service (`{{ $labels.stage }}`
        stage) has an apdex violating SLO
      description: |
        Thanos query gathers the data needed to evaluate Prometheus queries from multiple underlying prometheus and thanos instances. This SLI monitors the Thanos query HTTP interface. 5xx responses are considered failures.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1281064991"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (env,environment,tier,stage,le) (
            rate(http_request_duration_seconds_bucket{environment="{{ $labels.environment }}",job="thanos-query",shard="default",stage="{{ $labels.stage }}",type="monitoring"}[5m])
          )
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "no"
      window: 6h
    expr: |
      (
        (
          gitlab_component_apdex:ratio_6h{component="thanos_query",monitor="global",type="monitoring"}
          < (1 - 6 * 0.050000)
        )
        and
        (
          gitlab_component_apdex:ratio_30m{component="thanos_query",monitor="global",type="monitoring"}
          < (1 - 6 * 0.050000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_6h{component="thanos_query",monitor="global",type="monitoring"}) >= 1
      )
  - alert: MonitoringServiceThanosQueryErrorSLOViolation
    for: 2m
    annotations:
      title: The thanos_query SLI of the monitoring service (`{{ $labels.stage }}`
        stage) has an error rate violating SLO
      description: |
        Thanos query gathers the data needed to evaluate Prometheus queries from multiple underlying prometheus and thanos instances. This SLI monitors the Thanos query HTTP interface. 5xx responses are considered failures.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "4004985658"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          sli_aggregations:http_requests_total_rate5m{code=~"^5.*",environment="{{ $labels.environment }}",job="thanos-query",shard="default",stage="{{ $labels.stage }}",type="monitoring"}
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "no"
      window: 1h
    expr: |
      (
        (
          gitlab_component_errors:ratio_1h{component="thanos_query",monitor="global",type="monitoring"}
          > (14.4 * 0.050000)
        )
        and
        (
          gitlab_component_errors:ratio_5m{component="thanos_query",monitor="global",type="monitoring"}
          > (14.4 * 0.050000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_1h{component="thanos_query",monitor="global",type="monitoring"}) >= 1
      )
  - alert: MonitoringServiceThanosQueryErrorSLOViolation
    for: 2m
    annotations:
      title: The thanos_query SLI of the monitoring service (`{{ $labels.stage }}`
        stage) has an error rate violating SLO
      description: |
        Thanos query gathers the data needed to evaluate Prometheus queries from multiple underlying prometheus and thanos instances. This SLI monitors the Thanos query HTTP interface. 5xx responses are considered failures.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "4004985658"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          sli_aggregations:http_requests_total_rate5m{code=~"^5.*",environment="{{ $labels.environment }}",job="thanos-query",shard="default",stage="{{ $labels.stage }}",type="monitoring"}
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "no"
      window: 6h
    expr: |
      (
        (
          gitlab_component_errors:ratio_6h{component="thanos_query",monitor="global",type="monitoring"}
          > (6 * 0.050000)
        )
        and
        (
          gitlab_component_errors:ratio_30m{component="thanos_query",monitor="global",type="monitoring"}
          > (6 * 0.050000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_6h{component="thanos_query",monitor="global",type="monitoring"}) >= 1
      )
  - alert: MonitoringServiceThanosQueryTrafficCessation
    for: 5m
    annotations:
      title: The thanos_query SLI of the monitoring service (`{{ $labels.stage }}`
        stage) has not received any traffic in the past 30 minutes
      description: |
        Thanos query gathers the data needed to evaluate Prometheus queries from multiple underlying prometheus and thanos instances. This SLI monitors the Thanos query HTTP interface. 5xx responses are considered failures.

        This alert signifies that the SLI is reporting a cessation of traffic, but the signal is not absent.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "431223751"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          sli_aggregations:http_requests_total_rate5m{environment="{{ $labels.environment }}",job="thanos-query",shard="default",stage="{{ $labels.stage }}",type="monitoring"}
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "no"
    expr: |
      gitlab_component_ops:rate_30m{type="monitoring", component="thanos_query", stage="main", monitor="global"} == 0
  - alert: MonitoringServiceThanosQueryTrafficAbsent
    for: 30m
    annotations:
      title: The thanos_query SLI of the monitoring service (`{{ $labels.stage }}`
        stage) has not reported any traffic in the past 30 minutes
      description: |
        Thanos query gathers the data needed to evaluate Prometheus queries from multiple underlying prometheus and thanos instances. This SLI monitors the Thanos query HTTP interface. 5xx responses are considered failures.

        This alert signifies that the SLI was previously reporting traffic, but is no longer - the signal is absent.

        This could be caused by a change to the metrics used in the SLI, or by the service not receiving traffic.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "431223751"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          sli_aggregations:http_requests_total_rate5m{environment="{{ $labels.environment }}",job="thanos-query",shard="default",stage="{{ $labels.stage }}",type="monitoring"}
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "no"
    expr: |
      gitlab_component_ops:rate_5m{type="monitoring", component="thanos_query", stage="main", monitor="global"} offset 1h
      unless
      gitlab_component_ops:rate_5m{type="monitoring", component="thanos_query", stage="main", monitor="global"}
  - alert: MonitoringServiceThanosQueryFrontendApdexSLOViolation
    for: 2m
    annotations:
      title: The thanos_query_frontend SLI of the monitoring service (`{{ $labels.stage
        }}` stage) has an apdex violating SLO
      description: |
        Thanos query gathers the data needed to evaluate Prometheus queries from multiple underlying prometheus and thanos instances. This SLI monitors the Thanos query HTTP interface. 5xx responses are considered failures.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3015468996"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (env,environment,tier,stage,le) (
            rate(http_request_duration_seconds_bucket{environment="{{ $labels.environment }}",job=~"thanos|thanos-query-frontend",shard="default",stage="{{ $labels.stage }}",type="monitoring"}[5m])
          )
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "no"
      window: 1h
    expr: |
      (
        (
          gitlab_component_apdex:ratio_1h{component="thanos_query_frontend",monitor="global",type="monitoring"}
          < (1 - 14.4 * 0.001000)
        )
        and
        (
          gitlab_component_apdex:ratio_5m{component="thanos_query_frontend",monitor="global",type="monitoring"}
          < (1 - 14.4 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_1h{component="thanos_query_frontend",monitor="global",type="monitoring"}) >= 1
      )
  - alert: MonitoringServiceThanosQueryFrontendApdexSLOViolation
    for: 2m
    annotations:
      title: The thanos_query_frontend SLI of the monitoring service (`{{ $labels.stage
        }}` stage) has an apdex violating SLO
      description: |
        Thanos query gathers the data needed to evaluate Prometheus queries from multiple underlying prometheus and thanos instances. This SLI monitors the Thanos query HTTP interface. 5xx responses are considered failures.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3015468996"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (env,environment,tier,stage,le) (
            rate(http_request_duration_seconds_bucket{environment="{{ $labels.environment }}",job=~"thanos|thanos-query-frontend",shard="default",stage="{{ $labels.stage }}",type="monitoring"}[5m])
          )
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "no"
      window: 6h
    expr: |
      (
        (
          gitlab_component_apdex:ratio_6h{component="thanos_query_frontend",monitor="global",type="monitoring"}
          < (1 - 6 * 0.001000)
        )
        and
        (
          gitlab_component_apdex:ratio_30m{component="thanos_query_frontend",monitor="global",type="monitoring"}
          < (1 - 6 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_6h{component="thanos_query_frontend",monitor="global",type="monitoring"}) >= 1
      )
  - alert: MonitoringServiceThanosQueryFrontendErrorSLOViolation
    for: 2m
    annotations:
      title: The thanos_query_frontend SLI of the monitoring service (`{{ $labels.stage
        }}` stage) has an error rate violating SLO
      description: |
        Thanos query gathers the data needed to evaluate Prometheus queries from multiple underlying prometheus and thanos instances. This SLI monitors the Thanos query HTTP interface. 5xx responses are considered failures.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1994646344"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          sli_aggregations:http_requests_total_rate5m{code=~"^5.*",environment="{{ $labels.environment }}",job=~"thanos|thanos-query-frontend",shard="default",stage="{{ $labels.stage }}",type="monitoring"}
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "no"
      window: 1h
    expr: |
      (
        (
          gitlab_component_errors:ratio_1h{component="thanos_query_frontend",monitor="global",type="monitoring"}
          > (14.4 * 0.001000)
        )
        and
        (
          gitlab_component_errors:ratio_5m{component="thanos_query_frontend",monitor="global",type="monitoring"}
          > (14.4 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_1h{component="thanos_query_frontend",monitor="global",type="monitoring"}) >= 1
      )
  - alert: MonitoringServiceThanosQueryFrontendErrorSLOViolation
    for: 2m
    annotations:
      title: The thanos_query_frontend SLI of the monitoring service (`{{ $labels.stage
        }}` stage) has an error rate violating SLO
      description: |
        Thanos query gathers the data needed to evaluate Prometheus queries from multiple underlying prometheus and thanos instances. This SLI monitors the Thanos query HTTP interface. 5xx responses are considered failures.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1994646344"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          sli_aggregations:http_requests_total_rate5m{code=~"^5.*",environment="{{ $labels.environment }}",job=~"thanos|thanos-query-frontend",shard="default",stage="{{ $labels.stage }}",type="monitoring"}
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "no"
      window: 6h
    expr: |
      (
        (
          gitlab_component_errors:ratio_6h{component="thanos_query_frontend",monitor="global",type="monitoring"}
          > (6 * 0.001000)
        )
        and
        (
          gitlab_component_errors:ratio_30m{component="thanos_query_frontend",monitor="global",type="monitoring"}
          > (6 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_6h{component="thanos_query_frontend",monitor="global",type="monitoring"}) >= 1
      )
  - alert: MonitoringServiceThanosQueryFrontendTrafficCessation
    for: 5m
    annotations:
      title: The thanos_query_frontend SLI of the monitoring service (`{{ $labels.stage
        }}` stage) has not received any traffic in the past 30 minutes
      description: |
        Thanos query gathers the data needed to evaluate Prometheus queries from multiple underlying prometheus and thanos instances. This SLI monitors the Thanos query HTTP interface. 5xx responses are considered failures.

        This alert signifies that the SLI is reporting a cessation of traffic, but the signal is not absent.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1877464727"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          sli_aggregations:http_requests_total_rate5m{environment="{{ $labels.environment }}",job=~"thanos|thanos-query-frontend",shard="default",stage="{{ $labels.stage }}",type="monitoring"}
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "no"
    expr: |
      gitlab_component_ops:rate_30m{type="monitoring", component="thanos_query_frontend", stage="main", monitor="global"} == 0
  - alert: MonitoringServiceThanosQueryFrontendTrafficAbsent
    for: 30m
    annotations:
      title: The thanos_query_frontend SLI of the monitoring service (`{{ $labels.stage
        }}` stage) has not reported any traffic in the past 30 minutes
      description: |
        Thanos query gathers the data needed to evaluate Prometheus queries from multiple underlying prometheus and thanos instances. This SLI monitors the Thanos query HTTP interface. 5xx responses are considered failures.

        This alert signifies that the SLI was previously reporting traffic, but is no longer - the signal is absent.

        This could be caused by a change to the metrics used in the SLI, or by the service not receiving traffic.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1877464727"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          sli_aggregations:http_requests_total_rate5m{environment="{{ $labels.environment }}",job=~"thanos|thanos-query-frontend",shard="default",stage="{{ $labels.stage }}",type="monitoring"}
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "no"
    expr: |
      gitlab_component_ops:rate_5m{type="monitoring", component="thanos_query_frontend", stage="main", monitor="global"} offset 1h
      unless
      gitlab_component_ops:rate_5m{type="monitoring", component="thanos_query_frontend", stage="main", monitor="global"}
  - alert: MonitoringServiceThanosRuleAlertSenderErrorSLOViolation
    for: 2m
    annotations:
      title: The thanos_rule_alert_sender SLI of the monitoring service (`{{ $labels.stage
        }}` stage) has an error rate violating SLO
      description: |
        This SLI monitors alerts generated by Thanos Ruler. Alert delivery failure is considered a service-level failure.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3718148749"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(thanos_alert_sender_errors_total{environment="{{ $labels.environment }}",job="thanos",stage="{{ $labels.stage }}",type="monitoring"}[5m])
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "yes"
      window: 1h
    expr: |
      (
        (
          gitlab_component_errors:ratio_1h{component="thanos_rule_alert_sender",monitor="global",type="monitoring"}
          > (14.4 * 0.001000)
        )
        and
        (
          gitlab_component_errors:ratio_5m{component="thanos_rule_alert_sender",monitor="global",type="monitoring"}
          > (14.4 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_1h{component="thanos_rule_alert_sender",monitor="global",type="monitoring"}) >= 1
      )
  - alert: MonitoringServiceThanosRuleAlertSenderErrorSLOViolation
    for: 2m
    annotations:
      title: The thanos_rule_alert_sender SLI of the monitoring service (`{{ $labels.stage
        }}` stage) has an error rate violating SLO
      description: |
        This SLI monitors alerts generated by Thanos Ruler. Alert delivery failure is considered a service-level failure.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3718148749"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(thanos_alert_sender_errors_total{environment="{{ $labels.environment }}",job="thanos",stage="{{ $labels.stage }}",type="monitoring"}[5m])
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "yes"
      window: 6h
    expr: |
      (
        (
          gitlab_component_errors:ratio_6h{component="thanos_rule_alert_sender",monitor="global",type="monitoring"}
          > (6 * 0.001000)
        )
        and
        (
          gitlab_component_errors:ratio_30m{component="thanos_rule_alert_sender",monitor="global",type="monitoring"}
          > (6 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_6h{component="thanos_rule_alert_sender",monitor="global",type="monitoring"}) >= 1
      )
  - alert: MonitoringServiceThanosRuleAlertSenderTrafficCessation
    for: 5m
    annotations:
      title: The thanos_rule_alert_sender SLI of the monitoring service (`{{ $labels.stage
        }}` stage) has not received any traffic in the past 30 minutes
      description: |
        This SLI monitors alerts generated by Thanos Ruler. Alert delivery failure is considered a service-level failure.

        This alert signifies that the SLI is reporting a cessation of traffic, but the signal is not absent.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2608229891"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(thanos_alert_sender_alerts_sent_total{environment="{{ $labels.environment }}",job="thanos",stage="{{ $labels.stage }}",type="monitoring"}[5m])
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
    expr: |
      gitlab_component_ops:rate_30m{type="monitoring", component="thanos_rule_alert_sender", stage="main", monitor="global"} == 0
  - alert: MonitoringServiceThanosRuleAlertSenderTrafficAbsent
    for: 30m
    annotations:
      title: The thanos_rule_alert_sender SLI of the monitoring service (`{{ $labels.stage
        }}` stage) has not reported any traffic in the past 30 minutes
      description: |
        This SLI monitors alerts generated by Thanos Ruler. Alert delivery failure is considered a service-level failure.

        This alert signifies that the SLI was previously reporting traffic, but is no longer - the signal is absent.

        This could be caused by a change to the metrics used in the SLI, or by the service not receiving traffic.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2608229891"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(thanos_alert_sender_alerts_sent_total{environment="{{ $labels.environment }}",job="thanos",stage="{{ $labels.stage }}",type="monitoring"}[5m])
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
    expr: |
      gitlab_component_ops:rate_5m{type="monitoring", component="thanos_rule_alert_sender", stage="main", monitor="global"} offset 1h
      unless
      gitlab_component_ops:rate_5m{type="monitoring", component="thanos_rule_alert_sender", stage="main", monitor="global"}
  - alert: MonitoringServiceThanosStoreApdexSLOViolation
    for: 2m
    annotations:
      title: The thanos_store SLI of the monitoring service (`{{ $labels.stage }}`
        stage) has an apdex violating SLO
      description: |
        Thanos store will respond to Thanos Query (and other) requests for historical data. This historical data is kept in GCS buckets. This SLI monitors the Thanos StoreAPI GRPC endpoint. GRPC error responses are considered to be service-level failures.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3649067073"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (env,environment,tier,stage,le) (
            rate(grpc_server_handling_seconds_bucket{environment="{{ $labels.environment }}",grpc_service="thanos.Store",grpc_type="unary",job=~"thanos|thanos-store(-[0-9]+)?",stage="{{ $labels.stage }}",type="monitoring"}[5m])
          )
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "no"
      window: 1h
    expr: |
      (
        (
          gitlab_component_apdex:ratio_1h{component="thanos_store",monitor="global",type="monitoring"}
          < (1 - 14.4 * 0.050000)
        )
        and
        (
          gitlab_component_apdex:ratio_5m{component="thanos_store",monitor="global",type="monitoring"}
          < (1 - 14.4 * 0.050000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_1h{component="thanos_store",monitor="global",type="monitoring"}) >= 1
      )
  - alert: MonitoringServiceThanosStoreApdexSLOViolation
    for: 2m
    annotations:
      title: The thanos_store SLI of the monitoring service (`{{ $labels.stage }}`
        stage) has an apdex violating SLO
      description: |
        Thanos store will respond to Thanos Query (and other) requests for historical data. This historical data is kept in GCS buckets. This SLI monitors the Thanos StoreAPI GRPC endpoint. GRPC error responses are considered to be service-level failures.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3649067073"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (env,environment,tier,stage,le) (
            rate(grpc_server_handling_seconds_bucket{environment="{{ $labels.environment }}",grpc_service="thanos.Store",grpc_type="unary",job=~"thanos|thanos-store(-[0-9]+)?",stage="{{ $labels.stage }}",type="monitoring"}[5m])
          )
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "no"
      window: 6h
    expr: |
      (
        (
          gitlab_component_apdex:ratio_6h{component="thanos_store",monitor="global",type="monitoring"}
          < (1 - 6 * 0.050000)
        )
        and
        (
          gitlab_component_apdex:ratio_30m{component="thanos_store",monitor="global",type="monitoring"}
          < (1 - 6 * 0.050000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_6h{component="thanos_store",monitor="global",type="monitoring"}) >= 1
      )
  - alert: MonitoringServiceThanosStoreErrorSLOViolation
    for: 2m
    annotations:
      title: The thanos_store SLI of the monitoring service (`{{ $labels.stage }}`
        stage) has an error rate violating SLO
      description: |
        Thanos store will respond to Thanos Query (and other) requests for historical data. This historical data is kept in GCS buckets. This SLI monitors the Thanos StoreAPI GRPC endpoint. GRPC error responses are considered to be service-level failures.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "279093558"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(grpc_server_handled_total{environment="{{ $labels.environment }}",grpc_code!="OK",grpc_service="thanos.Store",grpc_type="unary",job=~"thanos|thanos-store(-[0-9]+)?",stage="{{ $labels.stage }}",type="monitoring"}[5m])
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "no"
      window: 1h
    expr: |
      (
        (
          gitlab_component_errors:ratio_1h{component="thanos_store",monitor="global",type="monitoring"}
          > (14.4 * 0.050000)
        )
        and
        (
          gitlab_component_errors:ratio_5m{component="thanos_store",monitor="global",type="monitoring"}
          > (14.4 * 0.050000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_1h{component="thanos_store",monitor="global",type="monitoring"}) >= 1
      )
  - alert: MonitoringServiceThanosStoreErrorSLOViolation
    for: 2m
    annotations:
      title: The thanos_store SLI of the monitoring service (`{{ $labels.stage }}`
        stage) has an error rate violating SLO
      description: |
        Thanos store will respond to Thanos Query (and other) requests for historical data. This historical data is kept in GCS buckets. This SLI monitors the Thanos StoreAPI GRPC endpoint. GRPC error responses are considered to be service-level failures.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "279093558"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(grpc_server_handled_total{environment="{{ $labels.environment }}",grpc_code!="OK",grpc_service="thanos.Store",grpc_type="unary",job=~"thanos|thanos-store(-[0-9]+)?",stage="{{ $labels.stage }}",type="monitoring"}[5m])
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "no"
      window: 6h
    expr: |
      (
        (
          gitlab_component_errors:ratio_6h{component="thanos_store",monitor="global",type="monitoring"}
          > (6 * 0.050000)
        )
        and
        (
          gitlab_component_errors:ratio_30m{component="thanos_store",monitor="global",type="monitoring"}
          > (6 * 0.050000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_6h{component="thanos_store",monitor="global",type="monitoring"}) >= 1
      )
  - alert: MonitoringServiceThanosStoreTrafficCessation
    for: 5m
    annotations:
      title: The thanos_store SLI of the monitoring service (`{{ $labels.stage }}`
        stage) has not received any traffic in the past 30 minutes
      description: |
        Thanos store will respond to Thanos Query (and other) requests for historical data. This historical data is kept in GCS buckets. This SLI monitors the Thanos StoreAPI GRPC endpoint. GRPC error responses are considered to be service-level failures.

        This alert signifies that the SLI is reporting a cessation of traffic, but the signal is not absent.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1091689110"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(grpc_server_handled_total{environment="{{ $labels.environment }}",grpc_service="thanos.Store",grpc_type="unary",job=~"thanos|thanos-store(-[0-9]+)?",stage="{{ $labels.stage }}",type="monitoring"}[5m])
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "no"
    expr: |
      gitlab_component_ops:rate_30m{type="monitoring", component="thanos_store", stage="main", monitor="global"} == 0
  - alert: MonitoringServiceThanosStoreTrafficAbsent
    for: 30m
    annotations:
      title: The thanos_store SLI of the monitoring service (`{{ $labels.stage }}`
        stage) has not reported any traffic in the past 30 minutes
      description: |
        Thanos store will respond to Thanos Query (and other) requests for historical data. This historical data is kept in GCS buckets. This SLI monitors the Thanos StoreAPI GRPC endpoint. GRPC error responses are considered to be service-level failures.

        This alert signifies that the SLI was previously reporting traffic, but is no longer - the signal is absent.

        This could be caused by a change to the metrics used in the SLI, or by the service not receiving traffic.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1091689110"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(grpc_server_handled_total{environment="{{ $labels.environment }}",grpc_service="thanos.Store",grpc_type="unary",job=~"thanos|thanos-store(-[0-9]+)?",stage="{{ $labels.stage }}",type="monitoring"}[5m])
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "no"
    expr: |
      gitlab_component_ops:rate_5m{type="monitoring", component="thanos_store", stage="main", monitor="global"} offset 1h
      unless
      gitlab_component_ops:rate_5m{type="monitoring", component="thanos_store", stage="main", monitor="global"}
  - alert: MonitoringServiceTricksterApdexSLOViolation
    for: 2m
    annotations:
      title: The trickster SLI of the monitoring service (`{{ $labels.stage }}` stage)
        has an apdex violating SLO
      description: |
        This SLI monitors the Trickster HTTP interface.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "135884220"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (env,environment,tier,stage,le) (
            rate(trickster_frontend_requests_duration_seconds_bucket{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}"}[5m])
          )
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "no"
      window: 1h
    expr: |
      (
        (
          gitlab_component_apdex:ratio_1h{component="trickster",monitor="global",type="monitoring"}
          < (1 - 14.4 * 0.001000)
        )
        and
        (
          gitlab_component_apdex:ratio_5m{component="trickster",monitor="global",type="monitoring"}
          < (1 - 14.4 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_1h{component="trickster",monitor="global",type="monitoring"}) >= 1
      )
  - alert: MonitoringServiceTricksterApdexSLOViolation
    for: 2m
    annotations:
      title: The trickster SLI of the monitoring service (`{{ $labels.stage }}` stage)
        has an apdex violating SLO
      description: |
        This SLI monitors the Trickster HTTP interface.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "135884220"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (env,environment,tier,stage,le) (
            rate(trickster_frontend_requests_duration_seconds_bucket{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}"}[5m])
          )
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "no"
      window: 6h
    expr: |
      (
        (
          gitlab_component_apdex:ratio_6h{component="trickster",monitor="global",type="monitoring"}
          < (1 - 6 * 0.001000)
        )
        and
        (
          gitlab_component_apdex:ratio_30m{component="trickster",monitor="global",type="monitoring"}
          < (1 - 6 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_6h{component="trickster",monitor="global",type="monitoring"}) >= 1
      )
  - alert: MonitoringServiceTricksterErrorSLOViolation
    for: 2m
    annotations:
      title: The trickster SLI of the monitoring service (`{{ $labels.stage }}` stage)
        has an error rate violating SLO
      description: |
        This SLI monitors the Trickster HTTP interface.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3348294780"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(trickster_frontend_requests_total{environment="{{ $labels.environment }}",http_status=~"5.*",stage="{{ $labels.stage }}"}[5m])
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "no"
      window: 1h
    expr: |
      (
        (
          gitlab_component_errors:ratio_1h{component="trickster",monitor="global",type="monitoring"}
          > (14.4 * 0.001000)
        )
        and
        (
          gitlab_component_errors:ratio_5m{component="trickster",monitor="global",type="monitoring"}
          > (14.4 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_1h{component="trickster",monitor="global",type="monitoring"}) >= 1
      )
  - alert: MonitoringServiceTricksterErrorSLOViolation
    for: 2m
    annotations:
      title: The trickster SLI of the monitoring service (`{{ $labels.stage }}` stage)
        has an error rate violating SLO
      description: |
        This SLI monitors the Trickster HTTP interface.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3348294780"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(trickster_frontend_requests_total{environment="{{ $labels.environment }}",http_status=~"5.*",stage="{{ $labels.stage }}"}[5m])
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "no"
      window: 6h
    expr: |
      (
        (
          gitlab_component_errors:ratio_6h{component="trickster",monitor="global",type="monitoring"}
          > (6 * 0.001000)
        )
        and
        (
          gitlab_component_errors:ratio_30m{component="trickster",monitor="global",type="monitoring"}
          > (6 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_6h{component="trickster",monitor="global",type="monitoring"}) >= 1
      )
