# WARNING. DO NOT EDIT THIS FILE BY HAND. USE ./thanos-rules-jsonnet/service-component-alerts.jsonnet TO GENERATE IT
# YOUR CHANGES WILL BE OVERRIDDEN
groups:
- name: 'Service Component Alerts: thanos-staging'
  interval: 1m
  partial_response_strategy: warn
  rules:
  - alert: ThanosStagingServiceRuleEvaluationApdexSLOViolation
    for: 2m
    annotations:
      title: The rule_evaluation SLI of the thanos-staging service (`{{ $labels.stage
        }}` stage) has an apdex violating SLO
      description: |
        This SLI monitors Prometheus recording rule evaluations. Recording rule evalution failures are considered to be service failures. Warnings are also considered failures.

        Rule groups are evaluating recording rules in a group in sequence at an interval. If the recording of all rules in a groups exceeds the interval for the group, we could be missing data points in the group.

        If a group is slow often, we should split it up or improve query performance

        To see which rules are often not meeting their target. Look at the SLI-details. The `rule_group` label will contain information about the slow group.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: thanos-staging-main/thanos-staging-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/thanos-staging-main/thanos-staging-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2667244350"
      grafana_variables: environment,stage
      runbook: docs/thanos-staging/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      rules_domain: general
      severity: s3
      sli_type: apdex
      slo_alert: "yes"
      team: reliability_observability
      user_impacting: "no"
      window: 1h
    expr: |
      (
        (
          gitlab_component_apdex:ratio_1h{component="rule_evaluation",env!~"gprd|ops",monitor="global",type="thanos-staging"}
          < (1 - 14.4 * 0.001000)
        )
        and
        (
          gitlab_component_apdex:ratio_5m{component="rule_evaluation",env!~"gprd|ops",monitor="global",type="thanos-staging"}
          < (1 - 14.4 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_1h{component="rule_evaluation",env!~"gprd|ops",monitor="global",type="thanos-staging"}) >= 1
      )
  - alert: ThanosStagingServiceRuleEvaluationApdexSLOViolation
    for: 2m
    annotations:
      title: The rule_evaluation SLI of the thanos-staging service (`{{ $labels.stage
        }}` stage) has an apdex violating SLO
      description: |
        This SLI monitors Prometheus recording rule evaluations. Recording rule evalution failures are considered to be service failures. Warnings are also considered failures.

        Rule groups are evaluating recording rules in a group in sequence at an interval. If the recording of all rules in a groups exceeds the interval for the group, we could be missing data points in the group.

        If a group is slow often, we should split it up or improve query performance

        To see which rules are often not meeting their target. Look at the SLI-details. The `rule_group` label will contain information about the slow group.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: thanos-staging-main/thanos-staging-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/thanos-staging-main/thanos-staging-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2667244350"
      grafana_variables: environment,stage
      runbook: docs/thanos-staging/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      rules_domain: general
      severity: s3
      sli_type: apdex
      slo_alert: "yes"
      team: reliability_observability
      user_impacting: "no"
      window: 6h
    expr: |
      (
        (
          gitlab_component_apdex:ratio_6h{component="rule_evaluation",env!~"gprd|ops",monitor="global",type="thanos-staging"}
          < (1 - 6 * 0.001000)
        )
        and
        (
          gitlab_component_apdex:ratio_30m{component="rule_evaluation",env!~"gprd|ops",monitor="global",type="thanos-staging"}
          < (1 - 6 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_6h{component="rule_evaluation",env!~"gprd|ops",monitor="global",type="thanos-staging"}) >= 0.16667
      )
  - alert: ThanosStagingServiceRuleEvaluationErrorSLOViolation
    for: 2m
    annotations:
      title: The rule_evaluation SLI of the thanos-staging service (`{{ $labels.stage
        }}` stage) has an error rate violating SLO
      description: |
        This SLI monitors Prometheus recording rule evaluations. Recording rule evalution failures are considered to be service failures. Warnings are also considered failures.

        Rule groups are evaluating recording rules in a group in sequence at an interval. If the recording of all rules in a groups exceeds the interval for the group, we could be missing data points in the group.

        If a group is slow often, we should split it up or improve query performance

        To see which rules are often not meeting their target. Look at the SLI-details. The `rule_group` label will contain information about the slow group.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: thanos-staging-main/thanos-staging-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/thanos-staging-main/thanos-staging-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3727131931"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          label_replace(rate(prometheus_rule_evaluation_failures_total{environment="{{ $labels.environment }}",job="thanos-ruler",namespace="thanos-staging",stage="{{ $labels.stage }}",type="thanos"}[5m]), "_c", "0", "", "")
          or
          label_replace(rate(thanos_rule_evaluation_with_warnings_total{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}"}[5m]), "_c", "1", "", "")
        )
      runbook: docs/thanos-staging/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      rules_domain: general
      severity: s3
      sli_type: error
      slo_alert: "yes"
      team: reliability_observability
      user_impacting: "no"
      window: 1h
    expr: |
      (
        (
          gitlab_component_errors:ratio_1h{component="rule_evaluation",env!~"gprd|ops",monitor="global",type="thanos-staging"}
          > (14.4 * 0.001000)
        )
        and
        (
          gitlab_component_errors:ratio_5m{component="rule_evaluation",env!~"gprd|ops",monitor="global",type="thanos-staging"}
          > (14.4 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_1h{component="rule_evaluation",env!~"gprd|ops",monitor="global",type="thanos-staging"}) >= 1
      )
  - alert: ThanosStagingServiceRuleEvaluationErrorSLOViolation
    for: 2m
    annotations:
      title: The rule_evaluation SLI of the thanos-staging service (`{{ $labels.stage
        }}` stage) has an error rate violating SLO
      description: |
        This SLI monitors Prometheus recording rule evaluations. Recording rule evalution failures are considered to be service failures. Warnings are also considered failures.

        Rule groups are evaluating recording rules in a group in sequence at an interval. If the recording of all rules in a groups exceeds the interval for the group, we could be missing data points in the group.

        If a group is slow often, we should split it up or improve query performance

        To see which rules are often not meeting their target. Look at the SLI-details. The `rule_group` label will contain information about the slow group.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: thanos-staging-main/thanos-staging-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/thanos-staging-main/thanos-staging-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3727131931"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          label_replace(rate(prometheus_rule_evaluation_failures_total{environment="{{ $labels.environment }}",job="thanos-ruler",namespace="thanos-staging",stage="{{ $labels.stage }}",type="thanos"}[5m]), "_c", "0", "", "")
          or
          label_replace(rate(thanos_rule_evaluation_with_warnings_total{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}"}[5m]), "_c", "1", "", "")
        )
      runbook: docs/thanos-staging/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      rules_domain: general
      severity: s3
      sli_type: error
      slo_alert: "yes"
      team: reliability_observability
      user_impacting: "no"
      window: 6h
    expr: |
      (
        (
          gitlab_component_errors:ratio_6h{component="rule_evaluation",env!~"gprd|ops",monitor="global",type="thanos-staging"}
          > (6 * 0.001000)
        )
        and
        (
          gitlab_component_errors:ratio_30m{component="rule_evaluation",env!~"gprd|ops",monitor="global",type="thanos-staging"}
          > (6 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_6h{component="rule_evaluation",env!~"gprd|ops",monitor="global",type="thanos-staging"}) >= 0.16667
      )
  - alert: ThanosStagingServiceRuleEvaluationTrafficCessation
    for: 5m
    annotations:
      title: The rule_evaluation SLI of the thanos-staging service (`{{ $labels.stage
        }}` stage) has not received any traffic in the past 30m
      description: |
        This SLI monitors Prometheus recording rule evaluations. Recording rule evalution failures are considered to be service failures. Warnings are also considered failures.

        Rule groups are evaluating recording rules in a group in sequence at an interval. If the recording of all rules in a groups exceeds the interval for the group, we could be missing data points in the group.

        If a group is slow often, we should split it up or improve query performance

        To see which rules are often not meeting their target. Look at the SLI-details. The `rule_group` label will contain information about the slow group.

        This alert signifies that the SLI is reporting a cessation of traffic; the signal is present, but is zero.
      grafana_dashboard_id: thanos-staging-main/thanos-staging-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/thanos-staging-main/thanos-staging-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2621394178"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(prometheus_rule_evaluations_total{environment="{{ $labels.environment }}",job="thanos-ruler",namespace="thanos-staging",stage="{{ $labels.stage }}",type="thanos"}[5m])
        )
      runbook: docs/thanos-staging/README.md
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      rules_domain: general
      severity: s3
      sli_type: ops
      slo_alert: "no"
      team: reliability_observability
      user_impacting: "no"
    expr: |
      gitlab_component_ops:rate_30m{component="rule_evaluation",env!~"gprd|ops",monitor="global",stage="main",type="thanos-staging"} == 0
      and
      gitlab_component_ops:rate_30m{component="rule_evaluation",env!~"gprd|ops",monitor="global",stage="main",type="thanos-staging"} offset 1h >= 0.16666666666666666
  - alert: ThanosStagingServiceRuleEvaluationTrafficAbsent
    for: 30m
    annotations:
      title: The rule_evaluation SLI of the thanos-staging service (`{{ $labels.stage
        }}` stage) has not reported any traffic in the past 30m
      description: |
        This SLI monitors Prometheus recording rule evaluations. Recording rule evalution failures are considered to be service failures. Warnings are also considered failures.

        Rule groups are evaluating recording rules in a group in sequence at an interval. If the recording of all rules in a groups exceeds the interval for the group, we could be missing data points in the group.

        If a group is slow often, we should split it up or improve query performance

        To see which rules are often not meeting their target. Look at the SLI-details. The `rule_group` label will contain information about the slow group.

        This alert signifies that the SLI was previously reporting traffic, but is no longer - the signal is absent.

        This could be caused by a change to the metrics used in the SLI, or by the service not receiving traffic.
      grafana_dashboard_id: thanos-staging-main/thanos-staging-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/thanos-staging-main/thanos-staging-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2621394178"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(prometheus_rule_evaluations_total{environment="{{ $labels.environment }}",job="thanos-ruler",namespace="thanos-staging",stage="{{ $labels.stage }}",type="thanos"}[5m])
        )
      runbook: docs/thanos-staging/README.md
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      rules_domain: general
      severity: s3
      sli_type: ops
      slo_alert: "no"
      team: reliability_observability
      user_impacting: "no"
    expr: |
      gitlab_component_ops:rate_5m{component="rule_evaluation",env!~"gprd|ops",monitor="global",stage="main",type="thanos-staging"} offset 1h
      unless
      gitlab_component_ops:rate_5m{component="rule_evaluation",env!~"gprd|ops",monitor="global",stage="main",type="thanos-staging"}
  - alert: ThanosStagingServiceThanosCompactorErrorSLOViolation
    for: 2m
    annotations:
      title: The thanos_compactor SLI of the thanos-staging service (`{{ $labels.stage
        }}` stage) has an error rate violating SLO
      description: |
        Thanos compactor is responsible for compaction of Prometheus series data into blocks, which are stored in GCS buckets. It also handles downsampling. This SLI monitors compaction operations and compaction failures.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: thanos-staging-main/thanos-staging-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/thanos-staging-main/thanos-staging-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3147757590"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(thanos_compact_group_compactions_failures_total{environment="{{ $labels.environment }}",job=~"thanos-(.+)-compactor",namespace="thanos-staging",stage="{{ $labels.stage }}",type="thanos"}[5m])
        )
      runbook: docs/thanos-staging/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      rules_domain: general
      severity: s3
      sli_type: error
      slo_alert: "yes"
      team: reliability_observability
      user_impacting: "no"
      window: 1h
    expr: |
      (
        (
          gitlab_component_errors:ratio_1h{component="thanos_compactor",env!~"gprd|ops",monitor="global",type="thanos-staging"}
          > (14.4 * 0.001000)
        )
        and
        (
          gitlab_component_errors:ratio_5m{component="thanos_compactor",env!~"gprd|ops",monitor="global",type="thanos-staging"}
          > (14.4 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_1h{component="thanos_compactor",env!~"gprd|ops",monitor="global",type="thanos-staging"}) >= 1
      )
  - alert: ThanosStagingServiceThanosCompactorErrorSLOViolation
    for: 2m
    annotations:
      title: The thanos_compactor SLI of the thanos-staging service (`{{ $labels.stage
        }}` stage) has an error rate violating SLO
      description: |
        Thanos compactor is responsible for compaction of Prometheus series data into blocks, which are stored in GCS buckets. It also handles downsampling. This SLI monitors compaction operations and compaction failures.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: thanos-staging-main/thanos-staging-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/thanos-staging-main/thanos-staging-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3147757590"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(thanos_compact_group_compactions_failures_total{environment="{{ $labels.environment }}",job=~"thanos-(.+)-compactor",namespace="thanos-staging",stage="{{ $labels.stage }}",type="thanos"}[5m])
        )
      runbook: docs/thanos-staging/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      rules_domain: general
      severity: s3
      sli_type: error
      slo_alert: "yes"
      team: reliability_observability
      user_impacting: "no"
      window: 6h
    expr: |
      (
        (
          gitlab_component_errors:ratio_6h{component="thanos_compactor",env!~"gprd|ops",monitor="global",type="thanos-staging"}
          > (6 * 0.001000)
        )
        and
        (
          gitlab_component_errors:ratio_30m{component="thanos_compactor",env!~"gprd|ops",monitor="global",type="thanos-staging"}
          > (6 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_6h{component="thanos_compactor",env!~"gprd|ops",monitor="global",type="thanos-staging"}) >= 0.16667
      )
  - alert: ThanosStagingServiceThanosMemcachedApdexSLOViolation
    for: 2m
    annotations:
      title: The thanos_memcached SLI of the thanos-staging service (`{{ $labels.stage
        }}` stage) has an apdex violating SLO
      description: |
        Various memcached instances support our thanos infrastructure, for the store and query-frontend components.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: thanos-staging-main/thanos-staging-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/thanos-staging-main/thanos-staging-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1018147051"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (env,environment,tier,stage,le) (
            rate(thanos_memcached_operation_duration_seconds_bucket{environment="{{ $labels.environment }}",grpc_type="unary",job=~"thanos-(labels|querier)-metrics|thanos-(.+)-(bucket|index)-metrics",namespace="thanos-staging",stage="{{ $labels.stage }}",type="thanos"}[5m])
          )
        )
      runbook: docs/thanos-staging/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      rules_domain: general
      severity: s3
      sli_type: apdex
      slo_alert: "yes"
      team: reliability_observability
      user_impacting: "no"
      window: 1h
    expr: |
      (
        (
          gitlab_component_apdex:ratio_1h{component="thanos_memcached",env!~"gprd|ops",monitor="global",type="thanos-staging"}
          < (1 - 14.4 * 0.001000)
        )
        and
        (
          gitlab_component_apdex:ratio_5m{component="thanos_memcached",env!~"gprd|ops",monitor="global",type="thanos-staging"}
          < (1 - 14.4 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_1h{component="thanos_memcached",env!~"gprd|ops",monitor="global",type="thanos-staging"}) >= 1
      )
  - alert: ThanosStagingServiceThanosMemcachedApdexSLOViolation
    for: 2m
    annotations:
      title: The thanos_memcached SLI of the thanos-staging service (`{{ $labels.stage
        }}` stage) has an apdex violating SLO
      description: |
        Various memcached instances support our thanos infrastructure, for the store and query-frontend components.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: thanos-staging-main/thanos-staging-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/thanos-staging-main/thanos-staging-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1018147051"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (env,environment,tier,stage,le) (
            rate(thanos_memcached_operation_duration_seconds_bucket{environment="{{ $labels.environment }}",grpc_type="unary",job=~"thanos-(labels|querier)-metrics|thanos-(.+)-(bucket|index)-metrics",namespace="thanos-staging",stage="{{ $labels.stage }}",type="thanos"}[5m])
          )
        )
      runbook: docs/thanos-staging/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      rules_domain: general
      severity: s3
      sli_type: apdex
      slo_alert: "yes"
      team: reliability_observability
      user_impacting: "no"
      window: 6h
    expr: |
      (
        (
          gitlab_component_apdex:ratio_6h{component="thanos_memcached",env!~"gprd|ops",monitor="global",type="thanos-staging"}
          < (1 - 6 * 0.001000)
        )
        and
        (
          gitlab_component_apdex:ratio_30m{component="thanos_memcached",env!~"gprd|ops",monitor="global",type="thanos-staging"}
          < (1 - 6 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_6h{component="thanos_memcached",env!~"gprd|ops",monitor="global",type="thanos-staging"}) >= 0.16667
      )
  - alert: ThanosStagingServiceThanosMemcachedErrorSLOViolation
    for: 2m
    annotations:
      title: The thanos_memcached SLI of the thanos-staging service (`{{ $labels.stage
        }}` stage) has an error rate violating SLO
      description: |
        Various memcached instances support our thanos infrastructure, for the store and query-frontend components.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: thanos-staging-main/thanos-staging-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/thanos-staging-main/thanos-staging-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "362238449"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(thanos_memcached_operation_failures_total{environment="{{ $labels.environment }}",grpc_type="unary",job=~"thanos-(labels|querier)-metrics|thanos-(.+)-(bucket|index)-metrics",namespace="thanos-staging",stage="{{ $labels.stage }}",type="thanos"}[5m])
        )
      runbook: docs/thanos-staging/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      rules_domain: general
      severity: s3
      sli_type: error
      slo_alert: "yes"
      team: reliability_observability
      user_impacting: "no"
      window: 1h
    expr: |
      (
        (
          gitlab_component_errors:ratio_1h{component="thanos_memcached",env!~"gprd|ops",monitor="global",type="thanos-staging"}
          > (14.4 * 0.050000)
        )
        and
        (
          gitlab_component_errors:ratio_5m{component="thanos_memcached",env!~"gprd|ops",monitor="global",type="thanos-staging"}
          > (14.4 * 0.050000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_1h{component="thanos_memcached",env!~"gprd|ops",monitor="global",type="thanos-staging"}) >= 1
      )
  - alert: ThanosStagingServiceThanosMemcachedErrorSLOViolation
    for: 2m
    annotations:
      title: The thanos_memcached SLI of the thanos-staging service (`{{ $labels.stage
        }}` stage) has an error rate violating SLO
      description: |
        Various memcached instances support our thanos infrastructure, for the store and query-frontend components.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: thanos-staging-main/thanos-staging-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/thanos-staging-main/thanos-staging-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "362238449"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(thanos_memcached_operation_failures_total{environment="{{ $labels.environment }}",grpc_type="unary",job=~"thanos-(labels|querier)-metrics|thanos-(.+)-(bucket|index)-metrics",namespace="thanos-staging",stage="{{ $labels.stage }}",type="thanos"}[5m])
        )
      runbook: docs/thanos-staging/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      rules_domain: general
      severity: s3
      sli_type: error
      slo_alert: "yes"
      team: reliability_observability
      user_impacting: "no"
      window: 6h
    expr: |
      (
        (
          gitlab_component_errors:ratio_6h{component="thanos_memcached",env!~"gprd|ops",monitor="global",type="thanos-staging"}
          > (6 * 0.050000)
        )
        and
        (
          gitlab_component_errors:ratio_30m{component="thanos_memcached",env!~"gprd|ops",monitor="global",type="thanos-staging"}
          > (6 * 0.050000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_6h{component="thanos_memcached",env!~"gprd|ops",monitor="global",type="thanos-staging"}) >= 0.16667
      )
  - alert: ThanosStagingServiceThanosQueryApdexSLOViolation
    for: 2m
    annotations:
      title: The thanos_query SLI of the thanos-staging service (`{{ $labels.stage
        }}` stage) has an apdex violating SLO
      description: |
        Thanos query gathers the data needed to evaluate Prometheus queries from multiple underlying prometheus and thanos instances. This SLI monitors the Thanos query HTTP interface. 5xx responses are considered failures.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: thanos-staging-main/thanos-staging-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/thanos-staging-main/thanos-staging-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1281064991"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (env,environment,tier,stage,le) (
            rate(http_request_duration_seconds_bucket{environment="{{ $labels.environment }}",job="thanos-query",namespace="thanos-staging",stage="{{ $labels.stage }}",type="thanos"}[5m])
          )
        )
      runbook: docs/thanos-staging/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      rules_domain: general
      severity: s3
      sli_type: apdex
      slo_alert: "yes"
      team: reliability_observability
      user_impacting: "no"
      window: 1h
    expr: |
      (
        (
          gitlab_component_apdex:ratio_1h{component="thanos_query",env!~"gprd|ops",monitor="global",type="thanos-staging"}
          < (1 - 14.4 * 0.050000)
        )
        and
        (
          gitlab_component_apdex:ratio_5m{component="thanos_query",env!~"gprd|ops",monitor="global",type="thanos-staging"}
          < (1 - 14.4 * 0.050000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_1h{component="thanos_query",env!~"gprd|ops",monitor="global",type="thanos-staging"}) >= 1
      )
  - alert: ThanosStagingServiceThanosQueryApdexSLOViolation
    for: 2m
    annotations:
      title: The thanos_query SLI of the thanos-staging service (`{{ $labels.stage
        }}` stage) has an apdex violating SLO
      description: |
        Thanos query gathers the data needed to evaluate Prometheus queries from multiple underlying prometheus and thanos instances. This SLI monitors the Thanos query HTTP interface. 5xx responses are considered failures.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: thanos-staging-main/thanos-staging-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/thanos-staging-main/thanos-staging-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1281064991"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (env,environment,tier,stage,le) (
            rate(http_request_duration_seconds_bucket{environment="{{ $labels.environment }}",job="thanos-query",namespace="thanos-staging",stage="{{ $labels.stage }}",type="thanos"}[5m])
          )
        )
      runbook: docs/thanos-staging/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      rules_domain: general
      severity: s3
      sli_type: apdex
      slo_alert: "yes"
      team: reliability_observability
      user_impacting: "no"
      window: 6h
    expr: |
      (
        (
          gitlab_component_apdex:ratio_6h{component="thanos_query",env!~"gprd|ops",monitor="global",type="thanos-staging"}
          < (1 - 6 * 0.050000)
        )
        and
        (
          gitlab_component_apdex:ratio_30m{component="thanos_query",env!~"gprd|ops",monitor="global",type="thanos-staging"}
          < (1 - 6 * 0.050000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_6h{component="thanos_query",env!~"gprd|ops",monitor="global",type="thanos-staging"}) >= 0.16667
      )
  - alert: ThanosStagingServiceThanosQueryErrorSLOViolation
    for: 2m
    annotations:
      title: The thanos_query SLI of the thanos-staging service (`{{ $labels.stage
        }}` stage) has an error rate violating SLO
      description: |
        Thanos query gathers the data needed to evaluate Prometheus queries from multiple underlying prometheus and thanos instances. This SLI monitors the Thanos query HTTP interface. 5xx responses are considered failures.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: thanos-staging-main/thanos-staging-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/thanos-staging-main/thanos-staging-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "4004985658"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          sli_aggregations:http_requests_total_rate5m{code=~"^5.*",environment="{{ $labels.environment }}",job="thanos-query",namespace="thanos-staging",stage="{{ $labels.stage }}",type="thanos"}
        )
      runbook: docs/thanos-staging/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      rules_domain: general
      severity: s3
      sli_type: error
      slo_alert: "yes"
      team: reliability_observability
      user_impacting: "no"
      window: 1h
    expr: |
      (
        (
          gitlab_component_errors:ratio_1h{component="thanos_query",env!~"gprd|ops",monitor="global",type="thanos-staging"}
          > (14.4 * 0.050000)
        )
        and
        (
          gitlab_component_errors:ratio_5m{component="thanos_query",env!~"gprd|ops",monitor="global",type="thanos-staging"}
          > (14.4 * 0.050000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_1h{component="thanos_query",env!~"gprd|ops",monitor="global",type="thanos-staging"}) >= 1
      )
  - alert: ThanosStagingServiceThanosQueryErrorSLOViolation
    for: 2m
    annotations:
      title: The thanos_query SLI of the thanos-staging service (`{{ $labels.stage
        }}` stage) has an error rate violating SLO
      description: |
        Thanos query gathers the data needed to evaluate Prometheus queries from multiple underlying prometheus and thanos instances. This SLI monitors the Thanos query HTTP interface. 5xx responses are considered failures.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: thanos-staging-main/thanos-staging-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/thanos-staging-main/thanos-staging-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "4004985658"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          sli_aggregations:http_requests_total_rate5m{code=~"^5.*",environment="{{ $labels.environment }}",job="thanos-query",namespace="thanos-staging",stage="{{ $labels.stage }}",type="thanos"}
        )
      runbook: docs/thanos-staging/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      rules_domain: general
      severity: s3
      sli_type: error
      slo_alert: "yes"
      team: reliability_observability
      user_impacting: "no"
      window: 6h
    expr: |
      (
        (
          gitlab_component_errors:ratio_6h{component="thanos_query",env!~"gprd|ops",monitor="global",type="thanos-staging"}
          > (6 * 0.050000)
        )
        and
        (
          gitlab_component_errors:ratio_30m{component="thanos_query",env!~"gprd|ops",monitor="global",type="thanos-staging"}
          > (6 * 0.050000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_6h{component="thanos_query",env!~"gprd|ops",monitor="global",type="thanos-staging"}) >= 0.16667
      )
  - alert: ThanosStagingServiceThanosQueryTrafficCessation
    for: 5m
    annotations:
      title: The thanos_query SLI of the thanos-staging service (`{{ $labels.stage
        }}` stage) has not received any traffic in the past 30m
      description: |
        Thanos query gathers the data needed to evaluate Prometheus queries from multiple underlying prometheus and thanos instances. This SLI monitors the Thanos query HTTP interface. 5xx responses are considered failures.

        This alert signifies that the SLI is reporting a cessation of traffic; the signal is present, but is zero.
      grafana_dashboard_id: thanos-staging-main/thanos-staging-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/thanos-staging-main/thanos-staging-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "431223751"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          sli_aggregations:http_requests_total_rate5m{environment="{{ $labels.environment }}",job="thanos-query",namespace="thanos-staging",stage="{{ $labels.stage }}",type="thanos"}
        )
      runbook: docs/thanos-staging/README.md
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      rules_domain: general
      severity: s3
      sli_type: ops
      slo_alert: "no"
      team: reliability_observability
      user_impacting: "no"
    expr: |
      gitlab_component_ops:rate_30m{component="thanos_query",env!~"gprd|ops",monitor="global",stage="main",type="thanos-staging"} == 0
      and
      gitlab_component_ops:rate_30m{component="thanos_query",env!~"gprd|ops",monitor="global",stage="main",type="thanos-staging"} offset 1h >= 0.16666666666666666
  - alert: ThanosStagingServiceThanosQueryTrafficAbsent
    for: 30m
    annotations:
      title: The thanos_query SLI of the thanos-staging service (`{{ $labels.stage
        }}` stage) has not reported any traffic in the past 30m
      description: |
        Thanos query gathers the data needed to evaluate Prometheus queries from multiple underlying prometheus and thanos instances. This SLI monitors the Thanos query HTTP interface. 5xx responses are considered failures.

        This alert signifies that the SLI was previously reporting traffic, but is no longer - the signal is absent.

        This could be caused by a change to the metrics used in the SLI, or by the service not receiving traffic.
      grafana_dashboard_id: thanos-staging-main/thanos-staging-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/thanos-staging-main/thanos-staging-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "431223751"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          sli_aggregations:http_requests_total_rate5m{environment="{{ $labels.environment }}",job="thanos-query",namespace="thanos-staging",stage="{{ $labels.stage }}",type="thanos"}
        )
      runbook: docs/thanos-staging/README.md
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      rules_domain: general
      severity: s3
      sli_type: ops
      slo_alert: "no"
      team: reliability_observability
      user_impacting: "no"
    expr: |
      gitlab_component_ops:rate_5m{component="thanos_query",env!~"gprd|ops",monitor="global",stage="main",type="thanos-staging"} offset 1h
      unless
      gitlab_component_ops:rate_5m{component="thanos_query",env!~"gprd|ops",monitor="global",stage="main",type="thanos-staging"}
  - alert: ThanosStagingServiceThanosQueryFrontendApdexSLOViolation
    for: 2m
    annotations:
      title: The thanos_query_frontend SLI of the thanos-staging service (`{{ $labels.stage
        }}` stage) has an apdex violating SLO
      description: |
        Thanos query gathers the data needed to evaluate Prometheus queries from multiple underlying prometheus and thanos instances. This SLI monitors the Thanos query HTTP interface. 5xx responses are considered failures.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: thanos-staging-main/thanos-staging-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/thanos-staging-main/thanos-staging-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3015468996"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (env,environment,tier,stage,le) (
            rate(http_request_duration_seconds_bucket{environment="{{ $labels.environment }}",job="thanos-query-frontend",namespace="thanos-staging",stage="{{ $labels.stage }}",type="thanos"}[5m])
          )
        )
      runbook: docs/thanos-staging/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      rules_domain: general
      severity: s3
      sli_type: apdex
      slo_alert: "yes"
      team: reliability_observability
      user_impacting: "no"
      window: 1h
    expr: |
      (
        (
          gitlab_component_apdex:ratio_1h{component="thanos_query_frontend",env!~"gprd|ops",monitor="global",type="thanos-staging"}
          < (1 - 14.4 * 0.050000)
        )
        and
        (
          gitlab_component_apdex:ratio_5m{component="thanos_query_frontend",env!~"gprd|ops",monitor="global",type="thanos-staging"}
          < (1 - 14.4 * 0.050000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_1h{component="thanos_query_frontend",env!~"gprd|ops",monitor="global",type="thanos-staging"}) >= 1
      )
  - alert: ThanosStagingServiceThanosQueryFrontendApdexSLOViolation
    for: 2m
    annotations:
      title: The thanos_query_frontend SLI of the thanos-staging service (`{{ $labels.stage
        }}` stage) has an apdex violating SLO
      description: |
        Thanos query gathers the data needed to evaluate Prometheus queries from multiple underlying prometheus and thanos instances. This SLI monitors the Thanos query HTTP interface. 5xx responses are considered failures.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: thanos-staging-main/thanos-staging-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/thanos-staging-main/thanos-staging-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3015468996"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (env,environment,tier,stage,le) (
            rate(http_request_duration_seconds_bucket{environment="{{ $labels.environment }}",job="thanos-query-frontend",namespace="thanos-staging",stage="{{ $labels.stage }}",type="thanos"}[5m])
          )
        )
      runbook: docs/thanos-staging/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      rules_domain: general
      severity: s3
      sli_type: apdex
      slo_alert: "yes"
      team: reliability_observability
      user_impacting: "no"
      window: 6h
    expr: |
      (
        (
          gitlab_component_apdex:ratio_6h{component="thanos_query_frontend",env!~"gprd|ops",monitor="global",type="thanos-staging"}
          < (1 - 6 * 0.050000)
        )
        and
        (
          gitlab_component_apdex:ratio_30m{component="thanos_query_frontend",env!~"gprd|ops",monitor="global",type="thanos-staging"}
          < (1 - 6 * 0.050000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_6h{component="thanos_query_frontend",env!~"gprd|ops",monitor="global",type="thanos-staging"}) >= 0.16667
      )
  - alert: ThanosStagingServiceThanosQueryFrontendErrorSLOViolation
    for: 2m
    annotations:
      title: The thanos_query_frontend SLI of the thanos-staging service (`{{ $labels.stage
        }}` stage) has an error rate violating SLO
      description: |
        Thanos query gathers the data needed to evaluate Prometheus queries from multiple underlying prometheus and thanos instances. This SLI monitors the Thanos query HTTP interface. 5xx responses are considered failures.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: thanos-staging-main/thanos-staging-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/thanos-staging-main/thanos-staging-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1994646344"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          sli_aggregations:http_requests_total_rate5m{code=~"^5.*",environment="{{ $labels.environment }}",job="thanos-query-frontend",namespace="thanos-staging",stage="{{ $labels.stage }}",type="thanos"}
        )
      runbook: docs/thanos-staging/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      rules_domain: general
      severity: s3
      sli_type: error
      slo_alert: "yes"
      team: reliability_observability
      user_impacting: "no"
      window: 1h
    expr: |
      (
        (
          gitlab_component_errors:ratio_1h{component="thanos_query_frontend",env!~"gprd|ops",monitor="global",type="thanos-staging"}
          > (14.4 * 0.050000)
        )
        and
        (
          gitlab_component_errors:ratio_5m{component="thanos_query_frontend",env!~"gprd|ops",monitor="global",type="thanos-staging"}
          > (14.4 * 0.050000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_1h{component="thanos_query_frontend",env!~"gprd|ops",monitor="global",type="thanos-staging"}) >= 1
      )
  - alert: ThanosStagingServiceThanosQueryFrontendErrorSLOViolation
    for: 2m
    annotations:
      title: The thanos_query_frontend SLI of the thanos-staging service (`{{ $labels.stage
        }}` stage) has an error rate violating SLO
      description: |
        Thanos query gathers the data needed to evaluate Prometheus queries from multiple underlying prometheus and thanos instances. This SLI monitors the Thanos query HTTP interface. 5xx responses are considered failures.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: thanos-staging-main/thanos-staging-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/thanos-staging-main/thanos-staging-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1994646344"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          sli_aggregations:http_requests_total_rate5m{code=~"^5.*",environment="{{ $labels.environment }}",job="thanos-query-frontend",namespace="thanos-staging",stage="{{ $labels.stage }}",type="thanos"}
        )
      runbook: docs/thanos-staging/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      rules_domain: general
      severity: s3
      sli_type: error
      slo_alert: "yes"
      team: reliability_observability
      user_impacting: "no"
      window: 6h
    expr: |
      (
        (
          gitlab_component_errors:ratio_6h{component="thanos_query_frontend",env!~"gprd|ops",monitor="global",type="thanos-staging"}
          > (6 * 0.050000)
        )
        and
        (
          gitlab_component_errors:ratio_30m{component="thanos_query_frontend",env!~"gprd|ops",monitor="global",type="thanos-staging"}
          > (6 * 0.050000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_6h{component="thanos_query_frontend",env!~"gprd|ops",monitor="global",type="thanos-staging"}) >= 0.16667
      )
  - alert: ThanosStagingServiceThanosQueryFrontendTrafficCessation
    for: 5m
    annotations:
      title: The thanos_query_frontend SLI of the thanos-staging service (`{{ $labels.stage
        }}` stage) has not received any traffic in the past 30m
      description: |
        Thanos query gathers the data needed to evaluate Prometheus queries from multiple underlying prometheus and thanos instances. This SLI monitors the Thanos query HTTP interface. 5xx responses are considered failures.

        This alert signifies that the SLI is reporting a cessation of traffic; the signal is present, but is zero.
      grafana_dashboard_id: thanos-staging-main/thanos-staging-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/thanos-staging-main/thanos-staging-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1877464727"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          sli_aggregations:http_requests_total_rate5m{environment="{{ $labels.environment }}",job="thanos-query-frontend",namespace="thanos-staging",stage="{{ $labels.stage }}",type="thanos"}
        )
      runbook: docs/thanos-staging/README.md
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      rules_domain: general
      severity: s3
      sli_type: ops
      slo_alert: "no"
      team: reliability_observability
      user_impacting: "no"
    expr: |
      gitlab_component_ops:rate_30m{component="thanos_query_frontend",env!~"gprd|ops",monitor="global",stage="main",type="thanos-staging"} == 0
      and
      gitlab_component_ops:rate_30m{component="thanos_query_frontend",env!~"gprd|ops",monitor="global",stage="main",type="thanos-staging"} offset 1h >= 0.16666666666666666
  - alert: ThanosStagingServiceThanosQueryFrontendTrafficAbsent
    for: 30m
    annotations:
      title: The thanos_query_frontend SLI of the thanos-staging service (`{{ $labels.stage
        }}` stage) has not reported any traffic in the past 30m
      description: |
        Thanos query gathers the data needed to evaluate Prometheus queries from multiple underlying prometheus and thanos instances. This SLI monitors the Thanos query HTTP interface. 5xx responses are considered failures.

        This alert signifies that the SLI was previously reporting traffic, but is no longer - the signal is absent.

        This could be caused by a change to the metrics used in the SLI, or by the service not receiving traffic.
      grafana_dashboard_id: thanos-staging-main/thanos-staging-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/thanos-staging-main/thanos-staging-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1877464727"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          sli_aggregations:http_requests_total_rate5m{environment="{{ $labels.environment }}",job="thanos-query-frontend",namespace="thanos-staging",stage="{{ $labels.stage }}",type="thanos"}
        )
      runbook: docs/thanos-staging/README.md
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      rules_domain: general
      severity: s3
      sli_type: ops
      slo_alert: "no"
      team: reliability_observability
      user_impacting: "no"
    expr: |
      gitlab_component_ops:rate_5m{component="thanos_query_frontend",env!~"gprd|ops",monitor="global",stage="main",type="thanos-staging"} offset 1h
      unless
      gitlab_component_ops:rate_5m{component="thanos_query_frontend",env!~"gprd|ops",monitor="global",stage="main",type="thanos-staging"}
  - alert: ThanosStagingServiceThanosRuleAlertSenderErrorSLOViolation
    for: 2m
    annotations:
      title: The thanos_rule_alert_sender SLI of the thanos-staging service (`{{ $labels.stage
        }}` stage) has an error rate violating SLO
      description: |
        This SLI monitors alerts generated by Thanos Ruler. Alert delivery failure is considered a service-level failure.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: thanos-staging-main/thanos-staging-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/thanos-staging-main/thanos-staging-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3718148749"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(thanos_alert_sender_alerts_dropped_total{environment="{{ $labels.environment }}",job="thanos-ruler",namespace="thanos-staging",stage="{{ $labels.stage }}",type="thanos"}[5m])
        )
      runbook: docs/thanos-staging/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      rules_domain: general
      severity: s3
      sli_type: error
      slo_alert: "yes"
      team: reliability_observability
      user_impacting: "yes"
      window: 1h
    expr: |
      (
        (
          gitlab_component_errors:ratio_1h{component="thanos_rule_alert_sender",env!~"gprd|ops",monitor="global",type="thanos-staging"}
          > (14.4 * 0.001000)
        )
        and
        (
          gitlab_component_errors:ratio_5m{component="thanos_rule_alert_sender",env!~"gprd|ops",monitor="global",type="thanos-staging"}
          > (14.4 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_1h{component="thanos_rule_alert_sender",env!~"gprd|ops",monitor="global",type="thanos-staging"}) >= 1
      )
  - alert: ThanosStagingServiceThanosRuleAlertSenderErrorSLOViolation
    for: 2m
    annotations:
      title: The thanos_rule_alert_sender SLI of the thanos-staging service (`{{ $labels.stage
        }}` stage) has an error rate violating SLO
      description: |
        This SLI monitors alerts generated by Thanos Ruler. Alert delivery failure is considered a service-level failure.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: thanos-staging-main/thanos-staging-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/thanos-staging-main/thanos-staging-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3718148749"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(thanos_alert_sender_alerts_dropped_total{environment="{{ $labels.environment }}",job="thanos-ruler",namespace="thanos-staging",stage="{{ $labels.stage }}",type="thanos"}[5m])
        )
      runbook: docs/thanos-staging/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      rules_domain: general
      severity: s3
      sli_type: error
      slo_alert: "yes"
      team: reliability_observability
      user_impacting: "yes"
      window: 6h
    expr: |
      (
        (
          gitlab_component_errors:ratio_6h{component="thanos_rule_alert_sender",env!~"gprd|ops",monitor="global",type="thanos-staging"}
          > (6 * 0.001000)
        )
        and
        (
          gitlab_component_errors:ratio_30m{component="thanos_rule_alert_sender",env!~"gprd|ops",monitor="global",type="thanos-staging"}
          > (6 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_6h{component="thanos_rule_alert_sender",env!~"gprd|ops",monitor="global",type="thanos-staging"}) >= 0.16667
      )
  - alert: ThanosStagingServiceThanosRuleAlertSenderTrafficCessation
    for: 5m
    annotations:
      title: The thanos_rule_alert_sender SLI of the thanos-staging service (`{{ $labels.stage
        }}` stage) has not received any traffic in the past 30m
      description: |
        This SLI monitors alerts generated by Thanos Ruler. Alert delivery failure is considered a service-level failure.

        This alert signifies that the SLI is reporting a cessation of traffic; the signal is present, but is zero.
      grafana_dashboard_id: thanos-staging-main/thanos-staging-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/thanos-staging-main/thanos-staging-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2608229891"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(thanos_alert_sender_alerts_sent_total{environment="{{ $labels.environment }}",job="thanos-ruler",namespace="thanos-staging",stage="{{ $labels.stage }}",type="thanos"}[5m])
        )
      runbook: docs/thanos-staging/README.md
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      rules_domain: general
      severity: s3
      sli_type: ops
      slo_alert: "no"
      team: reliability_observability
      user_impacting: "yes"
    expr: |
      gitlab_component_ops:rate_30m{component="thanos_rule_alert_sender",env!~"gprd|ops",monitor="global",stage="main",type="thanos-staging"} == 0
      and
      gitlab_component_ops:rate_30m{component="thanos_rule_alert_sender",env!~"gprd|ops",monitor="global",stage="main",type="thanos-staging"} offset 1h >= 0.16666666666666666
  - alert: ThanosStagingServiceThanosRuleAlertSenderTrafficAbsent
    for: 30m
    annotations:
      title: The thanos_rule_alert_sender SLI of the thanos-staging service (`{{ $labels.stage
        }}` stage) has not reported any traffic in the past 30m
      description: |
        This SLI monitors alerts generated by Thanos Ruler. Alert delivery failure is considered a service-level failure.

        This alert signifies that the SLI was previously reporting traffic, but is no longer - the signal is absent.

        This could be caused by a change to the metrics used in the SLI, or by the service not receiving traffic.
      grafana_dashboard_id: thanos-staging-main/thanos-staging-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/thanos-staging-main/thanos-staging-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2608229891"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(thanos_alert_sender_alerts_sent_total{environment="{{ $labels.environment }}",job="thanos-ruler",namespace="thanos-staging",stage="{{ $labels.stage }}",type="thanos"}[5m])
        )
      runbook: docs/thanos-staging/README.md
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      rules_domain: general
      severity: s3
      sli_type: ops
      slo_alert: "no"
      team: reliability_observability
      user_impacting: "yes"
    expr: |
      gitlab_component_ops:rate_5m{component="thanos_rule_alert_sender",env!~"gprd|ops",monitor="global",stage="main",type="thanos-staging"} offset 1h
      unless
      gitlab_component_ops:rate_5m{component="thanos_rule_alert_sender",env!~"gprd|ops",monitor="global",stage="main",type="thanos-staging"}
  - alert: ThanosStagingServiceThanosStoreApdexSLOViolation
    for: 2m
    annotations:
      title: The thanos_store SLI of the thanos-staging service (`{{ $labels.stage
        }}` stage) has an apdex violating SLO
      description: |
        Thanos store will respond to Thanos Query (and other) requests for historical data. This historical data is kept in GCS buckets. This SLI monitors the Thanos StoreAPI GRPC endpoint. GRPC error responses are considered to be service-level failures.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: thanos-staging-main/thanos-staging-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/thanos-staging-main/thanos-staging-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3649067073"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (env,environment,tier,stage,le) (
            rate(grpc_server_handling_seconds_bucket{environment="{{ $labels.environment }}",grpc_type="unary",job=~"thanos-(.+)-storegateway-(.+)",namespace="thanos-staging",stage="{{ $labels.stage }}",type="thanos"}[5m])
          )
        )
      runbook: docs/thanos-staging/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      rules_domain: general
      severity: s3
      sli_type: apdex
      slo_alert: "yes"
      team: reliability_observability
      user_impacting: "no"
      window: 1h
    expr: |
      (
        (
          gitlab_component_apdex:ratio_1h{component="thanos_store",env!~"gprd|ops",monitor="global",type="thanos-staging"}
          < (1 - 14.4 * 0.050000)
        )
        and
        (
          gitlab_component_apdex:ratio_5m{component="thanos_store",env!~"gprd|ops",monitor="global",type="thanos-staging"}
          < (1 - 14.4 * 0.050000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_1h{component="thanos_store",env!~"gprd|ops",monitor="global",type="thanos-staging"}) >= 1
      )
  - alert: ThanosStagingServiceThanosStoreApdexSLOViolation
    for: 2m
    annotations:
      title: The thanos_store SLI of the thanos-staging service (`{{ $labels.stage
        }}` stage) has an apdex violating SLO
      description: |
        Thanos store will respond to Thanos Query (and other) requests for historical data. This historical data is kept in GCS buckets. This SLI monitors the Thanos StoreAPI GRPC endpoint. GRPC error responses are considered to be service-level failures.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: thanos-staging-main/thanos-staging-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/thanos-staging-main/thanos-staging-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3649067073"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (env,environment,tier,stage,le) (
            rate(grpc_server_handling_seconds_bucket{environment="{{ $labels.environment }}",grpc_type="unary",job=~"thanos-(.+)-storegateway-(.+)",namespace="thanos-staging",stage="{{ $labels.stage }}",type="thanos"}[5m])
          )
        )
      runbook: docs/thanos-staging/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      rules_domain: general
      severity: s3
      sli_type: apdex
      slo_alert: "yes"
      team: reliability_observability
      user_impacting: "no"
      window: 6h
    expr: |
      (
        (
          gitlab_component_apdex:ratio_6h{component="thanos_store",env!~"gprd|ops",monitor="global",type="thanos-staging"}
          < (1 - 6 * 0.050000)
        )
        and
        (
          gitlab_component_apdex:ratio_30m{component="thanos_store",env!~"gprd|ops",monitor="global",type="thanos-staging"}
          < (1 - 6 * 0.050000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_6h{component="thanos_store",env!~"gprd|ops",monitor="global",type="thanos-staging"}) >= 0.16667
      )
  - alert: ThanosStagingServiceThanosStoreErrorSLOViolation
    for: 2m
    annotations:
      title: The thanos_store SLI of the thanos-staging service (`{{ $labels.stage
        }}` stage) has an error rate violating SLO
      description: |
        Thanos store will respond to Thanos Query (and other) requests for historical data. This historical data is kept in GCS buckets. This SLI monitors the Thanos StoreAPI GRPC endpoint. GRPC error responses are considered to be service-level failures.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: thanos-staging-main/thanos-staging-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/thanos-staging-main/thanos-staging-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "279093558"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(grpc_server_handled_total{environment="{{ $labels.environment }}",grpc_code!="OK",grpc_type="unary",job=~"thanos-(.+)-storegateway-(.+)",namespace="thanos-staging",stage="{{ $labels.stage }}",type="thanos"}[5m])
        )
      runbook: docs/thanos-staging/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      rules_domain: general
      severity: s3
      sli_type: error
      slo_alert: "yes"
      team: reliability_observability
      user_impacting: "no"
      window: 1h
    expr: |
      (
        (
          gitlab_component_errors:ratio_1h{component="thanos_store",env!~"gprd|ops",monitor="global",type="thanos-staging"}
          > (14.4 * 0.050000)
        )
        and
        (
          gitlab_component_errors:ratio_5m{component="thanos_store",env!~"gprd|ops",monitor="global",type="thanos-staging"}
          > (14.4 * 0.050000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_1h{component="thanos_store",env!~"gprd|ops",monitor="global",type="thanos-staging"}) >= 1
      )
  - alert: ThanosStagingServiceThanosStoreErrorSLOViolation
    for: 2m
    annotations:
      title: The thanos_store SLI of the thanos-staging service (`{{ $labels.stage
        }}` stage) has an error rate violating SLO
      description: |
        Thanos store will respond to Thanos Query (and other) requests for historical data. This historical data is kept in GCS buckets. This SLI monitors the Thanos StoreAPI GRPC endpoint. GRPC error responses are considered to be service-level failures.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: thanos-staging-main/thanos-staging-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/thanos-staging-main/thanos-staging-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "279093558"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(grpc_server_handled_total{environment="{{ $labels.environment }}",grpc_code!="OK",grpc_type="unary",job=~"thanos-(.+)-storegateway-(.+)",namespace="thanos-staging",stage="{{ $labels.stage }}",type="thanos"}[5m])
        )
      runbook: docs/thanos-staging/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      rules_domain: general
      severity: s3
      sli_type: error
      slo_alert: "yes"
      team: reliability_observability
      user_impacting: "no"
      window: 6h
    expr: |
      (
        (
          gitlab_component_errors:ratio_6h{component="thanos_store",env!~"gprd|ops",monitor="global",type="thanos-staging"}
          > (6 * 0.050000)
        )
        and
        (
          gitlab_component_errors:ratio_30m{component="thanos_store",env!~"gprd|ops",monitor="global",type="thanos-staging"}
          > (6 * 0.050000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_6h{component="thanos_store",env!~"gprd|ops",monitor="global",type="thanos-staging"}) >= 0.16667
      )
  - alert: ThanosStagingServiceThanosStoreTrafficCessation
    for: 5m
    annotations:
      title: The thanos_store SLI of the thanos-staging service (`{{ $labels.stage
        }}` stage) has not received any traffic in the past 30m
      description: |
        Thanos store will respond to Thanos Query (and other) requests for historical data. This historical data is kept in GCS buckets. This SLI monitors the Thanos StoreAPI GRPC endpoint. GRPC error responses are considered to be service-level failures.

        This alert signifies that the SLI is reporting a cessation of traffic; the signal is present, but is zero.
      grafana_dashboard_id: thanos-staging-main/thanos-staging-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/thanos-staging-main/thanos-staging-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1091689110"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(grpc_server_handled_total{environment="{{ $labels.environment }}",grpc_type="unary",job=~"thanos-(.+)-storegateway-(.+)",namespace="thanos-staging",stage="{{ $labels.stage }}",type="thanos"}[5m])
        )
      runbook: docs/thanos-staging/README.md
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      rules_domain: general
      severity: s3
      sli_type: ops
      slo_alert: "no"
      team: reliability_observability
      user_impacting: "no"
    expr: |
      gitlab_component_ops:rate_30m{component="thanos_store",env!~"gprd|ops",monitor="global",stage="main",type="thanos-staging"} == 0
      and
      gitlab_component_ops:rate_30m{component="thanos_store",env!~"gprd|ops",monitor="global",stage="main",type="thanos-staging"} offset 1h >= 0.16666666666666666
  - alert: ThanosStagingServiceThanosStoreTrafficAbsent
    for: 30m
    annotations:
      title: The thanos_store SLI of the thanos-staging service (`{{ $labels.stage
        }}` stage) has not reported any traffic in the past 30m
      description: |
        Thanos store will respond to Thanos Query (and other) requests for historical data. This historical data is kept in GCS buckets. This SLI monitors the Thanos StoreAPI GRPC endpoint. GRPC error responses are considered to be service-level failures.

        This alert signifies that the SLI was previously reporting traffic, but is no longer - the signal is absent.

        This could be caused by a change to the metrics used in the SLI, or by the service not receiving traffic.
      grafana_dashboard_id: thanos-staging-main/thanos-staging-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/thanos-staging-main/thanos-staging-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1091689110"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(grpc_server_handled_total{environment="{{ $labels.environment }}",grpc_type="unary",job=~"thanos-(.+)-storegateway-(.+)",namespace="thanos-staging",stage="{{ $labels.stage }}",type="thanos"}[5m])
        )
      runbook: docs/thanos-staging/README.md
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      rules_domain: general
      severity: s3
      sli_type: ops
      slo_alert: "no"
      team: reliability_observability
      user_impacting: "no"
    expr: |
      gitlab_component_ops:rate_5m{component="thanos_store",env!~"gprd|ops",monitor="global",stage="main",type="thanos-staging"} offset 1h
      unless
      gitlab_component_ops:rate_5m{component="thanos_store",env!~"gprd|ops",monitor="global",stage="main",type="thanos-staging"}
