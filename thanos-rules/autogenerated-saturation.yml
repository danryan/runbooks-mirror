# WARNING. DO NOT EDIT THIS FILE BY HAND. USE ./thanos-rules-jsonnet/saturation.jsonnet TO GENERATE IT
# YOUR CHANGES WILL BE OVERRIDDEN
groups:
- name: Saturation Rules (autogenerated)
  interval: 1m
  partial_response_strategy: warn
  rules:
  - record: gitlab_component_saturation:ratio
    labels:
      component: kube_pool_max_nodes
    expr: |
      max by(environment, tier, type, stage) (
        clamp_min(
          clamp_max(
            count by (cluster, env, environment, label_pool, tier, type, stage, shard) (
              kube_node_labels:labeled{type=~"git|mailroom|registry|sidekiq|websockets|kas|api|kube"}
            )
            / on(cluster, env, environment, label_pool) group_left() (
              label_replace(
                terraform_report_google_cluster_node_pool_max_node_count,
                "label_pool", "$0", "pool_name", ".*"
              )
              * on(cluster, env, environment) group_left()
              count by (cluster, env, environment) (
                group by (cluster, env, environment, label_topology_kubernetes_io_zone) (
                  kube_node_labels:labeled{type=~"git|mailroom|registry|sidekiq|websockets|kas|api|kube"}
                )
              )
            )
            ,
            1)
        ,
        0)
      )
- name: GitLab Component Saturation Max SLOs
  interval: 5m
  partial_response_strategy: warn
  rules:
  - record: slo:max:soft:gitlab_component_saturation:ratio
    labels:
      component: kube_pool_max_nodes
    expr: '0.9'
  - record: slo:max:hard:gitlab_component_saturation:ratio
    labels:
      component: kube_pool_max_nodes
    expr: '0.95'
- name: GitLab Component Saturation Metadata
  interval: 5m
  partial_response_strategy: warn
  rules:
  - record: gitlab_component_saturation_info
    labels:
      component: kube_pool_max_nodes
      horiz_scaling: 'yes'
      severity: s3
    expr: '1'
- name: GitLab Component Saturation Statistics
  interval: 5m
  partial_response_strategy: warn
  rules:
  - record: gitlab_component_saturation:ratio_quantile95_1w
    expr: quantile_over_time(0.95, gitlab_component_saturation:ratio{monitor="global"}[1w])
  - record: gitlab_component_saturation:ratio_quantile99_1w
    expr: quantile_over_time(0.99, gitlab_component_saturation:ratio{monitor="global"}[1w])
  - record: gitlab_component_saturation:ratio_quantile95_1h
    expr: quantile_over_time(0.95, gitlab_component_saturation:ratio{monitor="global"}[1h])
  - record: gitlab_component_saturation:ratio_quantile99_1h
    expr: quantile_over_time(0.99, gitlab_component_saturation:ratio{monitor="global"}[1h])
  - record: gitlab_component_saturation:ratio_avg_1h
    expr: avg_over_time(gitlab_component_saturation:ratio{monitor="global"}[1h])
- name: GitLab Saturation Alerts
  interval: 1m
  partial_response_strategy: warn
  rules:
  - alert: component_saturation_slo_out_of_bounds
    for: 5m
    annotations:
      title: >-
        The Kube Pool Max Node Limit resource of the {{ $labels.type }} service ({{ $labels.stage }} stage),
        component has a saturation exceeding SLO and is close to its capacity limit.
      description: |
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Kube Pool Max Node Limit resource:

        A GKE kubernetes node pool is close to it's maximum number of nodes.

        The maximum is defined in terraform, via the `max_node_count` field of a node pool. The limit is per-zone, so for single zone clusters the number of nodes will match the limit, for regional clusters, the limit is multiplied by the number of zones the cluster is deployed over.
      grafana_dashboard_id: alerts-sat_kube_pool_max_nodes
      grafana_dashboard_link: >-
        https://dashboards.gitlab.net/d/alerts-sat_kube_pool_max_nodes?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-type={{ $labels.type }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: '6'
      grafana_panel_id: '1686893332'
      grafana_variables: environment,type,stage
      promql_query: |
        max by(environment, tier, type, stage, cluster, label_pool, shard) (
          clamp_min(
            clamp_max(
              count by (cluster, env, environment, label_pool, tier, type, stage, shard) (
                kube_node_labels:labeled{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
              )
              / on(cluster, env, environment, label_pool) group_left() (
                label_replace(
                  terraform_report_google_cluster_node_pool_max_node_count,
                  "label_pool", "$0", "pool_name", ".*"
                )
                * on(cluster, env, environment) group_left()
                count by (cluster, env, environment) (
                  group by (cluster, env, environment, label_topology_kubernetes_io_zone) (
                    kube_node_labels:labeled{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
                  )
                )
              )
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
    labels:
      alert_type: cause
      rules_domain: general
      severity: s3
    expr: |
      gitlab_component_saturation:ratio{component="kube_pool_max_nodes",monitor="global"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="kube_pool_max_nodes",monitor="global"}
