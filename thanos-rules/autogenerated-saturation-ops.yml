# WARNING. DO NOT EDIT THIS FILE BY HAND. USE ./thanos-rules-jsonnet/saturation.jsonnet TO GENERATE IT
# YOUR CHANGES WILL BE OVERRIDDEN
groups:
- name: Saturation Rules (autogenerated)
  interval: 1m
  partial_response_strategy: warn
  rules:
  - record: gitlab_component_saturation:ratio
    labels:
      component: gcp_quota_limit_vertex_ai
    expr: |
      max by(env, environment, tier, type, stage, shard) (
        clamp_min(
          clamp_max(
            (
              sum without (method) (stackdriver_aiplatform_googleapis_com_location_aiplatform_googleapis_com_quota_online_prediction_requests_per_base_model_usage{env="ops",type="ai-gateway",base_model!="code-gecko"})
            /
              stackdriver_aiplatform_googleapis_com_location_aiplatform_googleapis_com_quota_online_prediction_requests_per_base_model_limit{env="ops",type="ai-gateway",base_model!="code-gecko"}
            ) > 0
            ,
            1)
        ,
        0)
      )
  - record: gitlab_component_saturation:ratio
    labels:
      component: gcp_quota_limit_vertex_ai_code_gecko
    expr: |
      max by(env, environment, tier, type, stage, shard) (
        clamp_min(
          clamp_max(
            (
              sum without (method) (stackdriver_aiplatform_googleapis_com_location_aiplatform_googleapis_com_quota_online_prediction_requests_per_base_model_usage{env="ops",type="ai-gateway",base_model="code-gecko"})
            /
              stackdriver_aiplatform_googleapis_com_location_aiplatform_googleapis_com_quota_online_prediction_requests_per_base_model_limit{env="ops",type="ai-gateway",base_model="code-gecko"}
            ) > 0
            ,
            1)
        ,
        0)
      )
  - record: gitlab_component_saturation:ratio
    labels:
      component: go_goroutines
    expr: |
      max by(env, environment, tier, type, stage, shard) (
        clamp_min(
          clamp_max(
            sum by (env, environment, tier, type, stage, shard, fqdn) (
              go_goroutines{env="ops",type=~"gitaly|monitoring|registry|web-pages"}
            )
            /
            250000
            ,
            1)
        ,
        0)
      )
  - record: gitlab_component_saturation:ratio
    labels:
      component: go_memory
    expr: |
      max by(env, environment, tier, type, stage, shard) (
        clamp_min(
          clamp_max(
            max by (env, environment, tier, type, stage, shard, fqdn) (
              go_memstats_alloc_bytes{env="ops",type=~"gitaly|monitoring|registry|web-pages"}
            )
            /
            sum by (env, environment, tier, type, stage, shard, fqdn) (
              node_memory_MemTotal_bytes{env="ops",type=~"gitaly|monitoring|registry|web-pages"}
            )
            ,
            1)
        ,
        0)
      )
  - record: gitlab_component_saturation:ratio
    labels:
      component: kube_go_memory
    expr: |
      max by(env, environment, tier, type, stage, shard) (
        clamp_min(
          clamp_max(
            go_memstats_alloc_bytes{env="ops",type=~"ai-assisted|api|atlantis|git|internal-api|kas|monitoring|ops-gitlab-net|registry|vault|web|web-pages|websockets"}
            / on(env, environment, tier, type, stage, shard, cluster, pod) group_left()
            topk by(env, environment, tier, type, stage, shard, cluster, pod) (1,
              container_spec_memory_limit_bytes:labeled{container=~"gitlab-pages|gitlab-workhorse|kas|registry|thanos-store",env="ops",type=~"ai-assisted|api|atlantis|git|internal-api|kas|monitoring|ops-gitlab-net|registry|vault|web|web-pages|websockets"}
            )
            ,
            1)
        ,
        0)
      )
  - record: gitlab_component_saturation:ratio
    labels:
      component: kube_node_ips
      stage: main
      tier: inf
      type: kube
    expr: |
      max by(env, environment, shard) (
        clamp_min(
          clamp_max(
            sum(:kube_pod_info_node_count:{env="ops"}) by (env, environment, shard, cluster)
            /
            sum(
              gitlab:gcp_subnet_max_ips{env="ops"} * on (subnet) group_right gitlab:cluster:subnet:mapping{env="ops"}
            ) by (env, environment, shard, cluster)
            ,
            1)
        ,
        0)
      )
  - record: gitlab_component_saturation:ratio
    labels:
      component: kube_pool_max_nodes
    expr: |
      max by(env, environment, tier, type, stage, shard) (
        clamp_min(
          clamp_max(
            count by (cluster, env, environment, label_pool, tier, type, stage, shard) (
              kube_node_labels:labeled{env="ops",type=~"kube|redis-cluster-cache|redis-cluster-feature-flag|redis-cluster-shared-state|redis-cluster-queues-meta|redis-cluster-repo-cache|redis-sidekiq|redis-tracechunks|redis-cluster-ratelimiting|redis-cluster-chat-cache|redis-sessions|redis-registry-cache|redis-db-load-balancing|redis-pubsub|redis|vault"}
            )
            / on(cluster, env, environment, label_pool) group_left() (
              label_replace(
                terraform_report_google_cluster_node_pool_max_node_count,
                "label_pool", "$0", "pool_name", ".*"
              )
              * on(cluster, env, environment) group_left()
              count by (cluster, env, environment) (
                group by (cluster, env, environment, label_topology_kubernetes_io_zone) (
                  kube_node_labels:labeled{env="ops",type=~"kube|redis-cluster-cache|redis-cluster-feature-flag|redis-cluster-shared-state|redis-cluster-queues-meta|redis-cluster-repo-cache|redis-sidekiq|redis-tracechunks|redis-cluster-ratelimiting|redis-cluster-chat-cache|redis-sessions|redis-registry-cache|redis-db-load-balancing|redis-pubsub|redis|vault"}
                )
              )
            )
            ,
            1)
        ,
        0)
      )
  - record: gitlab_component_saturation:ratio
    labels:
      component: max_concurrent_inferences
    expr: |
      max by(env, environment, tier, type, stage, shard, model_engine, model_name) (
        clamp_min(
          clamp_max(
            sum by (env, environment, tier, type, stage, shard, model_engine, model_name)(max_over_time(model_inferences_in_flight{env="ops",type="ai-gateway"}[10m]))
            /
            min by (env, environment, tier, type, stage, shard, model_engine, model_name)(min_over_time(model_inferences_max_concurrent{env="ops",type="ai-gateway"}[10m]))
            ,
            1)
        ,
        0)
      )
  - record: gitlab_component_saturation:ratio
    labels:
      component: max_concurrent_inferences_per_engine
    expr: |
      max by(env, environment, tier, type, stage, shard) (
        clamp_min(
          clamp_max(
            sum by (env, environment, tier, type, stage, shard, model_engine)(max_over_time(model_inferences_in_flight{env="ops",type="ai-gateway"}[10m]))
            /
            max by (env, environment, tier, type, stage, shard, model_engine)(min_over_time(model_inferences_max_concurrent{env="ops",type="ai-gateway"}[10m]))
            ,
            1)
        ,
        0)
      )
  - record: gitlab_component_saturation:ratio
    labels:
      component: open_fds
    expr: |
      max by(env, environment, tier, type, stage, shard) (
        clamp_min(
          clamp_max(
            (
              process_open_fds{env="ops",type=~"ai-gateway|errortracking|ext-pvs|glgo|tracing"}
              /
              process_max_fds{env="ops",type=~"ai-gateway|errortracking|ext-pvs|glgo|tracing"}
            )
            or
            (
              ruby_file_descriptors{env="ops",type=~"ai-gateway|errortracking|ext-pvs|glgo|tracing"}
              /
              ruby_process_max_fds{env="ops",type=~"ai-gateway|errortracking|ext-pvs|glgo|tracing"}
            )
            ,
            1)
        ,
        0)
      )
  - record: gitlab_component_saturation:ratio
    labels:
      component: pg_primary_cpu
    expr: |
      max by(env, environment, tier, type, stage, shard) (
        clamp_min(
          clamp_max(
            avg without(cpu, mode) (
              1
              -
              (
                rate(node_cpu_seconds_total{mode="idle", env="ops",type=~"patroni|patroni-registry|patroni-embedding|patroni-ci|postgres-archive|sentry"}[5m])
                and on(fqdn)
                pg_replication_is_replica{env="ops",type=~"patroni|patroni-registry|patroni-embedding|patroni-ci|postgres-archive|sentry"} == 0
              )
            )
            ,
            1)
        ,
        0)
      )
  - record: gitlab_component_saturation:ratio
    labels:
      component: pg_vacuum_activity_v2
    expr: |
      max by(env, environment, tier, type, stage, shard) (
        clamp_min(
          clamp_max(
            sum by (env, environment, tier, type, stage, shard) (
              rate(fluentd_pg_auto_vacuum_elapsed_seconds_total{env="ops",type=~"patroni|patroni-registry|patroni-embedding|patroni-ci"}[1d])
              and on (fqdn) (pg_replication_is_replica{env="ops",type=~"patroni|patroni-registry|patroni-embedding|patroni-ci"} == 0)
            )
            /
            avg by (env, environment, tier, type, stage, shard) (
              pg_settings_autovacuum_max_workers{env="ops",type=~"patroni|patroni-registry|patroni-embedding|patroni-ci"}
              and on (instance, job) (pg_replication_is_replica{env="ops",type=~"patroni|patroni-registry|patroni-embedding|patroni-ci"} == 0)
            )
            ,
            1)
        ,
        0)
      )
  - record: gitlab_component_saturation:ratio
    labels:
      component: pg_walsender_cpu
    expr: |
      max by(env, environment, tier, type, stage, shard) (
        clamp_min(
          clamp_max(
            sum by (env, environment, tier, type, stage, shard) (
              sum by(env, environment, tier, type, stage, shard, fqdn) (
                rate(namedprocess_namegroup_cpu_seconds_total{env="ops",type=~"patroni|patroni-registry|patroni-embedding|patroni-ci", groupname=~"pg.worker.walsender|pg.worker.walwriter|wal-g"}[5m])
                and on (fqdn) (pg_replication_is_replica{env="ops",type=~"patroni|patroni-registry|patroni-embedding|patroni-ci"} == 0)
              )
              /
              count by (env, environment, tier, type, stage, shard, fqdn) (
                node_cpu_seconds_total{env="ops",type=~"patroni|patroni-registry|patroni-embedding|patroni-ci", mode="idle"} and on(fqdn) (pg_replication_is_replica{env="ops",type=~"patroni|patroni-registry|patroni-embedding|patroni-ci"} == 0)
              )
            )
            ,
            1)
        ,
        0)
      )
  - record: gitlab_component_saturation:ratio
    labels:
      component: runway_container_cpu_utilization
      stage: main
      tier: inf
    expr: |
      max by(env, environment, type, shard) (
        clamp_min(
          clamp_max(
            histogram_quantile(0.9999, sum by(le, env, environment, type, shard, revision_name, region, location)
              (
                avg_over_time(
                  stackdriver_cloud_run_revision_run_googleapis_com_container_cpu_utilizations_bucket{job="runway-exporter",env="ops",type=~"ai-gateway|ext-pvs|glgo"}[30m]
                )
              )
            )
            ,
            1)
        ,
        0)
      )
  - record: gitlab_component_saturation:ratio
    labels:
      component: runway_container_instance_utilization
      stage: main
      tier: inf
    expr: |
      max by(env, environment, type, shard) (
        clamp_min(
          clamp_max(
            sum by (env, environment, type, shard, revision_name, region, location) (
              stackdriver_cloud_run_revision_run_googleapis_com_container_instance_count{job="runway-exporter",state="active",env="ops",type=~"ai-gateway|ext-pvs|glgo"}
            )
            /
            100
            ,
            1)
        ,
        0)
      )
  - record: gitlab_component_saturation:ratio
    labels:
      component: runway_container_max_concurrent_requests
      stage: main
      tier: inf
    expr: |
      max by(env, environment, type, shard) (
        clamp_min(
          clamp_max(
            histogram_quantile(0.9999, sum by(le, env, environment, type, shard, revision_name, region, location)
              (
                rate(
                  stackdriver_cloud_run_revision_run_googleapis_com_container_max_request_concurrencies_bucket{job="runway-exporter",state="active",env="ops",type=~"ai-gateway|ext-pvs|glgo"}[30m]
                )
              )
            ) / 100
            ,
            1)
        ,
        0)
      )
  - record: gitlab_component_saturation:ratio
    labels:
      component: runway_container_memory_utilization
      stage: main
      tier: inf
    expr: |
      max by(env, environment, type, shard) (
        clamp_min(
          clamp_max(
            histogram_quantile(0.9999, sum by(le, env, environment, type, shard, revision_name, region)
              (
                avg_over_time(
                  stackdriver_cloud_run_revision_run_googleapis_com_container_memory_utilizations_bucket{job="runway-exporter",env="ops",type=~"ai-gateway|ext-pvs|glgo"}[30m]
                )
              )
            )
            ,
            1)
        ,
        0)
      )
- name: GitLab Component Saturation Statistics
  interval: 5m
  partial_response_strategy: warn
  rules:
  - record: gitlab_component_saturation:ratio_quantile95_1w
    expr: quantile_over_time(0.95, gitlab_component_saturation:ratio{env="ops",monitor="global"}[1w])
  - record: gitlab_component_saturation:ratio_quantile99_1w
    expr: quantile_over_time(0.99, gitlab_component_saturation:ratio{env="ops",monitor="global"}[1w])
  - record: gitlab_component_saturation:ratio_quantile95_1h
    expr: quantile_over_time(0.95, gitlab_component_saturation:ratio{env="ops",monitor="global"}[1h])
  - record: gitlab_component_saturation:ratio_quantile99_1h
    expr: quantile_over_time(0.99, gitlab_component_saturation:ratio{env="ops",monitor="global"}[1h])
  - record: gitlab_component_saturation:ratio_avg_1h
    expr: avg_over_time(gitlab_component_saturation:ratio{env="ops",monitor="global"}[1h])
- name: GitLab Saturation Alerts
  interval: 1m
  partial_response_strategy: warn
  rules:
  - alert: component_saturation_slo_out_of_bounds:gcp_quota_limit_vertex_ai
    for: 15m
    annotations:
      title: The GCP Quota utilization per environment resource of the {{ $labels.type
        }} service ({{ $labels.stage }} stage) has a saturation exceeding SLO and
        is close to its capacity limit.
      description: |
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the GCP Quota utilization per environment resource:

        GCP Quota utilization / limit ratio for all vertex AI models except code-gecko.

        Saturation on the quota may cause problems with the requests.

        To fix, we can request a quota increase for the specific resource to the GCP support team.
      grafana_dashboard_id: alerts-sat_gcp_quota_limit_vertex_ai
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_gcp_quota_limit_vertex_ai?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-type={{ $labels.type }}&var-stage={{ $labels.stage
        }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1515902021"
      grafana_variables: environment,type,stage
      promql_query: |
        max by(environment, tier, type, stage, shard, base_model, region, location) (
          clamp_min(
            clamp_max(
              (
                sum without (method) (stackdriver_aiplatform_googleapis_com_location_aiplatform_googleapis_com_quota_online_prediction_requests_per_base_model_usage{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}",base_model!="code-gecko"})
              /
                stackdriver_aiplatform_googleapis_com_location_aiplatform_googleapis_com_quota_online_prediction_requests_per_base_model_limit{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}",base_model!="code-gecko"}
              ) > 0
              ,
              1)
          ,
          0)
        )
      promql_template_1: |
        max by(environment, tier, type, stage, shard, base_model, region, location) (
          clamp_min(
            clamp_max(
              (
                sum without (method) (stackdriver_aiplatform_googleapis_com_location_aiplatform_googleapis_com_quota_online_prediction_requests_per_base_model_usage{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}",base_model!="code-gecko"})
              /
                stackdriver_aiplatform_googleapis_com_location_aiplatform_googleapis_com_quota_online_prediction_requests_per_base_model_limit{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}",base_model!="code-gecko"}
              ) > 0
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
    labels:
      alert_type: cause
      rules_domain: general
      severity: s4
    expr: |
      gitlab_component_saturation:ratio{component="gcp_quota_limit_vertex_ai",env="ops",monitor="global"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="gcp_quota_limit_vertex_ai",env="ops",monitor="global"}
  - alert: component_saturation_slo_out_of_bounds:gcp_quota_limit_vertex_ai_code_gecko
    for: 15m
    annotations:
      title: The GCP Quota utilization per environment resource of the {{ $labels.type
        }} service ({{ $labels.stage }} stage) has a saturation exceeding SLO and
        is close to its capacity limit.
      description: |
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the GCP Quota utilization per environment resource:

        GCP Quota utilization / limit ratio for Vertex AI for code-gecko model (used by code completion part of Code Suggestions)

        Saturation on the quota may cause problems with code completion requests.

        To fix, we can request a quota increase for the specific resource to the GCP support team.
      grafana_dashboard_id: alerts-sat_vertex_ai_code_gecko_quota
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_vertex_ai_code_gecko_quota?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-type={{ $labels.type }}&var-stage={{ $labels.stage
        }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3658506870"
      grafana_variables: environment,type,stage
      promql_query: |
        max by(environment, tier, type, stage, shard, base_model, region, location) (
          clamp_min(
            clamp_max(
              (
                sum without (method) (stackdriver_aiplatform_googleapis_com_location_aiplatform_googleapis_com_quota_online_prediction_requests_per_base_model_usage{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}",base_model="code-gecko"})
              /
                stackdriver_aiplatform_googleapis_com_location_aiplatform_googleapis_com_quota_online_prediction_requests_per_base_model_limit{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}",base_model="code-gecko"}
              ) > 0
              ,
              1)
          ,
          0)
        )
      promql_template_1: |
        max by(environment, tier, type, stage, shard, base_model, region, location) (
          clamp_min(
            clamp_max(
              (
                sum without (method) (stackdriver_aiplatform_googleapis_com_location_aiplatform_googleapis_com_quota_online_prediction_requests_per_base_model_usage{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}",base_model="code-gecko"})
              /
                stackdriver_aiplatform_googleapis_com_location_aiplatform_googleapis_com_quota_online_prediction_requests_per_base_model_limit{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}",base_model="code-gecko"}
              ) > 0
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
    labels:
      alert_type: cause
      rules_domain: general
      severity: s4
    expr: |
      gitlab_component_saturation:ratio{component="gcp_quota_limit_vertex_ai_code_gecko",env="ops",monitor="global"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="gcp_quota_limit_vertex_ai_code_gecko",env="ops",monitor="global"}
  - alert: component_saturation_slo_out_of_bounds:go_goroutines
    for: 5m
    annotations:
      title: The Go goroutines Utilization per Node resource of the {{ $labels.type
        }} service ({{ $labels.stage }} stage) has a saturation exceeding SLO and
        is close to its capacity limit.
      description: |
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Go goroutines Utilization per Node resource:

        Go goroutines utilization per node.

        Goroutines leaks can cause memory saturation which can cause service degradation.

        A limit of 250k goroutines is very generous, so if a service exceeds this limit, it's a sign of a leak and it should be dealt with.
      grafana_dashboard_id: alerts-sat_go_goroutines
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_go_goroutines?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-type={{ $labels.type }}&var-stage={{ $labels.stage
        }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3712788301"
      grafana_variables: environment,type,stage
      promql_query: |
        max by(env, environment, tier, type, stage, shard, fqdn) (
          clamp_min(
            clamp_max(
              sum by (env, environment, tier, type, stage, shard, fqdn) (
                go_goroutines{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
              )
              /
              250000
              ,
              1)
          ,
          0)
        )
      promql_template_1: |
        max by(env, environment, tier, type, stage, shard, fqdn) (
          clamp_min(
            clamp_max(
              sum by (env, environment, tier, type, stage, shard, fqdn) (
                go_goroutines{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
              )
              /
              250000
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
    labels:
      alert_type: cause
      pager: pagerduty
      rules_domain: general
      severity: s2
    expr: |
      gitlab_component_saturation:ratio{component="go_goroutines",env="ops",monitor="global"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="go_goroutines",env="ops",monitor="global"}
  - alert: component_saturation_slo_out_of_bounds:go_memory
    for: 5m
    annotations:
      title: The Go Memory Utilization per Node resource of the {{ $labels.type }}
        service ({{ $labels.stage }} stage) has a saturation exceeding SLO and is
        close to its capacity limit.
      description: |
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Go Memory Utilization per Node resource:

        Go's memory allocation strategy can make it look like a Go process is saturating memory when measured using RSS, when in fact the process is not at risk of memory saturation. For this reason, we measure Go processes using the `go_memstat_alloc_bytes`
      grafana_dashboard_id: alerts-sat_go_memory
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_go_memory?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-type={{ $labels.type }}&var-stage={{ $labels.stage
        }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3631721613"
      grafana_variables: environment,type,stage
      promql_query: |
        max by(env, environment, tier, type, stage, shard, fqdn) (
          clamp_min(
            clamp_max(
              max by (env, environment, tier, type, stage, shard, fqdn) (
                go_memstats_alloc_bytes{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
              )
              /
              sum by (env, environment, tier, type, stage, shard, fqdn) (
                node_memory_MemTotal_bytes{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
              )
              ,
              1)
          ,
          0)
        )
      promql_template_1: |
        max by(env, environment, tier, type, stage, shard, fqdn) (
          clamp_min(
            clamp_max(
              max by (env, environment, tier, type, stage, shard, fqdn) (
                go_memstats_alloc_bytes{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
              )
              /
              sum by (env, environment, tier, type, stage, shard, fqdn) (
                node_memory_MemTotal_bytes{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
              )
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
    labels:
      alert_type: cause
      rules_domain: general
      severity: s4
    expr: |
      gitlab_component_saturation:ratio{component="go_memory",env="ops",monitor="global"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="go_memory",env="ops",monitor="global"}
  - alert: component_saturation_slo_out_of_bounds:kube_go_memory
    for: 5m
    annotations:
      title: The Go Memory Utilization per Node resource of the {{ $labels.type }}
        service ({{ $labels.stage }} stage) has a saturation exceeding SLO and is
        close to its capacity limit.
      description: |
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Go Memory Utilization per Node resource:

        Measures Go memory usage as a percentage of container memory limit
      grafana_dashboard_id: alerts-sat_kube_go_memory
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_kube_go_memory?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-type={{ $labels.type }}&var-stage={{ $labels.stage
        }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "4163523952"
      grafana_variables: environment,type,stage
      promql_query: |
        max by(env, environment, tier, type, stage, shard, cluster, pod) (
          clamp_min(
            clamp_max(
              go_memstats_alloc_bytes{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
              / on(env, environment, tier, type, stage, shard, cluster, pod) group_left()
              topk by(env, environment, tier, type, stage, shard, cluster, pod) (1,
                container_spec_memory_limit_bytes:labeled{container=~"gitlab-pages|gitlab-workhorse|kas|registry|thanos-store",environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
              )
              ,
              1)
          ,
          0)
        )
      promql_template_1: |
        max by(env, environment, tier, type, stage, shard, cluster, pod) (
          clamp_min(
            clamp_max(
              go_memstats_alloc_bytes{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
              / on(env, environment, tier, type, stage, shard, cluster, pod) group_left()
              topk by(env, environment, tier, type, stage, shard, cluster, pod) (1,
                container_spec_memory_limit_bytes:labeled{container=~"gitlab-pages|gitlab-workhorse|kas|registry|thanos-store",environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
              )
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
    labels:
      alert_type: cause
      rules_domain: general
      severity: s4
    expr: |
      gitlab_component_saturation:ratio{component="kube_go_memory",env="ops",monitor="global"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="kube_go_memory",env="ops",monitor="global"}
  - alert: component_saturation_slo_out_of_bounds:kube_node_ips
    for: 5m
    annotations:
      title: The Node IP subnet saturation resource of the {{ $labels.type }} service
        ({{ $labels.stage }} stage) has a saturation exceeding SLO and is close to
        its capacity limit.
      description: |
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Node IP subnet saturation resource:

        This resource measures the number of nodes per subnet.

        If it is becoming saturated, it may indicate that clusters need to be rebuilt with a larger subnet.
      grafana_dashboard_id: alerts-sat_kube_node_ips
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_kube_node_ips?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-type={{ $labels.type }}&var-stage={{ $labels.stage
        }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "776071338"
      grafana_variables: environment,type,stage
      promql_query: |
        max by(env, environment, shard, cluster) (
          clamp_min(
            clamp_max(
              sum(:kube_pod_info_node_count:{environment="{{ $labels.environment }}"}) by (env, environment, shard, cluster)
              /
              sum(
                gitlab:gcp_subnet_max_ips{environment="{{ $labels.environment }}"} * on (subnet) group_right gitlab:cluster:subnet:mapping{environment="{{ $labels.environment }}"}
              ) by (env, environment, shard, cluster)
              ,
              1)
          ,
          0)
        )
      promql_template_1: |
        max by(env, environment, shard, cluster) (
          clamp_min(
            clamp_max(
              sum(:kube_pod_info_node_count:{environment="{{ $labels.environment }}"}) by (env, environment, shard, cluster)
              /
              sum(
                gitlab:gcp_subnet_max_ips{environment="{{ $labels.environment }}"} * on (subnet) group_right gitlab:cluster:subnet:mapping{environment="{{ $labels.environment }}"}
              ) by (env, environment, shard, cluster)
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
    labels:
      alert_type: cause
      rules_domain: general
      severity: s3
    expr: |
      gitlab_component_saturation:ratio{component="kube_node_ips",env="ops",monitor="global"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="kube_node_ips",env="ops",monitor="global"}
  - alert: component_saturation_slo_out_of_bounds:kube_pool_max_nodes
    for: 5m
    annotations:
      title: The Kube Pool Max Node Limit resource of the {{ $labels.type }} service
        ({{ $labels.stage }} stage) has a saturation exceeding SLO and is close to
        its capacity limit.
      description: |
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Kube Pool Max Node Limit resource:

        A GKE kubernetes node pool is close to it's maximum number of nodes.

        The maximum is defined in terraform, via the `max_node_count` field of a node pool. The limit is per-zone, so for single zone clusters the number of nodes will match the limit, for regional clusters, the limit is multiplied by the number of zones the cluster is deployed over.
      grafana_dashboard_id: alerts-sat_kube_pool_max_nodes
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_kube_pool_max_nodes?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-type={{ $labels.type }}&var-stage={{ $labels.stage
        }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1686893332"
      grafana_variables: environment,type,stage
      promql_query: |
        max by(env, environment, tier, type, stage, shard, cluster, label_pool, shard) (
          clamp_min(
            clamp_max(
              count by (cluster, env, environment, label_pool, tier, type, stage, shard) (
                kube_node_labels:labeled{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
              )
              / on(cluster, env, environment, label_pool) group_left() (
                label_replace(
                  terraform_report_google_cluster_node_pool_max_node_count,
                  "label_pool", "$0", "pool_name", ".*"
                )
                * on(cluster, env, environment) group_left()
                count by (cluster, env, environment) (
                  group by (cluster, env, environment, label_topology_kubernetes_io_zone) (
                    kube_node_labels:labeled{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
                  )
                )
              )
              ,
              1)
          ,
          0)
        )
      promql_template_1: |
        max by(env, environment, tier, type, stage, shard, cluster, label_pool, shard) (
          clamp_min(
            clamp_max(
              count by (cluster, env, environment, label_pool, tier, type, stage, shard) (
                kube_node_labels:labeled{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
              )
              / on(cluster, env, environment, label_pool) group_left() (
                label_replace(
                  terraform_report_google_cluster_node_pool_max_node_count,
                  "label_pool", "$0", "pool_name", ".*"
                )
                * on(cluster, env, environment) group_left()
                count by (cluster, env, environment) (
                  group by (cluster, env, environment, label_topology_kubernetes_io_zone) (
                    kube_node_labels:labeled{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
                  )
                )
              )
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
    labels:
      alert_type: cause
      rules_domain: general
      severity: s3
    expr: |
      gitlab_component_saturation:ratio{component="kube_pool_max_nodes",env="ops",monitor="global"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="kube_pool_max_nodes",env="ops",monitor="global"}
  - alert: component_saturation_slo_out_of_bounds:max_concurrent_inferences
    for: 5m
    annotations:
      title: The Maximum number of concurrent inferences to a large language model
        resource of the {{ $labels.type }} service ({{ $labels.stage }} stage) has
        a saturation exceeding SLO and is close to its capacity limit.
      description: |
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Maximum number of concurrent inferences to a large language model resource:

        The maximum number of inferences (requests) we can concurrently make to a single LLM.

        Anthropic is enforcing different concurrency limits per model they provide. When we make a new request when we already have the maximum number of concurrent requests in flight, Anthropic responds with a 429. The client-library in the AI-gateway is set to retry once on errors.

        When Anthropic rejects the requests, this leads to a 500 error in the AI-gateway and Workhorse. This results in clients not getting a response (code suggestion).

        To fix this, we need to request a larger concurrency to Anthropic. Currently in the `#ext-anthropic` slack channel.

        Bear in mind that this metric is sampled at scrape time. So it is only an approximation of the actual number of requests in flight. We should assume the actual utilization is higher and request increases sooner.
      grafana_dashboard_id: alerts-max_concurrent_inferences
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-max_concurrent_inferences?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-type={{ $labels.type }}&var-stage={{ $labels.stage
        }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "621984338"
      grafana_variables: environment,type,stage
      promql_query: |
        max by(environment, tier, type, stage, shard, model_engine, model_name) (
          clamp_min(
            clamp_max(
              sum by (environment, tier, type, stage, shard, model_engine, model_name)(max_over_time(model_inferences_in_flight{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[10m]))
              /
              min by (environment, tier, type, stage, shard, model_engine, model_name)(min_over_time(model_inferences_max_concurrent{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[10m]))
              ,
              1)
          ,
          0)
        )
      promql_template_1: |
        max by(environment, tier, type, stage, shard, model_engine, model_name) (
          clamp_min(
            clamp_max(
              sum by (environment, tier, type, stage, shard, model_engine, model_name)(max_over_time(model_inferences_in_flight{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[10m]))
              /
              min by (environment, tier, type, stage, shard, model_engine, model_name)(min_over_time(model_inferences_max_concurrent{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[10m]))
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
    labels:
      alert_type: cause
      rules_domain: general
      severity: s3
    expr: |
      gitlab_component_saturation:ratio{component="max_concurrent_inferences",env="ops",monitor="global"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="max_concurrent_inferences",env="ops",monitor="global"}
  - alert: component_saturation_slo_out_of_bounds:max_concurrent_inferences_per_engine
    for: 5m
    annotations:
      title: The Maximum number of concurrent inferences to a large language model
        resource of the {{ $labels.type }} service ({{ $labels.stage }} stage) has
        a saturation exceeding SLO and is close to its capacity limit.
      description: |
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Maximum number of concurrent inferences to a large language model resource:

        The maximum number of inferences (requests) we can concurrently make to a all models for a single provider (engine).

        Anthropic is enforcing different concurrency limits per model they provide. But across all models, we can not exceed the global limit that is equal to the largest allowed limit.

        When Anthropic rejects the requests, this leads to a 500 error in the AI-gateway and Workhorse. This results in clients not getting a response (code suggestion).

        To fix this, we need to request a larger concurrency to Anthropic. Currently in the `#ext-anthropic` slack channel.

        Bear in mind that this metric is sampled at scrape time. So it is only an approximation of the actual number of requests in flight. We should assume the actual utilization is higher and request increases sooner.
      grafana_dashboard_id: alerts-max_inferences_per_engine
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-max_inferences_per_engine?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-type={{ $labels.type }}&var-stage={{ $labels.stage
        }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2029158470"
      grafana_variables: environment,type,stage
      promql_query: |
        max by(environment, tier, type, stage, shard, model_engine) (
          clamp_min(
            clamp_max(
              sum by (environment, tier, type, stage, shard, model_engine)(max_over_time(model_inferences_in_flight{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[10m]))
              /
              max by (environment, tier, type, stage, shard, model_engine)(min_over_time(model_inferences_max_concurrent{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[10m]))
              ,
              1)
          ,
          0)
        )
      promql_template_1: |
        max by(environment, tier, type, stage, shard, model_engine) (
          clamp_min(
            clamp_max(
              sum by (environment, tier, type, stage, shard, model_engine)(max_over_time(model_inferences_in_flight{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[10m]))
              /
              max by (environment, tier, type, stage, shard, model_engine)(min_over_time(model_inferences_max_concurrent{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[10m]))
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
    labels:
      alert_type: cause
      rules_domain: general
      severity: s3
    expr: |
      gitlab_component_saturation:ratio{component="max_concurrent_inferences_per_engine",env="ops",monitor="global"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="max_concurrent_inferences_per_engine",env="ops",monitor="global"}
  - alert: component_saturation_slo_out_of_bounds:open_fds
    for: 5m
    annotations:
      title: The Open file descriptor utilization per instance resource of the {{
        $labels.type }} service ({{ $labels.stage }} stage) has a saturation exceeding
        SLO and is close to its capacity limit.
      description: |
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Open file descriptor utilization per instance resource:

        Open file descriptor utilization per instance.

        Saturation on file descriptor limits may indicate a resource-descriptor leak in the application.

        As a temporary fix, you may want to consider restarting the affected process.
      grafana_dashboard_id: alerts-sat_open_fds
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_open_fds?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-type={{ $labels.type }}&var-stage={{ $labels.stage
        }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1001792825"
      grafana_variables: environment,type,stage
      promql_query: |
        max by(environment, tier, type, stage, shard, job, instance) (
          clamp_min(
            clamp_max(
              (
                process_open_fds{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
                /
                process_max_fds{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
              )
              or
              (
                ruby_file_descriptors{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
                /
                ruby_process_max_fds{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
              )
              ,
              1)
          ,
          0)
        )
      promql_template_1: |
        max by(environment, tier, type, stage, shard, job, instance) (
          clamp_min(
            clamp_max(
              (
                process_open_fds{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
                /
                process_max_fds{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
              )
              or
              (
                ruby_file_descriptors{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
                /
                ruby_process_max_fds{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
              )
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
    labels:
      alert_type: cause
      pager: pagerduty
      rules_domain: general
      severity: s2
    expr: |
      gitlab_component_saturation:ratio{component="open_fds",env="ops",monitor="global"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="open_fds",env="ops",monitor="global"}
  - alert: component_saturation_slo_out_of_bounds:pg_primary_cpu
    for: 5m
    annotations:
      title: The Average CPU Utilization on Postgres Primary Instance resource of
        the {{ $labels.type }} service ({{ $labels.stage }} stage) has a saturation
        exceeding SLO and is close to its capacity limit.
      description: |
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Average CPU Utilization on Postgres Primary Instance resource:

        Average CPU utilization across all cores on the Postgres primary instance.
      grafana_dashboard_id: alerts-sat_pg_primary_cpu
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_pg_primary_cpu?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-type={{ $labels.type }}&var-stage={{ $labels.stage
        }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3989464622"
      grafana_variables: environment,type,stage
      promql_query: |
        max by(env, environment, tier, type, stage, shard, fqdn) (
          clamp_min(
            clamp_max(
              avg without(cpu, mode) (
                1
                -
                (
                  rate(node_cpu_seconds_total{mode="idle", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[5m])
                  and on(fqdn)
                  pg_replication_is_replica{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"} == 0
                )
              )
              ,
              1)
          ,
          0)
        )
      promql_template_1: |
        max by(env, environment, tier, type, stage, shard, fqdn) (
          clamp_min(
            clamp_max(
              avg without(cpu, mode) (
                1
                -
                (
                  rate(node_cpu_seconds_total{mode="idle", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[5m])
                  and on(fqdn)
                  pg_replication_is_replica{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"} == 0
                )
              )
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
    labels:
      alert_type: cause
      pager: pagerduty
      rules_domain: general
      severity: s2
    expr: |
      gitlab_component_saturation:ratio{component="pg_primary_cpu",env="ops",monitor="global"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="pg_primary_cpu",env="ops",monitor="global"}
  - alert: component_saturation_slo_out_of_bounds:pg_vacuum_activity_v2
    for: 5m
    annotations:
      title: The Postgres Autovacuum Activity (non-sampled) resource of the {{ $labels.type
        }} service ({{ $labels.stage }} stage) has a saturation exceeding SLO and
        is close to its capacity limit.
      description: |
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Postgres Autovacuum Activity (non-sampled) resource:

        This measures the total amount of time spent each day by autovacuum workers, as a percentage of total autovacuum capacity.

        This resource uses the `auto_vacuum_elapsed_seconds` value logged by the autovacuum worker, and aggregates this across all autovacuum jobs. In the case that there are 10 autovacuum workers, the total capacity is 10-days worth of autovacuum time per day.

        Once the system is performing 10 days worth of autovacuum per day, the capacity will be saturated.

        This resource is primarily intended to be used for long-term capacity planning.
      grafana_dashboard_id: alerts-sat_pg_vacuum_activity_v2
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_pg_vacuum_activity_v2?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-type={{ $labels.type }}&var-stage={{ $labels.stage
        }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3412018865"
      grafana_variables: environment,type,stage
      promql_query: |
        max by(env, environment, tier, type, stage, shard) (
          clamp_min(
            clamp_max(
              sum by (env, environment, tier, type, stage, shard) (
                rate(fluentd_pg_auto_vacuum_elapsed_seconds_total{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[1d])
                and on (fqdn) (pg_replication_is_replica{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"} == 0)
              )
              /
              avg by (env, environment, tier, type, stage, shard) (
                pg_settings_autovacuum_max_workers{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
                and on (instance, job) (pg_replication_is_replica{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"} == 0)
              )
              ,
              1)
          ,
          0)
        )
      promql_template_1: |
        max by(env, environment, tier, type, stage, shard) (
          clamp_min(
            clamp_max(
              sum by (env, environment, tier, type, stage, shard) (
                rate(fluentd_pg_auto_vacuum_elapsed_seconds_total{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[1d])
                and on (fqdn) (pg_replication_is_replica{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"} == 0)
              )
              /
              avg by (env, environment, tier, type, stage, shard) (
                pg_settings_autovacuum_max_workers{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
                and on (instance, job) (pg_replication_is_replica{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"} == 0)
              )
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
    labels:
      alert_type: cause
      rules_domain: general
      severity: s3
    expr: |
      gitlab_component_saturation:ratio{component="pg_vacuum_activity_v2",env="ops",monitor="global"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="pg_vacuum_activity_v2",env="ops",monitor="global"}
  - alert: component_saturation_slo_out_of_bounds:pg_walsender_cpu
    for: 5m
    annotations:
      title: The Walsender CPU Saturation resource of the {{ $labels.type }} service
        ({{ $labels.stage }} stage) has a saturation exceeding SLO and is close to
        its capacity limit.
      description: |
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Walsender CPU Saturation resource:

        This saturation metric measures the total amount of time that the primary postgres instance is spending sending WAL segments to replicas. It is expressed as a percentage of all CPU available on the primary postgres instance.

        The more replicas connected, the higher this metric will be.

        Since it's expressed as a percentage of all CPU, this should always remain low, since the CPU primarily needs to be available for handling SQL statements.
      grafana_dashboard_id: alerts-sat_pg_walsender_cpu
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_pg_walsender_cpu?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-type={{ $labels.type }}&var-stage={{ $labels.stage
        }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1879384722"
      grafana_variables: environment,type,stage
      promql_query: |
        max by(env, environment, tier, type, stage, shard) (
          clamp_min(
            clamp_max(
              sum by (env, environment, tier, type, stage, shard) (
                sum by(env, environment, tier, type, stage, shard, fqdn) (
                  rate(namedprocess_namegroup_cpu_seconds_total{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}", groupname=~"pg.worker.walsender|pg.worker.walwriter|wal-g"}[5m])
                  and on (fqdn) (pg_replication_is_replica{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"} == 0)
                )
                /
                count by (env, environment, tier, type, stage, shard, fqdn) (
                  node_cpu_seconds_total{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}", mode="idle"} and on(fqdn) (pg_replication_is_replica{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"} == 0)
                )
              )
              ,
              1)
          ,
          0)
        )
      promql_template_1: |
        max by(env, environment, tier, type, stage, shard) (
          clamp_min(
            clamp_max(
              sum by (env, environment, tier, type, stage, shard) (
                sum by(env, environment, tier, type, stage, shard, fqdn) (
                  rate(namedprocess_namegroup_cpu_seconds_total{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}", groupname=~"pg.worker.walsender|pg.worker.walwriter|wal-g"}[5m])
                  and on (fqdn) (pg_replication_is_replica{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"} == 0)
                )
                /
                count by (env, environment, tier, type, stage, shard, fqdn) (
                  node_cpu_seconds_total{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}", mode="idle"} and on(fqdn) (pg_replication_is_replica{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"} == 0)
                )
              )
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
    labels:
      alert_type: cause
      rules_domain: general
      severity: s3
    expr: |
      gitlab_component_saturation:ratio{component="pg_walsender_cpu",env="ops",monitor="global"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="pg_walsender_cpu",env="ops",monitor="global"}
  - alert: component_saturation_slo_out_of_bounds:runway_container_cpu_utilization
    for: 5m
    annotations:
      title: The Runway Container CPU Utilization resource of the {{ $labels.type
        }} service ({{ $labels.stage }} stage) has a saturation exceeding SLO and
        is close to its capacity limit.
      description: |
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Runway Container CPU Utilization resource:

        Container CPU utilization of the Runway service distributed across all container instances.

        For scaling, refer to https://cloud.google.com/run/docs/configuring/services/cpu.
      grafana_dashboard_id: alerts-sat_runway_container_cpu
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_runway_container_cpu?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-type={{ $labels.type }}&var-stage={{ $labels.stage
        }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1050857443"
      grafana_variables: environment,type,stage
      promql_query: |
        max by(environment, type, shard, revision_name, region, location) (
          clamp_min(
            clamp_max(
              histogram_quantile(0.9999, sum by(le, environment, type, shard, revision_name, region, location)
                (
                  avg_over_time(
                    stackdriver_cloud_run_revision_run_googleapis_com_container_cpu_utilizations_bucket{job="runway-exporter",environment="{{ $labels.environment }}",type="{{ $labels.type }}"}[30m]
                  )
                )
              )
              ,
              1)
          ,
          0)
        )
      promql_template_1: |
        max by(environment, type, shard, revision_name, region, location) (
          clamp_min(
            clamp_max(
              histogram_quantile(0.9999, sum by(le, environment, type, shard, revision_name, region, location)
                (
                  avg_over_time(
                    stackdriver_cloud_run_revision_run_googleapis_com_container_cpu_utilizations_bucket{job="runway-exporter",environment="{{ $labels.environment }}",type="{{ $labels.type }}"}[30m]
                  )
                )
              )
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
    labels:
      alert_type: cause
      rules_domain: general
      severity: s4
    expr: |
      gitlab_component_saturation:ratio{component="runway_container_cpu_utilization",env="ops",monitor="global"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="runway_container_cpu_utilization",env="ops",monitor="global"}
  - alert: component_saturation_slo_out_of_bounds:runway_container_instance_utilization
    for: 5m
    annotations:
      title: The Runway Container Instance Utilization resource of the {{ $labels.type
        }} service ({{ $labels.stage }} stage) has a saturation exceeding SLO and
        is close to its capacity limit.
      description: |
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Runway Container Instance Utilization resource:

        Container instance utilization of the Runway service.

        For scaling, refer to https://cloud.google.com/run/docs/configuring/max-instances.
      grafana_dashboard_id: alerts-sat_runway_container_instance
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_runway_container_instance?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-type={{ $labels.type }}&var-stage={{ $labels.stage
        }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1738137433"
      grafana_variables: environment,type,stage
      promql_query: |
        max by(environment, type, shard, revision_name, region, location) (
          clamp_min(
            clamp_max(
              sum by (environment, type, shard, revision_name, region, location) (
                stackdriver_cloud_run_revision_run_googleapis_com_container_instance_count{job="runway-exporter",state="active",environment="{{ $labels.environment }}",type="{{ $labels.type }}"}
              )
              /
              100
              ,
              1)
          ,
          0)
        )
      promql_template_1: |
        max by(environment, type, shard, revision_name, region, location) (
          clamp_min(
            clamp_max(
              sum by (environment, type, shard, revision_name, region, location) (
                stackdriver_cloud_run_revision_run_googleapis_com_container_instance_count{job="runway-exporter",state="active",environment="{{ $labels.environment }}",type="{{ $labels.type }}"}
              )
              /
              100
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
    labels:
      alert_type: cause
      rules_domain: general
      severity: s4
    expr: |
      gitlab_component_saturation:ratio{component="runway_container_instance_utilization",env="ops",monitor="global"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="runway_container_instance_utilization",env="ops",monitor="global"}
  - alert: component_saturation_slo_out_of_bounds:runway_container_max_concurrent_requests
    for: 5m
    annotations:
      title: The Runway Max Concurrent Requests resource of the {{ $labels.type }}
        service ({{ $labels.stage }} stage) has a saturation exceeding SLO and is
        close to its capacity limit.
      description: |
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Runway Max Concurrent Requests resource:

        Max number of concurrent requests being served by each container instance of the Runway service.

        For scaling, refer to https://cloud.google.com/run/docs/configuring/concurrency.
      grafana_dashboard_id: alerts-sat_runway_container_max_con_reqs
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_runway_container_max_con_reqs?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-type={{ $labels.type }}&var-stage={{ $labels.stage
        }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "4285373877"
      grafana_variables: environment,type,stage
      promql_query: |
        max by(environment, type, shard, revision_name, region, location) (
          clamp_min(
            clamp_max(
              histogram_quantile(0.9999, sum by(le, environment, type, shard, revision_name, region, location)
                (
                  rate(
                    stackdriver_cloud_run_revision_run_googleapis_com_container_max_request_concurrencies_bucket{job="runway-exporter",state="active",environment="{{ $labels.environment }}",type="{{ $labels.type }}"}[30m]
                  )
                )
              ) / 100
              ,
              1)
          ,
          0)
        )
      promql_template_1: |
        max by(environment, type, shard, revision_name, region, location) (
          clamp_min(
            clamp_max(
              histogram_quantile(0.9999, sum by(le, environment, type, shard, revision_name, region, location)
                (
                  rate(
                    stackdriver_cloud_run_revision_run_googleapis_com_container_max_request_concurrencies_bucket{job="runway-exporter",state="active",environment="{{ $labels.environment }}",type="{{ $labels.type }}"}[30m]
                  )
                )
              ) / 100
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
    labels:
      alert_type: cause
      rules_domain: general
      severity: s4
    expr: |
      gitlab_component_saturation:ratio{component="runway_container_max_concurrent_requests",env="ops",monitor="global"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="runway_container_max_concurrent_requests",env="ops",monitor="global"}
  - alert: component_saturation_slo_out_of_bounds:runway_container_memory_utilization
    for: 5m
    annotations:
      title: The Runway Container Memory Utilization resource of the {{ $labels.type
        }} service ({{ $labels.stage }} stage) has a saturation exceeding SLO and
        is close to its capacity limit.
      description: |
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Runway Container Memory Utilization resource:

        Container memory utilization of the Runway service distributed across all container instances.

        For scaling, refer to https://cloud.google.com/run/docs/configuring/services/memory-limits.
      grafana_dashboard_id: alerts-sat_runway_container_memory
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_runway_container_memory?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-type={{ $labels.type }}&var-stage={{ $labels.stage
        }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "377718254"
      grafana_variables: environment,type,stage
      promql_query: |
        max by(environment, type, shard, revision_name, region) (
          clamp_min(
            clamp_max(
              histogram_quantile(0.9999, sum by(le, environment, type, shard, revision_name, region)
                (
                  avg_over_time(
                    stackdriver_cloud_run_revision_run_googleapis_com_container_memory_utilizations_bucket{job="runway-exporter",environment="{{ $labels.environment }}",type="{{ $labels.type }}"}[30m]
                  )
                )
              )
              ,
              1)
          ,
          0)
        )
      promql_template_1: |
        max by(environment, type, shard, revision_name, region) (
          clamp_min(
            clamp_max(
              histogram_quantile(0.9999, sum by(le, environment, type, shard, revision_name, region)
                (
                  avg_over_time(
                    stackdriver_cloud_run_revision_run_googleapis_com_container_memory_utilizations_bucket{job="runway-exporter",environment="{{ $labels.environment }}",type="{{ $labels.type }}"}[30m]
                  )
                )
              )
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
    labels:
      alert_type: cause
      rules_domain: general
      severity: s4
    expr: |
      gitlab_component_saturation:ratio{component="runway_container_memory_utilization",env="ops",monitor="global"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="runway_container_memory_utilization",env="ops",monitor="global"}
