# WARNING. DO NOT EDIT THIS FILE BY HAND. USE ./thanos-rules-jsonnet/saturation.jsonnet TO GENERATE IT
# YOUR CHANGES WILL BE OVERRIDDEN
groups:
- name: Saturation Rules (autogenerated)
  interval: 1m
  partial_response_strategy: warn
  rules:
  - record: gitlab_component_saturation:ratio
    labels:
      component: filestore_disk_utilization
      stage: main
      tier: inf
      type: monitoring
    expr: |
      max by(environment, shard) (
        clamp_min(
          clamp_max(
            avg_over_time(stackdriver_filestore_instance_file_googleapis_com_nfs_server_used_bytes_percent{env="ops"}[5m]) / 100
            ,
            1)
        ,
        0)
      )
  - record: gitlab_component_saturation:ratio
    labels:
      component: go_memory
    expr: |
      max by(env, environment, tier, type, stage, shard) (
        clamp_min(
          clamp_max(
            sum by (env, environment, tier, type, stage, shard, fqdn) (
              go_memstats_alloc_bytes{env="ops",type=~"gitaly|monitoring|praefect|registry|web-pages"}
            )
            /
            sum by (env, environment, tier, type, stage, shard, fqdn) (
              node_memory_MemTotal_bytes{env="ops",type=~"gitaly|monitoring|praefect|registry|web-pages"}
            )
            ,
            1)
        ,
        0)
      )
  - record: gitlab_component_saturation:ratio
    labels:
      component: kube_container_cpu
    expr: |
      max by(env, environment, tier, type, stage, shard) (
        clamp_min(
          clamp_max(
            sum by (env, environment, tier, type, stage, shard, pod, container) (
              rate(container_cpu_usage_seconds_total:labeled{container!="", container!="POD", env="ops",type="code_suggestions"}[5m])
            )
            /
            sum by(env, environment, tier, type, stage, shard, pod, container) (
              container_spec_cpu_quota:labeled{container!="", container!="POD", env="ops",type="code_suggestions"}
              /
              container_spec_cpu_period:labeled{container!="", container!="POD", env="ops",type="code_suggestions"}
            )
            ,
            1)
        ,
        0)
      )
  - record: gitlab_component_saturation:ratio
    labels:
      component: kube_container_memory
    expr: |
      max by(env, environment, tier, type, stage, shard) (
        clamp_min(
          clamp_max(
            container_memory_working_set_bytes:labeled{container!="", container!="POD", env="ops",type="code_suggestions"}
            /
            (container_spec_memory_limit_bytes:labeled{container!="", container!="POD", env="ops",type="code_suggestions"} > 0)
            ,
            1)
        ,
        0)
      )
  - record: gitlab_component_saturation:ratio
    labels:
      component: kube_go_memory
    expr: |
      max by(env, environment, tier, type, stage, shard) (
        clamp_min(
          clamp_max(
            go_memstats_alloc_bytes{env="ops",type=~"api|git|internal-api|kas|monitoring|ops-gitlab-net|registry|vault|web|web-pages|websockets"}
            / on(env, environment, tier, type, stage, shard, cluster, pod) group_left()
            topk by(env, environment, tier, type, stage, shard, cluster, pod) (1,
              container_spec_memory_limit_bytes:labeled{container=~"gitlab-pages|gitlab-workhorse|kas|registry|thanos-store",env="ops",type=~"api|git|internal-api|kas|monitoring|ops-gitlab-net|registry|vault|web|web-pages|websockets"}
            )
            ,
            1)
        ,
        0)
      )
  - record: gitlab_component_saturation:ratio
    labels:
      component: kube_horizontalpodautoscaler_desired_replicas
    expr: |
      max by(env, environment, tier, type, stage, shard) (
        clamp_min(
          clamp_max(
            kube_horizontalpodautoscaler_status_desired_replicas:labeled{env="ops",type="code_suggestions", shard!~"database-throttled|elasticsearch|gitaly-throttled|urgent-authorized-projects|urgent-other", namespace!~"pubsubbeat"}
            /
            kube_horizontalpodautoscaler_spec_max_replicas:labeled{env="ops",type="code_suggestions", shard!~"database-throttled|elasticsearch|gitaly-throttled|urgent-authorized-projects|urgent-other", namespace!~"pubsubbeat"}
            ,
            1)
        ,
        0)
      )
  - record: gitlab_component_saturation:ratio
    labels:
      component: kube_node_ips
      stage: main
      tier: inf
      type: kube
    expr: |
      max by(env, environment, shard) (
        clamp_min(
          clamp_max(
            sum(:kube_pod_info_node_count:{env="ops"}) by (env, environment, shard, cluster)
            /
            sum(
              gitlab:gcp_subnet_max_ips{env="ops"} * on (subnet) group_right gitlab:cluster:subnet:mapping{env="ops"}
            ) by (env, environment, shard, cluster)
            ,
            1)
        ,
        0)
      )
  - record: gitlab_component_saturation:ratio
    labels:
      component: kube_pool_max_nodes
    expr: |
      max by(env, environment, tier, type, stage, shard) (
        clamp_min(
          clamp_max(
            count by (cluster, env, environment, label_pool, tier, type, stage, shard) (
              kube_node_labels:labeled{env="ops",type=~"api|internal-api|git|kube|redis-cache|redis-cluster-feature-flag|redis-sidekiq|redis-tracechunks|redis-cluster-ratelimiting|redis-cluster-chat-cache|redis-sessions|redis-registry-cache|redis-repository-cache|redis-db-load-balancing|redis|registry|sidekiq|vault|web-pages|web|websockets"}
            )
            / on(cluster, env, environment, label_pool) group_left() (
              label_replace(
                terraform_report_google_cluster_node_pool_max_node_count,
                "label_pool", "$0", "pool_name", ".*"
              )
              * on(cluster, env, environment) group_left()
              count by (cluster, env, environment) (
                group by (cluster, env, environment, label_topology_kubernetes_io_zone) (
                  kube_node_labels:labeled{env="ops",type=~"api|internal-api|git|kube|redis-cache|redis-cluster-feature-flag|redis-sidekiq|redis-tracechunks|redis-cluster-ratelimiting|redis-cluster-chat-cache|redis-sessions|redis-registry-cache|redis-repository-cache|redis-db-load-balancing|redis|registry|sidekiq|vault|web-pages|web|websockets"}
                )
              )
            )
            ,
            1)
        ,
        0)
      )
  - record: gitlab_component_saturation:ratio
    labels:
      component: nv_gpu_memory
    expr: |
      max by(env, environment, tier, type, stage, shard) (
        clamp_min(
          clamp_max(
            sum by (env, environment, tier, type, stage, shard, gpu_uuid, pod, container) (
              nv_gpu_memory_used_bytes{env="ops",type="code_suggestions"}
            )
            /
            sum by (env, environment, tier, type, stage, shard, gpu_uuid, pod, container) (
              nv_gpu_memory_total_bytes{env="ops",type="code_suggestions"}
            )
            ,
            1)
        ,
        0)
      )
  - record: gitlab_component_saturation:ratio
    labels:
      component: nv_gpu_power
    expr: |
      max by(env, environment, tier, type, stage, shard) (
        clamp_min(
          clamp_max(
            sum by (env, environment, tier, type, stage, shard, gpu_uuid, pod, container) (
              avg_over_time(nv_gpu_power_usage{env="ops",type="code_suggestions"}[30m])
            )
            /
            sum by (env, environment, tier, type, stage, shard, gpu_uuid, pod, container) (
              avg_over_time(nv_gpu_power_limit{env="ops",type="code_suggestions"}[30m])
            )
            ,
            1)
        ,
        0)
      )
  - record: gitlab_component_saturation:ratio
    labels:
      component: nv_gpu_utilization
    expr: |
      max by(env, environment, tier, type, stage, shard) (
        clamp_min(
          clamp_max(
            avg_over_time(nv_gpu_utilization{env="ops",type="code_suggestions"}[30m])
            ,
            1)
        ,
        0)
      )
  - record: gitlab_component_saturation:ratio
    labels:
      component: open_fds
    expr: |
      max by(env, environment, tier, type, stage, shard) (
        clamp_min(
          clamp_max(
            (
              process_open_fds{env="ops",type=~"code_suggestions|filestore"}
              /
              process_max_fds{env="ops",type=~"code_suggestions|filestore"}
            )
            or
            (
              ruby_file_descriptors{env="ops",type=~"code_suggestions|filestore"}
              /
              ruby_process_max_fds{env="ops",type=~"code_suggestions|filestore"}
            )
            ,
            1)
        ,
        0)
      )
  - record: gitlab_component_saturation:ratio
    labels:
      component: pg_primary_cpu
    expr: |
      max by(env, environment, tier, type, stage, shard) (
        clamp_min(
          clamp_max(
            avg without(cpu, mode) (
              1
              -
              (
                rate(node_cpu_seconds_total{mode="idle", env="ops",type=~"patroni|patroni-registry|patroni-embedding|patroni-ci|postgres-archive|sentry"}[5m])
                and on(fqdn)
                pg_replication_is_replica{env="ops",type=~"patroni|patroni-registry|patroni-embedding|patroni-ci|postgres-archive|sentry"} == 0
              )
            )
            ,
            1)
        ,
        0)
      )
  - record: gitlab_component_saturation:ratio
    labels:
      component: pg_vacuum_activity_v2
    expr: |
      max by(env, environment, tier, type, stage, shard) (
        clamp_min(
          clamp_max(
            sum by (env, environment, tier, type, stage, shard) (
              rate(fluentd_pg_auto_vacuum_elapsed_seconds_total{env="ops",type=~"patroni|patroni-registry|patroni-embedding|patroni-ci"}[1d])
              and on (fqdn) (pg_replication_is_replica{env="ops",type=~"patroni|patroni-registry|patroni-embedding|patroni-ci"} == 0)
            )
            /
            avg by (env, environment, tier, type, stage, shard) (
              pg_settings_autovacuum_max_workers{env="ops",type=~"patroni|patroni-registry|patroni-embedding|patroni-ci"}
              and on (instance, job) (pg_replication_is_replica{env="ops",type=~"patroni|patroni-registry|patroni-embedding|patroni-ci"} == 0)
            )
            ,
            1)
        ,
        0)
      )
  - record: gitlab_component_saturation:ratio
    labels:
      component: pg_walsender_cpu
    expr: |
      max by(env, environment, tier, type, stage, shard) (
        clamp_min(
          clamp_max(
            sum by (env, environment, tier, type, stage, shard) (
              sum by(env, environment, tier, type, stage, shard, fqdn) (
                rate(namedprocess_namegroup_cpu_seconds_total{env="ops",type=~"patroni|patroni-registry|patroni-embedding|patroni-ci", groupname=~"pg.worker.walsender|pg.worker.walwriter|wal-g"}[5m])
                and on (fqdn) (pg_replication_is_replica{env="ops",type=~"patroni|patroni-registry|patroni-embedding|patroni-ci"} == 0)
              )
              /
              count by (env, environment, tier, type, stage, shard, fqdn) (
                node_cpu_seconds_total{env="ops",type=~"patroni|patroni-registry|patroni-embedding|patroni-ci", mode="idle"} and on(fqdn) (pg_replication_is_replica{env="ops",type=~"patroni|patroni-registry|patroni-embedding|patroni-ci"} == 0)
              )
            )
            ,
            1)
        ,
        0)
      )
- name: GitLab Component Saturation Statistics
  interval: 5m
  partial_response_strategy: warn
  rules:
  - record: gitlab_component_saturation:ratio_quantile95_1w
    expr: quantile_over_time(0.95, gitlab_component_saturation:ratio{env="ops",monitor="global"}[1w])
  - record: gitlab_component_saturation:ratio_quantile99_1w
    expr: quantile_over_time(0.99, gitlab_component_saturation:ratio{env="ops",monitor="global"}[1w])
  - record: gitlab_component_saturation:ratio_quantile95_1h
    expr: quantile_over_time(0.95, gitlab_component_saturation:ratio{env="ops",monitor="global"}[1h])
  - record: gitlab_component_saturation:ratio_quantile99_1h
    expr: quantile_over_time(0.99, gitlab_component_saturation:ratio{env="ops",monitor="global"}[1h])
  - record: gitlab_component_saturation:ratio_avg_1h
    expr: avg_over_time(gitlab_component_saturation:ratio{env="ops",monitor="global"}[1h])
- name: GitLab Saturation Alerts
  interval: 1m
  partial_response_strategy: warn
  rules:
  - alert: component_saturation_slo_out_of_bounds
    for: 5m
    annotations:
      title: The Filestore Disk Utilization resource of the {{ $labels.type }} service
        ({{ $labels.stage }} stage) has a saturation exceeding SLO and is close to
        its capacity limit.
      description: |
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Filestore Disk Utilization resource:

        Filestore Disk utilization.

        See https://cloud.google.com/monitoring/api/metrics_gcp#gcp-file for more details
      grafana_dashboard_id: alerts-sat_filestore_disk
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_filestore_disk?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-type={{ $labels.type }}&var-stage={{ $labels.stage
        }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3135430381"
      grafana_variables: environment,type,stage
      promql_query: |
        max by(environment, shard, instance_name) (
          clamp_min(
            clamp_max(
              avg_over_time(stackdriver_filestore_instance_file_googleapis_com_nfs_server_used_bytes_percent{environment="{{ $labels.environment }}"}[5m]) / 100
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
    labels:
      alert_type: cause
      rules_domain: general
      severity: s4
    expr: |
      gitlab_component_saturation:ratio{component="filestore_disk_utilization",env="ops",monitor="global"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="filestore_disk_utilization",env="ops",monitor="global"}
  - alert: component_saturation_slo_out_of_bounds
    for: 5m
    annotations:
      title: The Go Memory Utilization per Node resource of the {{ $labels.type }}
        service ({{ $labels.stage }} stage) has a saturation exceeding SLO and is
        close to its capacity limit.
      description: |
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Go Memory Utilization per Node resource:

        Go's memory allocation strategy can make it look like a Go process is saturating memory when measured using RSS, when in fact the process is not at risk of memory saturation. For this reason, we measure Go processes using the `go_memstat_alloc_bytes` metric instead of RSS.
      grafana_dashboard_id: alerts-sat_go_memory
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_go_memory?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-type={{ $labels.type }}&var-stage={{ $labels.stage
        }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3631721613"
      grafana_variables: environment,type,stage
      promql_query: |
        max by(env, environment, tier, type, stage, shard, fqdn) (
          clamp_min(
            clamp_max(
              sum by (env, environment, tier, type, stage, shard, fqdn) (
                go_memstats_alloc_bytes{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
              )
              /
              sum by (env, environment, tier, type, stage, shard, fqdn) (
                node_memory_MemTotal_bytes{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
              )
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
    labels:
      alert_type: cause
      rules_domain: general
      severity: s4
    expr: |
      gitlab_component_saturation:ratio{component="go_memory",env="ops",monitor="global"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="go_memory",env="ops",monitor="global"}
  - alert: component_saturation_slo_out_of_bounds
    for: 15m
    annotations:
      title: The Kube Container CPU Utilization resource of the {{ $labels.type }}
        service ({{ $labels.stage }} stage) has a saturation exceeding SLO and is
        close to its capacity limit.
      description: |
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Kube Container CPU Utilization resource:

        Kubernetes containers are allocated a share of CPU. When this is exhausted, the container may be thottled.
      grafana_dashboard_id: alerts-sat_kube_container_cpu
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_kube_container_cpu?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-type={{ $labels.type }}&var-stage={{ $labels.stage
        }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2713861591"
      grafana_variables: environment,type,stage
      promql_query: |
        max by(environment, tier, type, stage, shard, pod, container) (
          clamp_min(
            clamp_max(
              sum by (environment, tier, type, stage, shard, pod, container) (
                rate(container_cpu_usage_seconds_total:labeled{container!="", container!="POD", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[5m])
              )
              /
              sum by(environment, tier, type, stage, shard, pod, container) (
                container_spec_cpu_quota:labeled{container!="", container!="POD", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
                /
                container_spec_cpu_period:labeled{container!="", container!="POD", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
              )
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
    labels:
      alert_type: cause
      rules_domain: general
      severity: s4
    expr: |
      gitlab_component_saturation:ratio{component="kube_container_cpu",env="ops",monitor="global"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="kube_container_cpu",env="ops",monitor="global"}
  - alert: component_saturation_slo_out_of_bounds
    for: 15m
    annotations:
      title: The Kube Container Memory Utilization resource of the {{ $labels.type
        }} service ({{ $labels.stage }} stage) has a saturation exceeding SLO and
        is close to its capacity limit.
      description: |
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Kube Container Memory Utilization resource:

        This uses the working set size from cAdvisor for the cgroup's memory usage. That may not be a good measure as it includes filesystem cache pages that are not necessarily attributable to the application inside the cgroup, and are permitted to be evicted instead of being OOM killed.
      grafana_dashboard_id: alerts-sat_kube_container_memory
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_kube_container_memory?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-type={{ $labels.type }}&var-stage={{ $labels.stage
        }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "172578411"
      grafana_variables: environment,type,stage
      promql_query: |
        max by(environment, tier, type, stage, shard) (
          clamp_min(
            clamp_max(
              container_memory_working_set_bytes:labeled{container!="", container!="POD", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
              /
              (container_spec_memory_limit_bytes:labeled{container!="", container!="POD", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"} > 0)
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
    labels:
      alert_type: cause
      rules_domain: general
      severity: s4
    expr: |
      gitlab_component_saturation:ratio{component="kube_container_memory",env="ops",monitor="global"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="kube_container_memory",env="ops",monitor="global"}
  - alert: component_saturation_slo_out_of_bounds
    for: 5m
    annotations:
      title: The Go Memory Utilization per Node resource of the {{ $labels.type }}
        service ({{ $labels.stage }} stage) has a saturation exceeding SLO and is
        close to its capacity limit.
      description: |
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Go Memory Utilization per Node resource:

        Measures Go memory usage as a percentage of container memory limit
      grafana_dashboard_id: alerts-sat_kube_go_memory
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_kube_go_memory?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-type={{ $labels.type }}&var-stage={{ $labels.stage
        }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "4163523952"
      grafana_variables: environment,type,stage
      promql_query: |
        max by(env, environment, tier, type, stage, shard, cluster, pod) (
          clamp_min(
            clamp_max(
              go_memstats_alloc_bytes{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
              / on(env, environment, tier, type, stage, shard, cluster, pod) group_left()
              topk by(env, environment, tier, type, stage, shard, cluster, pod) (1,
                container_spec_memory_limit_bytes:labeled{container=~"gitlab-pages|gitlab-workhorse|kas|registry|thanos-store",environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
              )
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
    labels:
      alert_type: cause
      rules_domain: general
      severity: s4
    expr: |
      gitlab_component_saturation:ratio{component="kube_go_memory",env="ops",monitor="global"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="kube_go_memory",env="ops",monitor="global"}
  - alert: component_saturation_slo_out_of_bounds
    for: 25m
    annotations:
      title: The Horizontal Pod Autoscaler Desired Replicas resource of the {{ $labels.type
        }} service ({{ $labels.stage }} stage) has a saturation exceeding SLO and
        is close to its capacity limit.
      description: |
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Horizontal Pod Autoscaler Desired Replicas resource:

        The [Horizontal Pod Autoscaler](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/) automatically scales the number of Pods in a deployment based on metrics.

        The Horizontal Pod Autoscaler has a configured upper maximum. When this limit is reached, the HPA will not increase the number of pods and other resource saturation (eg, CPU, memory) may occur.
      grafana_dashboard_id: alerts-sat_kube_horizontalpodautoscaler
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_kube_horizontalpodautoscaler?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-type={{ $labels.type }}&var-stage={{ $labels.stage
        }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "351198712"
      grafana_variables: environment,type,stage
      promql_query: |
        max by(environment, tier, type, stage, shard, horizontalpodautoscaler, shard) (
          clamp_min(
            clamp_max(
              kube_horizontalpodautoscaler_status_desired_replicas:labeled{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}", shard!~"database-throttled|elasticsearch|gitaly-throttled|urgent-authorized-projects|urgent-other", namespace!~"pubsubbeat"}
              /
              kube_horizontalpodautoscaler_spec_max_replicas:labeled{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}", shard!~"database-throttled|elasticsearch|gitaly-throttled|urgent-authorized-projects|urgent-other", namespace!~"pubsubbeat"}
              ,
              1)
          ,
          0)
        )
      runbook: docs/kube/kubernetes.md#hpascalecapability
    labels:
      alert_type: cause
      rules_domain: general
      severity: s3
    expr: |
      gitlab_component_saturation:ratio{component="kube_horizontalpodautoscaler_desired_replicas",env="ops",monitor="global"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="kube_horizontalpodautoscaler_desired_replicas",env="ops",monitor="global"}
  - alert: component_saturation_slo_out_of_bounds
    for: 5m
    annotations:
      title: The Node IP subnet saturation resource of the {{ $labels.type }} service
        ({{ $labels.stage }} stage) has a saturation exceeding SLO and is close to
        its capacity limit.
      description: |
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Node IP subnet saturation resource:

        This resource measures the number of nodes per subnet.

        If it is becoming saturated, it may indicate that clusters need to be rebuilt with a larger subnet.
      grafana_dashboard_id: alerts-sat_kube_node_ips
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_kube_node_ips?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-type={{ $labels.type }}&var-stage={{ $labels.stage
        }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "776071338"
      grafana_variables: environment,type,stage
      promql_query: |
        max by(env, environment, shard, cluster) (
          clamp_min(
            clamp_max(
              sum(:kube_pod_info_node_count:{environment="{{ $labels.environment }}"}) by (env, environment, shard, cluster)
              /
              sum(
                gitlab:gcp_subnet_max_ips{environment="{{ $labels.environment }}"} * on (subnet) group_right gitlab:cluster:subnet:mapping{environment="{{ $labels.environment }}"}
              ) by (env, environment, shard, cluster)
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
    labels:
      alert_type: cause
      rules_domain: general
      severity: s3
    expr: |
      gitlab_component_saturation:ratio{component="kube_node_ips",env="ops",monitor="global"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="kube_node_ips",env="ops",monitor="global"}
  - alert: component_saturation_slo_out_of_bounds
    for: 5m
    annotations:
      title: The Kube Pool Max Node Limit resource of the {{ $labels.type }} service
        ({{ $labels.stage }} stage) has a saturation exceeding SLO and is close to
        its capacity limit.
      description: |
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Kube Pool Max Node Limit resource:

        A GKE kubernetes node pool is close to it's maximum number of nodes.

        The maximum is defined in terraform, via the `max_node_count` field of a node pool. The limit is per-zone, so for single zone clusters the number of nodes will match the limit, for regional clusters, the limit is multiplied by the number of zones the cluster is deployed over.
      grafana_dashboard_id: alerts-sat_kube_pool_max_nodes
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_kube_pool_max_nodes?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-type={{ $labels.type }}&var-stage={{ $labels.stage
        }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1686893332"
      grafana_variables: environment,type,stage
      promql_query: |
        max by(env, environment, tier, type, stage, shard, cluster, label_pool, shard) (
          clamp_min(
            clamp_max(
              count by (cluster, env, environment, label_pool, tier, type, stage, shard) (
                kube_node_labels:labeled{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
              )
              / on(cluster, env, environment, label_pool) group_left() (
                label_replace(
                  terraform_report_google_cluster_node_pool_max_node_count,
                  "label_pool", "$0", "pool_name", ".*"
                )
                * on(cluster, env, environment) group_left()
                count by (cluster, env, environment) (
                  group by (cluster, env, environment, label_topology_kubernetes_io_zone) (
                    kube_node_labels:labeled{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
                  )
                )
              )
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
    labels:
      alert_type: cause
      rules_domain: general
      severity: s3
    expr: |
      gitlab_component_saturation:ratio{component="kube_pool_max_nodes",env="ops",monitor="global"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="kube_pool_max_nodes",env="ops",monitor="global"}
  - alert: component_saturation_slo_out_of_bounds
    for: 5m
    annotations:
      title: The GPU Memory Utilization resource of the {{ $labels.type }} service
        ({{ $labels.stage }} stage) has a saturation exceeding SLO and is close to
        its capacity limit.
      description: |
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the GPU Memory Utilization resource:

        This resource measures GPU memory utilization per GPU.

        If this resource is becoming saturated, it may indicate that the fleet needs horizontal or vertical scaling.

        For metrics, refer to https://github.com/triton-inference-server/server/blob/main/docs/user_guide/metrics.md#gpu-metrics.

        For scaling, refer to https://gitlab.com/gitlab-com/runbooks/-/tree/master/docs/code_suggestions#scalability.
      grafana_dashboard_id: alerts-sat_nv_gpu_memory
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_nv_gpu_memory?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-type={{ $labels.type }}&var-stage={{ $labels.stage
        }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "4281943858"
      grafana_variables: environment,type,stage
      promql_query: |
        max by(environment, tier, type, stage, shard, gpu_uuid, pod, container) (
          clamp_min(
            clamp_max(
              sum by (environment, tier, type, stage, shard, gpu_uuid, pod, container) (
                nv_gpu_memory_used_bytes{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
              )
              /
              sum by (environment, tier, type, stage, shard, gpu_uuid, pod, container) (
                nv_gpu_memory_total_bytes{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
              )
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
    labels:
      alert_type: cause
      rules_domain: general
      severity: s4
    expr: |
      gitlab_component_saturation:ratio{component="nv_gpu_memory",env="ops",monitor="global"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="nv_gpu_memory",env="ops",monitor="global"}
  - alert: component_saturation_slo_out_of_bounds
    for: 5m
    annotations:
      title: The GPU Power Consumption resource of the {{ $labels.type }} service
        ({{ $labels.stage }} stage) has a saturation exceeding SLO and is close to
        its capacity limit.
      description: |
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the GPU Power Consumption resource:

        This resource measures GPU power consumption per GPU.

        If this resource is becoming saturated, it may indicate that the fleet needs horizontal or vertical scaling.

        For metrics, refer to https://github.com/triton-inference-server/server/blob/main/docs/user_guide/metrics.md#gpu-metrics.

        For scaling, refer to https://gitlab.com/gitlab-com/runbooks/-/tree/master/docs/code_suggestions#scalability.
      grafana_dashboard_id: alerts-sat_nv_gpu_power
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_nv_gpu_power?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-type={{ $labels.type }}&var-stage={{ $labels.stage
        }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "460505062"
      grafana_variables: environment,type,stage
      promql_query: |
        max by(environment, tier, type, stage, shard, gpu_uuid, pod, container) (
          clamp_min(
            clamp_max(
              sum by (environment, tier, type, stage, shard, gpu_uuid, pod, container) (
                avg_over_time(nv_gpu_power_usage{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[30m])
              )
              /
              sum by (environment, tier, type, stage, shard, gpu_uuid, pod, container) (
                avg_over_time(nv_gpu_power_limit{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[30m])
              )
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
    labels:
      alert_type: cause
      rules_domain: general
      severity: s4
    expr: |
      gitlab_component_saturation:ratio{component="nv_gpu_power",env="ops",monitor="global"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="nv_gpu_power",env="ops",monitor="global"}
  - alert: component_saturation_slo_out_of_bounds
    for: 5m
    annotations:
      title: The GPU Utilization resource of the {{ $labels.type }} service ({{ $labels.stage
        }} stage) has a saturation exceeding SLO and is close to its capacity limit.
      description: |
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the GPU Utilization resource:

        This resource measures GPU utilization per GPU.

        If this resource is becoming saturated, it may indicate that the fleet needs horizontal or vertical scaling.

        For metrics, refer to https://github.com/triton-inference-server/server/blob/main/docs/user_guide/metrics.md#gpu-metrics.

        For scaling, refer to https://gitlab.com/gitlab-com/runbooks/-/tree/master/docs/code_suggestions#scalability.
      grafana_dashboard_id: alerts-sat_nv_gpu_utilization
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_nv_gpu_utilization?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-type={{ $labels.type }}&var-stage={{ $labels.stage
        }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3799515334"
      grafana_variables: environment,type,stage
      promql_query: |
        max by(environment, tier, type, stage, shard, gpu_uuid, pod, container) (
          clamp_min(
            clamp_max(
              avg_over_time(nv_gpu_utilization{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[30m])
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
    labels:
      alert_type: cause
      rules_domain: general
      severity: s4
    expr: |
      gitlab_component_saturation:ratio{component="nv_gpu_utilization",env="ops",monitor="global"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="nv_gpu_utilization",env="ops",monitor="global"}
  - alert: component_saturation_slo_out_of_bounds
    for: 5m
    annotations:
      title: The Open file descriptor utilization per instance resource of the {{
        $labels.type }} service ({{ $labels.stage }} stage) has a saturation exceeding
        SLO and is close to its capacity limit.
      description: |
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Open file descriptor utilization per instance resource:

        Open file descriptor utilization per instance.

        Saturation on file descriptor limits may indicate a resource-descriptor leak in the application.

        As a temporary fix, you may want to consider restarting the affected process.
      grafana_dashboard_id: alerts-sat_open_fds
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_open_fds?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-type={{ $labels.type }}&var-stage={{ $labels.stage
        }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1001792825"
      grafana_variables: environment,type,stage
      promql_query: |
        max by(environment, tier, type, stage, shard, job, instance) (
          clamp_min(
            clamp_max(
              (
                process_open_fds{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
                /
                process_max_fds{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
              )
              or
              (
                ruby_file_descriptors{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
                /
                ruby_process_max_fds{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
              )
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
    labels:
      alert_type: cause
      pager: pagerduty
      rules_domain: general
      severity: s2
    expr: |
      gitlab_component_saturation:ratio{component="open_fds",env="ops",monitor="global"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="open_fds",env="ops",monitor="global"}
  - alert: component_saturation_slo_out_of_bounds
    for: 5m
    annotations:
      title: The Average CPU Utilization on Postgres Primary Instance resource of
        the {{ $labels.type }} service ({{ $labels.stage }} stage) has a saturation
        exceeding SLO and is close to its capacity limit.
      description: |
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Average CPU Utilization on Postgres Primary Instance resource:

        Average CPU utilization across all cores on the Postgres primary instance.
      grafana_dashboard_id: alerts-sat_pg_primary_cpu
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_pg_primary_cpu?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-type={{ $labels.type }}&var-stage={{ $labels.stage
        }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3989464622"
      grafana_variables: environment,type,stage
      promql_query: |
        max by(env, environment, tier, type, stage, shard, fqdn) (
          clamp_min(
            clamp_max(
              avg without(cpu, mode) (
                1
                -
                (
                  rate(node_cpu_seconds_total{mode="idle", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[5m])
                  and on(fqdn)
                  pg_replication_is_replica{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"} == 0
                )
              )
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
    labels:
      alert_type: cause
      pager: pagerduty
      rules_domain: general
      severity: s2
    expr: |
      gitlab_component_saturation:ratio{component="pg_primary_cpu",env="ops",monitor="global"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="pg_primary_cpu",env="ops",monitor="global"}
  - alert: component_saturation_slo_out_of_bounds
    for: 5m
    annotations:
      title: The Postgres Autovacuum Activity (non-sampled) resource of the {{ $labels.type
        }} service ({{ $labels.stage }} stage) has a saturation exceeding SLO and
        is close to its capacity limit.
      description: |
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Postgres Autovacuum Activity (non-sampled) resource:

        This measures the total amount of time spent each day by autovacuum workers, as a percentage of total autovacuum capacity.

        This resource uses the `auto_vacuum_elapsed_seconds` value logged by the autovacuum worker, and aggregates this across all autovacuum jobs. In the case that there are 10 autovacuum workers, the total capacity is 10-days worth of autovacuum time per day.

        Once the system is performing 10 days worth of autovacuum per day, the capacity will be saturated.

        This resource is primarily intended to be used for long-term capacity planning.
      grafana_dashboard_id: alerts-sat_pg_vacuum_activity_v2
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_pg_vacuum_activity_v2?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-type={{ $labels.type }}&var-stage={{ $labels.stage
        }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3412018865"
      grafana_variables: environment,type,stage
      promql_query: |
        max by(env, environment, tier, type, stage, shard) (
          clamp_min(
            clamp_max(
              sum by (env, environment, tier, type, stage, shard) (
                rate(fluentd_pg_auto_vacuum_elapsed_seconds_total{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[1d])
                and on (fqdn) (pg_replication_is_replica{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"} == 0)
              )
              /
              avg by (env, environment, tier, type, stage, shard) (
                pg_settings_autovacuum_max_workers{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
                and on (instance, job) (pg_replication_is_replica{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"} == 0)
              )
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
    labels:
      alert_type: cause
      rules_domain: general
      severity: s3
    expr: |
      gitlab_component_saturation:ratio{component="pg_vacuum_activity_v2",env="ops",monitor="global"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="pg_vacuum_activity_v2",env="ops",monitor="global"}
  - alert: component_saturation_slo_out_of_bounds
    for: 5m
    annotations:
      title: The Walsender CPU Saturation resource of the {{ $labels.type }} service
        ({{ $labels.stage }} stage) has a saturation exceeding SLO and is close to
        its capacity limit.
      description: |
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Walsender CPU Saturation resource:

        This saturation metric measures the total amount of time that the primary postgres instance is spending sending WAL segments to replicas. It is expressed as a percentage of all CPU available on the primary postgres instance.

        The more replicas connected, the higher this metric will be.

        Since it's expressed as a percentage of all CPU, this should always remain low, since the CPU primarily needs to be available for handling SQL statements.
      grafana_dashboard_id: alerts-sat_pg_walsender_cpu
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_pg_walsender_cpu?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-type={{ $labels.type }}&var-stage={{ $labels.stage
        }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1879384722"
      grafana_variables: environment,type,stage
      promql_query: |
        max by(env, environment, tier, type, stage, shard) (
          clamp_min(
            clamp_max(
              sum by (env, environment, tier, type, stage, shard) (
                sum by(env, environment, tier, type, stage, shard, fqdn) (
                  rate(namedprocess_namegroup_cpu_seconds_total{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}", groupname=~"pg.worker.walsender|pg.worker.walwriter|wal-g"}[5m])
                  and on (fqdn) (pg_replication_is_replica{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"} == 0)
                )
                /
                count by (env, environment, tier, type, stage, shard, fqdn) (
                  node_cpu_seconds_total{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}", mode="idle"} and on(fqdn) (pg_replication_is_replica{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"} == 0)
                )
              )
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
    labels:
      alert_type: cause
      rules_domain: general
      severity: s3
    expr: |
      gitlab_component_saturation:ratio{component="pg_walsender_cpu",env="ops",monitor="global"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="pg_walsender_cpu",env="ops",monitor="global"}
