In case of failing Postgres backup verification jobs, use the following to troubleshoot:

1. In ["gitlab-restore/postgres-grpd" CI/CD pipelines](https://ops.gitlab.net/gitlab-com/gl-infra/gitlab-restore/postgres-gprd/pipelines), find the pipeline that is subject to investigation and remember its ID.
1. First, look inside the pipeline's jobs output. Sometimes the instance even hasn't been provisioned – quite often due to hitting some quotas (such as number of vCPUs or IP addresses in "gitlab-restore" project). In this case, either clean up instances that are not needed anymore or increase the quotas in GCP.
1. In ["gitlab-restore" project at GCP console](https://console.cloud.google.com/compute/instances?project=gitlab-restore), find an instance with the pipeline ID in instance name. SSH to it and check:
    - Disk space (`df -hT`). If we hit the disk space limit, it is time to increase the disk size again – usually, it's done in the [source code of the "gitlab-restore" project](https://ops.gitlab.net/gitlab-com/gl-infra/gitlab-restore/postgres-gprd), but it is also possible to configure CI/CD schedules to override it.
    - Recent logs (`sudo journalctl -f`, `sudo journalctl --since yesterday | less`). There might be some insights related to, say, WAL-E/WAL-G failures.
    - Postgres replica is working (`sudo gitlab-psql`). If you cannot connect, then either Postgres is not installed properly, or it hasn't reached the point when PGDATA can be considered consistent. If the replaying of WALs is still happening (see the logs), then it is worth waiting some time. Otherwise, the logs should be carefully investigated.
1. Finally, if none of above items revealed any issues, try performing `backup-fetch` manually. For that:
    1. Run a new CI/CD pipeline in "gitlab-restore", using the CI variable values taking them from "Schedules" section (there is "Reveal" button there), and adding `NO_CLEANUP = 1` to preserve the instance.
    1. SSH to the instance after a few minutes, when it's up and running.
    1. Before proceeding, use WAL-E's (WAL-G's) `backup-list` to see the available backups. One of possible reasons of failure is lack of some daily basebackup. In such a case, you need to so to Postgres master node and analyze WAL-E (WAL-G) log (check `sudo -u postgres crontab -l`, it should show how daily basebackups are triggered and where the logs are located). If the list of backups looks right, continue troubleshooting.
    1. Wait until the issue repeats and backup verification fails (assuming it's permament – if not, we only can analyze the logs of the previous runs).
    1. Manually follow the steps from https://ops.gitlab.net/gitlab-com/gl-infra/gitlab-restore/postgres-gprd/blob/master/bootstrap.sh, starting from erasing PGDATA directory and proceedign to WAL-E's (WAL-G's) `backup-fetch` step which normally takes a few hours.
    1. Once `backup-fetch` is finished, you should have a Postgres "archive replica" – a Postgres instance that constantly pulls new WAL data from WAL-E (WAL-G) archive. Check it with `sudo gitlab-psql`. Note, that it is normal if you cannot connect during some period of time and see `FATAL: the database system is starting up` error: until recovery mode is reached a consistent point, Postgres performs REDO and doesn't allow connections. It may take some time (minutes, dozens of minutes), after which you should be able to connect and observe how the database state is constantly changing due to receving (via `wal-fetch`) and replaying new WALs. To see that, use either `select pg_last_xact_replay_timestamp()` or `select now(), created_at, now() - created_at from issues order by id desc limit 1`.
    1. Troubleshoot any failures in place, checking the logs, free disk space and so on.
    1. Finally, once troubleshooting is done, do not forget to destroy the instance manually, it won't get destroyed automatically because of `NO_CLEANUP = 1` we have used!