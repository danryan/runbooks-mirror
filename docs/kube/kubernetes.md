## Kubernetes Log Hunting

Our logging mechanism for GKE will capture all events coming from the Kubernetes
Cluster, but may not capture events for the nodes themselves.  We'll receive
logs from services running on the nodes, but not operations done by Google.
Keep this in mind if you are ever working with preemtible instances, that some
data may just stop showing up.

### Preemptible Searching

Preemptible instances are cycled roughly every 24 hours.  Since those nodes will
disappear from Google, we'll sometimes see entries from nonexisting nodes in
Kibana.  You can find in Stackdriver when a node was cycled using this example
filter: [`jsonPayload.event_subtype="compute.instances.preempted"`](https://console.cloud.google.com/logs/viewer?project=gitlab-pre&minLogLevel=0&expandAll=false&customFacets&limitCustomFacetWidth=true&dateRangeStart=2019-07-21T18%3A37%3A45.912Z&dateRangeEnd=2019-07-22T18%3A37%3A45.912Z&interval=P1D&resource=gce_instance%2Finstance_id%2F8024017080378216245&advancedFilter=jsonPayload.event_subtype%3D%22compute.instances.preempted%22%0A%0A&scrollTimestamp=2019-07-22T16%3A56%3A40.046986000Z)

Ensure to change to the correct project and adjust the times to search as
necessary.

### Kibana

#### Events log for a namespace

Events log stores data from objects similar to doing a `kubectl describe
<object> <objectID>`

This can be found in Kibana on object `json.logName`.  This will be set to
`proejcts/<PROJECT_NAME>/logs/events`.  Example:
`json.logName="projects/gitlab-pre/logs/events"`

This will provide all event data for that Cluster, generated by various
Kubernetes Objects.  We can then proceed to filter based on the data we are
looking for.

* Look for events related to things happening in a specific namespace:
  `json.jsonPayload.involvedObject.namespace` - simply provide it the name of
  the namespace we are looking for
* Look for events related to a specific _type_ of object:
  `json.jsonPayload.involvedObject.kind` - provide it the name of the object,
  example `Service`, `DaemonSet`, `Pod`, etc...
* Look for events targeting a specifc Pod:
  `json.jsonPayload.involvedObject.name` - bonus points when you use a wildcard
  here, you can find events related to all pods of a specific application and or
  replicaset. Examples to filter on:
  * Specific Pod: `gitlab-registry-68cbc8c489-nh9s9`
  * All pods of a replicaset: `gitlab-registry-68cbc8c489`
  * All pods of the deployment: `gitlab-registry`

With all of the above filters set, `json.jsonPayload.message` is going to have
the important bits of information.

#### Events from Pods

Pods emit logs from each container running inside of it, into it's own "log"
inside of stackdriver.  Which means we can search based on the name of the
container running inside of that Pod.  Using the following example:

* Filter `json.logName="projects/gitlab-pre/logs/registry"` - we'll see all log
  data generated by any container named registry.

You can use the same filters above to help sift through specific Pods and
namespaces as desired.

Log data is output in both `stderr` and `stdout`.  Utilize the filter
`json.labels.container.googleapis.com/stream` you can specify either if you
wish.

The desired event data for this style of search will exist in `json.textPayload`

# Alerts

## HighThrottleRate

This is triggered when a container is surpassing the CPU limits governed by the
configuration of the Pod.  This has the potential to lead to poor performance of
this container.  One should investigate the performance of this container over
the course of time and determine if there's unnecessary pressure on the node, or
a potentially misbehaving Pod.  Looking at the logs for said Pod and overall CPU
usage of the pod and node for which that pod lives should lead to necessary
discoveries and future corrective actions.

Keep in mind that there are some configurations we cannot manage.  It's safe to
say that nay Pod that exists in the `kube-system` namespace we may not have
control of the limits as these are created by GKE.  If a Pod is consistently
triggering this alert for a component we do not manage, check for node pressure
and consider opening a line of communication with Google to discuss measures to
alleviate the pain.  We alert for containers that trigger this alert that we
cannot manage as any container in this namespace is critical for the successful
operation of the cluster and it's health overall.


## HPAScaleCapability

The Horizontal Pod Autoscaler has reached it's maximum configured allowed Pods.
This doesn't necessarily mean we are in trouble.

1. Start troubleshooting by validating the service is able to successfully handle
   requests. Utilize the SLI/SLO metrics of the service that is alerting to determine
   if we are violating any service Apdex or Errors
1. If we are not suffering problems, this means we need to further tweak the HPA
   and better optimize its configuration.  Open an issue, noting your findings.
1. Create an alert silence for a period of time and ensure the newly created
   issue is appropriately prioritized to be completed prior to the expiration of
   the silence.
1. If we are violating the service SLI/SLO's we must take further action.  Use
   guidelines below to assist in taking the appropriate action.

When we reach this threshold we must start an investigation into the load that
this service is taking to see if there's been a trend upward that we simply
haven't noticed over time, or if there's a problem processing requests which led
to an undesired effect of scaling upwards out of normal.

Utilize the dashboard
https://dashboards.gitlab.net/d/alerts-sat_kube_hpa_instances/alerts-kube_hpa_desired_replicas-saturation-detail
and observe the Saturation over the course of time to take into account how many
Pods we've been scaling.  Normally we scale with traffic, but how this is
derived differs between services.  If we've been scaling up over a lengthy
period of time (say months), it may simply mean we need to bump the amount of
maximum allowed Pods only if we are in violation of other SLO's for said service.

During the investigation, take a look at the HPA configuration to understand
what drives the scaling needs.  This will help determine what signals to look
deeper into and drive the conversation for what changes need to be made.  When
making changes to the HPA we need to ensure that the cluster will not endure
undue stress.

These configurations are located:
https://gitlab.com/gitlab-com/gl-infra/k8s-workloads/gitlab-com/

## GKENodeCountCritical

We have reached the maximum configured amount of nodes allowed by all node pools.
We must make changes to the node pool configuration.  This is maintained in
Terraform here: [ops.gitlab.net/.../gitlab-com-infrastructure/.../gprd/main.tf](https://ops.gitlab.net/gitlab-com/gitlab-com-infrastructure/blob/e3f1f5edfe90d98f4e410bfc5cc79b265b5fa1f0/environments/gprd/main.tf#L1797)

## GKENodeCountHigh

We are close to reaching the maximum allowed nodes in the node_pool
configuration as defined by terraform.  It would be wise to open an issue an
investigate node resource contention and determine if we should consider raising
this limit or target a service which may be using more resources than considered
normal.

Observe our node pool counts to determine if a particular node pool is at it's
maximum.  If so, we need to investigate the workloads running on this node pool
to determine if we need to simply bump the maximum count for that node pool or
shift workloads around to a differing node pool

Trends for node scaling can be seen using this metric over the course of time:
`count(stackdriver_gce_instance_compute_googleapis_com_instance_uptime{instance_name=~"gke-gprd.*"})`
